id,Topic1,Topic2,Topic3,Topic4,Topic5,Topic6,Topic7,Topic8,Topic9,Topic10,Dominant_topic,summary,Document,year
0,0.273666224,0.046231112,0.001470735,0.00147078,0.001470871,0.001470792,0.001470817,0.001470825,0.497716476,0.173561368,Topic9,"We propose an architecture for VQA which utilizes recurrent layers to
generate visual and textual attention. The memory characteristic of the
proposed recurrent attention units offers a rich joint embedding of visual and
textual features and enables the model to reason relations between several
parts of the image and question. Our single model outperforms the first place
winner on the VQA 1.0 dataset, performs within margin to the current
state-of-the-art ensemble model. We also experiment with replacing attention
mechanisms in other state-of-the-art models with our implementation and show
increased accuracy. In both cases, our recurrent attention mechanism improves
performance in tasks requiring sequential or relational reasoning on the VQA
dataset.",Dual Recurrent Attention Units for Visual Question Answering,2018
1,0.001786053,0.00178595,0.001785897,0.001785938,0.001786032,0.001785904,0.001785916,0.001786014,0.452359498,0.533352799,Topic10,"Recent approaches based on artificial neural networks (ANNs) have shown
promising results for short-text classification. However, many short texts
occur in sequences (e.g., sentences in a document or utterances in a dialog),
and most existing ANN-based systems do not leverage the preceding short texts
when classifying a subsequent one. In this work, we present a model based on
recurrent neural networks and convolutional neural networks that incorporates
the preceding short texts. Our model achieves state-of-the-art results on three
different datasets for dialog act prediction.","Sequential Short-Text Classification with Recurrent and Convolutional
  Neural Networks",2016
2,0.000724872,0.00072483,0.000724874,0.000724839,0.000724794,0.209096132,0.000724794,0.000724808,0.071789675,0.714040383,Topic10,"We introduce the multiresolution recurrent neural network, which extends the
sequence-to-sequence framework to model natural language generation as two
parallel discrete stochastic processes: a sequence of high-level coarse tokens,
and a sequence of natural language tokens. There are many ways to estimate or
learn the high-level coarse tokens, but we argue that a simple extraction
procedure is sufficient to capture a wealth of high-level discourse semantics.
Such procedure allows training the multiresolution recurrent neural network by
maximizing the exact joint log-likelihood over both sequences. In contrast to
the standard log- likelihood objective w.r.t. natural language tokens (word
perplexity), optimizing the joint log-likelihood biases the model towards
modeling high-level abstractions. We apply the proposed model to the task of
dialogue response generation in two challenging domains: the Ubuntu technical
support domain, and Twitter conversations. On Ubuntu, the model outperforms
competing approaches by a substantial margin, achieving state-of-the-art
results according to both automatic evaluation metrics and a human evaluation
study. On Twitter, the model appears to generate more relevant and on-topic
responses according to automatic evaluation metrics. Finally, our experiments
demonstrate that the proposed model is more adept at overcoming the sparsity of
natural language and is better able to capture long-term structure.","Multiresolution Recurrent Neural Networks: An Application to Dialogue
  Response Generation",2016
3,0.000934769,0.000934863,0.021658287,0.000934795,0.000934926,0.000934859,0.10631408,0.000934765,0.614430235,0.251988422,Topic9,"Multi-task learning is motivated by the observation that humans bring to bear
what they know about related problems when solving new ones. Similarly, deep
neural networks can profit from related tasks by sharing parameters with other
networks. However, humans do not consciously decide to transfer knowledge
between tasks. In Natural Language Processing (NLP), it is hard to predict if
sharing will lead to improvements, particularly if tasks are only loosely
related. To overcome this, we introduce Sluice Networks, a general framework
for multi-task learning where trainable parameters control the amount of
sharing. Our framework generalizes previous proposals in enabling sharing of
all combinations of subspaces, layers, and skip connections. We perform
experiments on three task pairs, and across seven different domains, using data
from OntoNotes 5.0, and achieve up to 15% average error reductions over common
approaches to multi-task learning. We show that a) label entropy is predictive
of gains in sluice networks, confirming findings for hard parameter sharing and
b) while sluice networks easily fit noise, they are robust across domains in
practice.",Learning what to share between loosely related tasks,2017
4,0.001219768,0.001219676,0.001219686,0.001219773,0.085849282,0.2345062,0.001219804,0.001219822,0.17833703,0.493988959,Topic10,"We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including template-based
models, bag-of-words models, sequence-to-sequence neural network and latent
variable neural network models. By applying reinforcement learning to
crowdsourced data and real-world user interactions, the system has been trained
to select an appropriate response from the models in its ensemble. The system
has been evaluated through A/B testing with real-world users, where it
performed significantly better than many competing systems. Due to its machine
learning architecture, the system is likely to improve with additional data.",A Deep Reinforcement Learning Chatbot,2017
5,0.145462813,0.001852225,0.001852205,0.001852088,0.001852174,0.033356476,0.001852224,0.001852291,0.001852255,0.808215249,Topic10,"We propose a new generative model of sentences that first samples a prototype
sentence from the training corpus and then edits it into a new sentence.
Compared to traditional models that generate from scratch either left-to-right
or by first sampling a latent sentence vector, our prototype-then-edit model
improves perplexity on language modeling and generates higher quality outputs
according to human evaluation. Furthermore, the model gives rise to a latent
edit vector that captures interpretable semantics such as sentence similarity
and sentence-level analogies.",Generating Sentences by Editing Prototypes,2017
6,0.001219752,0.001219749,0.001219671,0.001219841,0.235593753,0.222299654,0.001219828,0.001219861,0.153810309,0.380977583,Topic10,"We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including neural network and
template-based models. By applying reinforcement learning to crowdsourced data
and real-world user interactions, the system has been trained to select an
appropriate response from the models in its ensemble. The system has been
evaluated through A/B testing with real-world users, where it performed
significantly better than other systems. The results highlight the potential of
coupling ensemble systems with deep reinforcement learning as a fruitful path
for developing real-world, open-domain conversational agents.",A Deep Reinforcement Learning Chatbot (Short Version),2018
7,0.001176769,0.001176707,0.203643094,0.143054798,0.001176647,0.001176777,0.001176824,0.14762125,0.001176717,0.498620416,Topic10,"The paper introduces a new method for discrimination of documents given in
different scripts. The document is mapped into a uniformly coded text of
numerical values. It is derived from the position of the letters in the text
line, based on their typographical characteristics. Each code is considered as
a gray level. Accordingly, the coded text determines a 1-D image, on which
texture analysis by run-length statistics and local binary pattern is
performed. It defines feature vectors representing the script content of the
document. A modified clustering approach employed on document feature vector
groups documents written in the same script. Experimentation performed on two
custom oriented databases of historical documents in old Cyrillic, angular and
round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the
superiority of the proposed method with respect to well-known methods in the
state-of-the-art.",Document Image Coding and Clustering for Script Discrimination,2016
8,0.434995573,0.001408876,0.00140863,0.001408706,0.077761014,0.001408677,0.001408713,0.001408785,0.167085477,0.311705548,Topic1,"Together with the development of more accurate methods in Computer Vision and
Natural Language Understanding, holistic architectures that answer on questions
about the content of real-world images have emerged. In this tutorial, we build
a neural-based approach to answer questions about images. We base our tutorial
on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the
models that we present here can achieve a competitive performance on both
datasets, in fact, they are among the best methods that use a combination of
LSTM with a global, full frame CNN representation of an image. We hope that
after reading this tutorial, the reader will be able to use Deep Learning
frameworks, such as Keras and introduced Kraino, to build various architectures
that will lead to a further performance improvement on this challenging task.",Tutorial on Answering Questions about Images with Deep Learning,2016
9,0.299958816,0.002222621,0.002222481,0.002222513,0.002222802,0.002222754,0.00222252,0.419967348,0.002223022,0.264515123,Topic8,"Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).",pix2code: Generating Code from a Graphical User Interface Screenshot,2017
10,0.001639676,0.001639689,0.001639861,0.00163955,0.001639522,0.001640157,0.001639684,0.001639636,0.548922336,0.43795989,Topic9,"Learned feature representations and sub-phoneme posteriors from Deep Neural
Networks (DNNs) have been used separately to produce significant performance
gains for speaker and language recognition tasks. In this work we show how
these gains are possible using a single DNN for both speaker and language
recognition. The unified DNN approach is shown to yield substantial performance
improvements on the the 2013 Domain Adaptation Challenge speaker recognition
task (55% reduction in EER for the out-of-domain condition) and on the NIST
2011 Language Recognition Evaluation (48% reduction in EER for the 30s test
condition).",A Unified Deep Neural Network for Speaker and Language Recognition,2015
11,0.000885157,0.000885068,0.000885077,0.000885122,0.000885169,0.071221465,0.237741605,0.000885269,0.603898333,0.081827734,Topic9,"We propose Efficient Neural Architecture Search (ENAS), a fast and
inexpensive approach for automatic model design. In ENAS, a controller learns
to discover neural network architectures by searching for an optimal subgraph
within a large computational graph. The controller is trained with policy
gradient to select a subgraph that maximizes the expected reward on the
validation set. Meanwhile the model corresponding to the selected subgraph is
trained to minimize a canonical cross entropy loss. Thanks to parameter sharing
between child models, ENAS is fast: it delivers strong empirical performances
using much fewer GPU-hours than all existing automatic model design approaches,
and notably, 1000x less expensive than standard Neural Architecture Search. On
the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a
test perplexity of 55.8, establishing a new state-of-the-art among all methods
without post-training processing. On the CIFAR-10 dataset, ENAS designs novel
architectures that achieve a test error of 2.89%, which is on par with NASNet
(Zoph et al., 2018), whose test error is 2.65%.",Efficient Neural Architecture Search via Parameter Sharing,2018
12,0.000877754,0.104584649,0.000877303,0.000877573,0.619730081,0.000877349,0.000877345,0.000877466,0.269543074,0.000877405,Topic5,"Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.",Building Machines That Learn and Think Like People,2016
13,0.000869806,0.066669563,0.00086965,0.000869707,0.367305928,0.224097101,0.000869673,0.000869769,0.237328694,0.100250107,Topic5,"While perception tasks such as visual object recognition and text
understanding play an important role in human intelligence, the subsequent
tasks that involve inference, reasoning and planning require an even higher
level of intelligence. The past few years have seen major advances in many
perception tasks using deep learning models. For higher-level inference,
however, probabilistic graphical models with their Bayesian nature are still
more powerful and flexible. To achieve integrated intelligence that involves
both perception and inference, it is naturally desirable to tightly integrate
deep learning and Bayesian models within a principled probabilistic framework,
which we call Bayesian deep learning. In this unified framework, the perception
of text or images using deep learning can boost the performance of higher-level
inference and in return, the feedback from the inference process is able to
enhance the perception of text or images. This survey provides a general
introduction to Bayesian deep learning and reviews its recent applications on
recommender systems, topic models, and control. In this survey, we also discuss
the relationship and differences between Bayesian deep learning and other
related topics like Bayesian treatment of neural networks.",Towards Bayesian Deep Learning: A Survey,2016
14,0.000909337,0.085733675,0.000909328,0.000909246,0.216891649,0.691009394,0.000909284,0.000909307,0.000909484,0.000909296,Topic6,"Learning goal-directed behavior in environments with sparse feedback is a
major challenge for reinforcement learning algorithms. The primary difficulty
arises due to insufficient exploration, resulting in an agent being unable to
learn robust value functions. Intrinsically motivated agents can explore new
behavior for its own sake rather than to directly solve problems. Such
intrinsic behaviors could eventually help the agent solve tasks posed by the
environment. We present hierarchical-DQN (h-DQN), a framework to integrate
hierarchical value functions, operating at different temporal scales, with
intrinsically motivated deep reinforcement learning. A top-level value function
learns a policy over intrinsic goals, and a lower-level function learns a
policy over atomic actions to satisfy the given goals. h-DQN allows for
flexible goal specifications, such as functions over entities and relations.
This provides an efficient space for exploration in complicated environments.
We demonstrate the strength of our approach on two problems with very sparse,
delayed feedback: (1) a complex discrete stochastic decision process, and (2)
the classic ATARI game `Montezuma's Revenge'.","Hierarchical Deep Reinforcement Learning: Integrating Temporal
  Abstraction and Intrinsic Motivation",2016
15,0.781689248,0.001136724,0.001136587,0.001136712,0.209217464,0.001136649,0.001136638,0.001136606,0.001136715,0.001136657,Topic1,"This paper presents a novel yet intuitive approach to unsupervised feature
learning. Inspired by the human visual system, we explore whether low-level
motion-based grouping cues can be used to learn an effective visual
representation. Specifically, we use unsupervised motion-based segmentation on
videos to obtain segments, which we use as 'pseudo ground truth' to train a
convolutional network to segment objects from a single frame. Given the
extensive evidence that motion plays a key role in the development of the human
visual system, we hope that this straightforward approach to unsupervised
learning will be more effective than cleverly designed 'pretext' tasks studied
in the literature. Indeed, our extensive experiments show that this is the
case. When used for transfer learning on object detection, our representation
significantly outperforms previous unsupervised approaches across multiple
settings, especially when training data for the target task is scarce.",Learning Features by Watching Objects Move,2016
16,0.42604176,0.00137019,0.001370203,0.001370198,0.001370214,0.290893434,0.001370189,0.001370086,0.273473521,0.001370205,Topic1,"We propose a simple neural network model to deal with the domain adaptation
problem in object recognition. Our model incorporates the Maximum Mean
Discrepancy (MMD) measure as a regularization in the supervised learning to
reduce the distribution mismatch between the source and target domains in the
latent space. From experiments, we demonstrate that the MMD regularization is
an effective tool to provide good domain adaptation models on both SURF
features and raw image pixels of a particular image data set. We also show that
our proposed model, preceded by the denoising auto-encoder pretraining,
achieves better performance than recent benchmark models on the same data sets.
This work represents the first study of MMD measure in the context of neural
networks.",Domain Adaptive Neural Networks for Object Recognition,2014
17,0.203655785,0.001075488,0.001075429,0.001075449,0.001075539,0.001075441,0.001075445,0.135856738,0.559155098,0.094879588,Topic9,"Recent studies have demonstrated the power of recurrent neural networks for
machine translation, image captioning and speech recognition. For the task of
capturing temporal structure in video, however, there still remain numerous
open research questions. Current research suggests using a simple temporal
feature pooling strategy to take into account the temporal aspect of video. We
demonstrate that this method is not sufficient for gesture recognition, where
temporal information is more discriminative compared to general video
classification tasks. We explore deep architectures for gesture recognition in
video and propose a new end-to-end trainable neural network architecture
incorporating temporal convolutions and bidirectional recurrence. Our main
contributions are twofold; first, we show that recurrence is crucial for this
task; second, we show that adding temporal convolutions leads to significant
improvements. We evaluate the different approaches on the Montalbano gesture
recognition dataset, where we achieve state-of-the-art results.","Beyond Temporal Pooling: Recurrence and Temporal Convolutions for
  Gesture Recognition in Video",2015
18,0.001299024,0.052027271,0.001298876,0.075777325,0.001298978,0.001299182,0.001298943,0.001298939,0.468389534,0.396011928,Topic9,"In this paper, we address the task of Optical Character Recognition(OCR) for
the Telugu script. We present an end-to-end framework that segments the text
image, classifies the characters and extracts lines using a language model. The
segmentation is based on mathematical morphology. The classification module,
which is the most challenging task of the three, is a deep convolutional neural
network. The language is modelled as a third degree markov chain at the glyph
level. Telugu script is a complex alphasyllabary and the language is
agglutinative, making the problem hard. In this paper we apply the latest
advances in neural networks to achieve state-of-the-art error rates. We also
review convolutional neural networks in great detail and expound the
statistical justification behind the many tricks needed to make Deep Learning
work.",Telugu OCR Framework using Deep Learning,2015
19,0.560567925,0.001149758,0.001149737,0.001149608,0.001149742,0.300656365,0.001149704,0.001149639,0.130727792,0.00114973,Topic1,"The ability of the Generative Adversarial Networks (GANs) framework to learn
generative models mapping from simple latent distributions to arbitrarily
complex data distributions has been demonstrated empirically, with compelling
results showing that the latent space of such generators captures semantic
variation in the data distribution. Intuitively, models trained to predict
these semantic latent representations given data may serve as useful feature
representations for auxiliary problems where semantics are relevant. However,
in their existing form, GANs have no means of learning the inverse mapping --
projecting data back into the latent space. We propose Bidirectional Generative
Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and
demonstrate that the resulting learned feature representation is useful for
auxiliary supervised discrimination tasks, competitive with contemporary
approaches to unsupervised and self-supervised feature learning.",Adversarial Feature Learning,2016
20,0.001149637,0.220952171,0.001149704,0.001149609,0.530953744,0.193832484,0.001149591,0.001149591,0.047363509,0.001149959,Topic5,"Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.",The Mythos of Model Interpretability,2016
21,0.000637083,0.000637124,0.326933798,0.000637099,0.052392388,0.055110215,0.257947417,0.000637137,0.290348099,0.01471964,Topic3,"In this paper, we focus on online representation learning in non-stationary
environments which may require continuous adaptation of model architecture. We
propose a novel online dictionary-learning (sparse-coding) framework which
incorporates the addition and deletion of hidden units (dictionary elements),
and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of
the hippocampus, known to be associated with improved cognitive function and
adaptation to new environments. In the online learning setting, where new input
instances arrive sequentially in batches, the neuronal-birth is implemented by
adding new units with random initial weights (random dictionary elements); the
number of new units is determined by the current performance (representation
error) of the dictionary, higher error causing an increase in the birth rate.
Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity)
on the dictionary within the block-coordinate descent optimization at each
iteration of our online alternating minimization scheme, which iterates between
the code and dictionary updates. Finally, hidden unit connectivity adaptation
is facilitated by introducing sparsity in dictionary elements. Our empirical
evaluation on several real-life datasets (images and language) as well as on
synthetic data demonstrates that the proposed approach can considerably
outperform the state-of-art fixed-size (nonadaptive) online sparse coding of
Mairal et al. (2009) in the presence of nonstationary data. Moreover, we
identify certain properties of the data (e.g., sparse inputs with nearly
non-overlapping supports) and of the model (e.g., dictionary sparsity)
associated with such improvements.","Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a
  Changing World",2017
22,0.178650569,0.000671229,0.000671301,0.000671305,0.000671321,0.000671266,0.00067135,0.000671268,0.8159791,0.000671292,Topic9,"Deep neural networks require a large amount of labeled training data during
supervised learning. However, collecting and labeling so much data might be
infeasible in many cases. In this paper, we introduce a source-target selective
joint fine-tuning scheme for improving the performance of deep learning tasks
with insufficient training data. In this scheme, a target learning task with
insufficient training data is carried out simultaneously with another source
learning task with abundant training data. However, the source learning task
does not use all existing training data. Our core idea is to identify and use a
subset of training images from the original source learning task whose
low-level characteristics are similar to those from the target learning task,
and jointly fine-tune shared convolutional layers for both tasks. Specifically,
we compute descriptors from linear or nonlinear filter bank responses on
training images from both tasks, and use such descriptors to search for a
desired subset of training samples for the source learning task.
  Experiments demonstrate that our selective joint fine-tuning scheme achieves
state-of-the-art performance on multiple visual classification tasks with
insufficient training data for deep learning. Such tasks include Caltech 256,
MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to
fine-tuning without a source domain, the proposed method can improve the
classification accuracy by 2% - 10% using a single model.","Borrowing Treasures from the Wealthy: Deep Transfer Learning through
  Selective Joint Fine-tuning",2017
23,0.2874083,0.001266295,0.001266013,0.00126606,0.001266171,0.0012661,0.001266089,0.001266134,0.409725537,0.2940033,Topic9,"An important goal of computer vision is to build systems that learn visual
representations over time that can be applied to many tasks. In this paper, we
investigate a vision-language embedding as a core representation and show that
it leads to better cross-task transfer than standard multi-task learning. In
particular, the task of visual recognition is aligned to the task of visual
question answering by forcing each to use the same word-region embeddings. We
show this leads to greater inductive transfer from recognition to VQA than
standard multitask learning. Visual recognition also improves, especially for
categories that have relatively few recognition training labels but appear
often in the VQA setting. Thus, our paper takes a small step towards creating
more general vision systems by showing the benefit of interpretable, flexible,
and trainable core representations.","Aligned Image-Word Representations Improve Inductive Transfer Across
  Vision-Language Tasks",2017
24,0.989021091,0.001219937,0.001219771,0.001219868,0.001219867,0.001219866,0.001219855,0.001219866,0.001220087,0.001219793,Topic1,"While deep learning is remarkably successful on perceptual tasks, it was also
shown to be vulnerable to adversarial perturbations of the input. These
perturbations denote noise added to the input that was generated specifically
to fool the system while being quasi-imperceptible for humans. More severely,
there even exist universal perturbations that are input-agnostic but fool the
network on the majority of inputs. While recent work has focused on image
classification, this work proposes attacks against semantic image segmentation:
we present an approach for generating (universal) adversarial perturbations
that make the network yield a desired target segmentation as output. We show
empirically that there exist barely perceptible universal noise patterns which
result in nearly the same predicted segmentation for arbitrary inputs.
Furthermore, we also show the existence of universal noise which removes a
target class (e.g., all pedestrians) from the segmentation while leaving the
segmentation mostly unchanged otherwise.",Universal Adversarial Perturbations Against Semantic Image Segmentation,2017
25,0.001818421,0.001818645,0.001818527,0.001818528,0.001818403,0.074350219,0.490531596,0.001818349,0.422388942,0.001818371,Topic7,"While the optimization problem behind deep neural networks is highly
non-convex, it is frequently observed in practice that training deep networks
seems possible without getting stuck in suboptimal points. It has been argued
that this is the case as all local minima are close to being globally optimal.
We show that this is (almost) true, in fact almost all local minima are
globally optimal, for a fully connected network with squared loss and analytic
activation function given that the number of hidden units of one layer of the
network is larger than the number of training points and the network structure
from this layer on is pyramidal.",The loss surface of deep and wide neural networks,2017
26,0.887532731,0.001087215,0.001087318,0.001087295,0.001087266,0.001087381,0.001087412,0.103769053,0.001087162,0.001087169,Topic1,"We propose a new algorithm for training generative adversarial networks that
jointly learns latent codes for both identities (e.g. individual humans) and
observations (e.g. specific photographs). By fixing the identity portion of the
latent codes, we can generate diverse images of the same subject, and by fixing
the observation portion, we can traverse the manifold of subjects while
maintaining contingent aspects such as lighting and pose. Our algorithm
features a pairwise training scheme in which each sample from the generator
consists of two images with a common identity code. Corresponding samples from
the real dataset consist of two distinct photographs of the same subject. In
order to fool the discriminator, the generator must produce pairs that are
photorealistic, distinct, and appear to depict the same individual. We augment
both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate
pairwise training. Experiments with human judges and an off-the-shelf face
verification system demonstrate our algorithm's ability to generate convincing,
identity-matched photographs.","Semantically Decomposing the Latent Spaces of Generative Adversarial
  Networks",2017
27,0.001613158,0.00161312,0.04737898,0.001613098,0.001613058,0.118365976,0.547496288,0.001613126,0.277080078,0.001613117,Topic7,"Adaptive gradient methods have become recently very popular, in particular as
they have been shown to be useful in the training of deep neural networks. In
this paper we have analyzed RMSProp, originally proposed for the training of
deep neural networks, in the context of online convex optimization and show
$\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and
SC-RMSProp for which we show logarithmic regret bounds for strongly convex
functions. Finally, we demonstrate in the experiments that these new variants
outperform other adaptive gradient techniques or stochastic gradient descent in
the optimization of strongly convex functions as well as in training of deep
neural networks.",Variants of RMSProp and Adagrad with Logarithmic Regret Bounds,2017
28,0.532017989,0.001695224,0.001695268,0.001695251,0.001695335,0.454419841,0.00169527,0.001695144,0.001695377,0.001695301,Topic1,"We investigate the non-identifiability issues associated with bidirectional
adversarial training for joint distribution matching. Within a framework of
conditional entropy, we propose both adversarial and non-adversarial approaches
to learn desirable matched joint distributions for unsupervised and supervised
tasks. We unify a broad family of adversarial models as joint distribution
matching problems. Our approach stabilizes learning of unsupervised
bidirectional adversarial learning methods. Further, we introduce an extension
for semi-supervised learning tasks. Theoretical results are validated in
synthetic data and real-world applications.","ALICE: Towards Understanding Adversarial Learning for Joint Distribution
  Matching",2017
29,0.000699467,0.000699493,0.000699505,0.000699489,0.162894481,0.000699481,0.425438319,0.000699434,0.406770889,0.000699441,Topic7,"In this study, we systematically investigate the impact of class imbalance on
classification performance of convolutional neural networks (CNNs) and compare
frequently used methods to address the issue. Class imbalance is a common
problem that has been comprehensively studied in classical machine learning,
yet very limited systematic research is available in the context of deep
learning. In our study, we use three benchmark datasets of increasing
complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of
imbalance on classification and perform an extensive comparison of several
methods to address the issue: oversampling, undersampling, two-phase training,
and thresholding that compensates for prior class probabilities. Our main
evaluation metric is area under the receiver operating characteristic curve
(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is
associated with notable difficulties in the context of imbalanced data. Based
on results from our experiments we conclude that (i) the effect of class
imbalance on classification performance is detrimental; (ii) the method of
addressing class imbalance that emerged as dominant in almost all analyzed
scenarios was oversampling; (iii) oversampling should be applied to the level
that totally eliminates the imbalance, whereas undersampling can perform better
when the imbalance is only removed to some extent; (iv) as opposed to some
classical machine learning models, oversampling does not necessarily cause
overfitting of CNNs; (v) thresholding should be applied to compensate for prior
class probabilities when overall number of properly classified cases is of
interest.","A systematic study of the class imbalance problem in convolutional
  neural networks",2017
30,0.001639791,0.111887329,0.254164031,0.001639724,0.458368121,0.001639793,0.052270443,0.00163974,0.115111152,0.001639875,Topic5,"Regularization is one of the crucial ingredients of deep learning, yet the
term regularization has various definitions, and regularization methods are
often studied separately from each other. In our work we present a systematic,
unifying taxonomy to categorize existing methods. We distinguish methods that
affect data, network architectures, error terms, regularization terms, and
optimization procedures. We do not provide all details about the listed
methods; instead, we present an overview of how the methods can be sorted into
meaningful categories and sub-categories. This helps revealing links and
fundamental similarities between them. Finally, we include practical
recommendations both for users and for developers of new regularization
methods.",Regularization for Deep Learning: A Taxonomy,2017
31,0.072631087,0.001666975,0.001667094,0.001667083,0.329843527,0.001667157,0.253129891,0.001666973,0.334393083,0.001667129,Topic9,"Clustering is a fundamental machine learning method. The quality of its
results is dependent on the data distribution. For this reason, deep neural
networks can be used for learning better representations of the data. In this
paper, we propose a systematic taxonomy for clustering with deep learning, in
addition to a review of methods from the field. Based on our taxonomy, creating
new methods is more straightforward. We also propose a new approach which is
built on the taxonomy and surpasses some of the limitations of some previous
work. Our experimental evaluation on image datasets shows that the method
approaches state-of-the-art clustering quality, and performs better in some
cases.",Clustering with Deep Learning: Taxonomy and New Methods,2018
32,0.001163151,0.001163233,0.001163125,0.442414105,0.001163232,0.001163105,0.187230134,0.108925751,0.25445114,0.001163023,Topic4,"We tackle here the problem of multimodal image non-rigid registration, which
is of prime importance in remote sensing and medical imaging. The difficulties
encountered by classical registration approaches include feature design and
slow optimization by gradient descent. By analyzing these methods, we note the
significance of the notion of scale. We design easy-to-train,
fully-convolutional neural networks able to learn scale-specific features. Once
chained appropriately, they perform global registration in linear time, getting
rid of gradient descent schemes by predicting directly the deformation.We show
their performance in terms of quality and speed through various tasks of remote
sensing multimodal image alignment. In particular, we are able to register
correctly cadastral maps of buildings as well as road polylines onto RGB
images, and outperform current keypoint matching methods.","Coarse to fine non-rigid registration: a chain of scale-specific neural
  networks for multimodal image alignment with application to remote sensing",2018
33,0.634771227,0.000926168,0.000926044,0.000926141,0.000926203,0.00092628,0.000926138,0.000926262,0.130017203,0.228728334,Topic1,"Recent progress in using recurrent neural networks (RNNs) for image
description has motivated the exploration of their application for video
description. However, while images are static, working with videos requires
modeling their dynamic temporal structure and then properly integrating that
information into a natural language description. In this context, we propose an
approach that successfully takes into account both the local and global
temporal structure of videos to produce descriptions. First, our approach
incorporates a spatial temporal 3-D convolutional neural network (3-D CNN)
representation of the short temporal dynamics. The 3-D CNN representation is
trained on video action recognition tasks, so as to produce a representation
that is tuned to human motion and behavior. Second we propose a temporal
attention mechanism that allows to go beyond local temporal modeling and learns
to automatically select the most relevant temporal segments given the
text-generating RNN. Our approach exceeds the current state-of-art for both
BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on
a new, larger and more challenging dataset of paired video and natural language
descriptions.",Describing Videos by Exploiting Temporal Structure,2015
34,0.157337681,0.000854841,0.000854898,0.000854885,0.000854869,0.034560192,0.000854993,0.000854902,0.189763854,0.613208886,Topic10,"Hybrid methods that utilize both content and rating information are commonly
used in many recommender systems. However, most of them use either handcrafted
features or the bag-of-words representation as a surrogate for the content
information but they are neither effective nor natural enough. To address this
problem, we develop a collaborative recurrent autoencoder (CRAE) which is a
denoising recurrent autoencoder (DRAE) that models the generation of content
sequences in the collaborative filtering (CF) setting. The model generalizes
recent advances in recurrent deep learning from i.i.d. input to non-i.i.d.
(CF-based) input and provides a new denoising scheme along with a novel
learnable pooling scheme for the recurrent autoencoder. To do this, we first
develop a hierarchical Bayesian model for the DRAE and then generalize it to
the CF setting. The synergy between denoising and CF enables CRAE to make
accurate recommendations while learning to fill in the blanks in sequences.
Experiments on real-world datasets from different domains (CiteULike and
Netflix) show that, by jointly modeling the order-aware generation of sequences
for the content information and performing CF for the ratings, CRAE is able to
significantly outperform the state of the art on both the recommendation task
based on ratings and the sequence generation task based on content information.","Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in
  the Blanks",2016
35,0.202621907,0.002439411,0.002439689,0.002439587,0.170619605,0.00243985,0.002439598,0.002439615,0.002439615,0.609681124,Topic10,"In this project we analysed how much semantic information images carry, and
how much value image data can add to sentiment analysis of the text associated
with the images. To better understand the contribution from images, we compared
models which only made use of image data, models which only made use of text
data, and models which combined both data types. We also analysed if this
approach could help sentiment classifiers generalize to unknown sentiments.",Sentiment Classification using Images and Label Embeddings,2017
36,0.000833519,0.000833524,0.000833496,0.000833475,0.000833557,0.360028726,0.000833505,0.000833473,0.633303207,0.000833519,Topic9,"Neural networks (NN) have achieved state-of-the-art performance in various
applications. Unfortunately in applications where training data is
insufficient, they are often prone to overfitting. One effective way to
alleviate this problem is to exploit the Bayesian approach by using Bayesian
neural networks (BNN). Another shortcoming of NN is the lack of flexibility to
customize different distributions for the weights and neurons according to the
data, as is often done in probabilistic graphical models. To address these
problems, we propose a class of probabilistic neural networks, dubbed
natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment
of NN. NPN allows the usage of arbitrary exponential-family distributions to
model the weights and neurons. Different from traditional NN and BNN, NPN takes
distributions as input and goes through layers of transformation before
producing distributions to match the target output distributions. As a Bayesian
treatment, efficient backpropagation (BP) is performed to learn the natural
parameters for the distributions over both the weights and neurons. The output
distributions of each layer, as byproducts, may be used as second-order
representations for the associated tasks such as link prediction. Experiments
on real-world datasets show that NPN can achieve state-of-the-art performance.",Natural-Parameter Networks: A Class of Probabilistic Neural Networks,2016
37,0.000952616,0.125751895,0.000952511,0.048459918,0.650126655,0.169945728,0.000952675,0.000952586,0.000952723,0.000952691,Topic5,"When encountering novel objects, humans are able to infer a wide range of
physical properties such as mass, friction and deformability by interacting
with them in a goal driven way. This process of active interaction is in the
same spirit as a scientist performing experiments to discover hidden facts.
Recent advances in artificial intelligence have yielded machines that can
achieve superhuman performance in Go, Atari, natural language processing, and
complex control problems; however, it is not clear that these systems can rival
the scientific intuition of even a young child. In this work we introduce a
basic set of tasks that require agents to estimate properties such as mass and
cohesion of objects in an interactive simulated environment where they can
manipulate the objects and observe the consequences. We found that state of art
deep reinforcement learning methods can learn to perform the experiments
necessary to discover such hidden properties. By systematically manipulating
the problem difficulty and the cost incurred by the agent for performing
experiments, we found that agents learn different strategies that balance the
cost of gathering information against the cost of making mistakes in different
situations.",Learning to Perform Physics Experiments via Deep Reinforcement Learning,2016
38,0.001250361,0.001250311,0.00125026,0.001250199,0.349582777,0.093636276,0.001250368,0.0012503,0.061552356,0.487726791,Topic10,"Teaching machines to accomplish tasks by conversing naturally with humans is
challenging. Currently, developing task-oriented dialogue systems requires
creating multiple components and typically this involves either a large amount
of handcrafting, or acquiring costly labelled datasets to solve a statistical
learning problem for each component. In this work we introduce a neural
network-based text-in, text-out end-to-end trainable goal-oriented dialogue
system along with a new way of collecting dialogue data based on a novel
pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue
systems easily and without making too many assumptions about the task at hand.
The results show that the model can converse with human subjects naturally
whilst helping them to accomplish tasks in a restaurant search domain.",A Network-based End-to-End Trainable Task-oriented Dialogue System,2016
39,0.002041428,0.002041743,0.178823691,0.002041133,0.002041414,0.002041212,0.002041241,0.00204106,0.002041396,0.80484568,Topic10,"Embedding-based Knowledge Base Completion models have so far mostly combined
distributed representations of individual entities or relations to compute
truth scores of missing links. Facts can however also be represented using
pairwise embeddings, i.e. embeddings for pairs of entities and relations. In
this paper we explore such bigram embeddings with a flexible Factorization
Machine model and several ablations from it. We investigate the relevance of
various bigram types on the fb15k237 dataset and find relative improvements
compared to a compositional model.","A Factorization Machine Framework for Testing Bigram Embeddings in
  Knowledgebase Completion",2016
40,0.001724494,0.001724447,0.001724509,0.001724408,0.001725189,0.001724511,0.001724422,0.001724462,0.589963807,0.396239751,Topic9,"Existing models based on artificial neural networks (ANNs) for sentence
classification often do not incorporate the context in which sentences appear,
and classify sentences individually. However, traditional sentence
classification approaches have been shown to greatly benefit from jointly
classifying subsequent sentences, such as with conditional random fields. In
this work, we present an ANN architecture that combines the effectiveness of
typical ANN models to classify sentences in isolation, with the strength of
structured prediction. Our model achieves state-of-the-art results on two
different datasets for sequential sentence classification in medical abstracts.","Neural Networks for Joint Sentence Classification in Medical Paper
  Abstracts",2016
41,0.000629076,0.000629088,0.000629042,0.000629021,0.435323835,0.000629056,0.000629064,0.000629134,0.146573274,0.413699409,Topic5,"Objective: Patient notes in electronic health records (EHRs) may contain
critical information for medical investigations. However, the vast majority of
medical investigators can only access de-identified notes, in order to protect
the confidentiality of patients. In the United States, the Health Insurance
Portability and Accountability Act (HIPAA) defines 18 types of protected health
information (PHI) that needs to be removed to de-identify patient notes. Manual
de-identification is impractical given the size of EHR databases, the limited
number of researchers with access to the non-de-identified notes, and the
frequent mistakes of human annotators. A reliable automated de-identification
system would consequently be of high value.
  Materials and Methods: We introduce the first de-identification system based
on artificial neural networks (ANNs), which requires no handcrafted features or
rules, unlike existing systems. We compare the performance of the system with
state-of-the-art systems on two datasets: the i2b2 2014 de-identification
challenge dataset, which is the largest publicly available de-identification
dataset, and the MIMIC de-identification dataset, which we assembled and is
twice as large as the i2b2 2014 dataset.
  Results: Our ANN model outperforms the state-of-the-art systems. It yields an
F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision
of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with
a recall 99.25 and a precision of 99.06.
  Conclusion: Our findings support the use of ANNs for de-identification of
patient notes, as they show better performance than previously published
systems while requiring no feature engineering.",De-identification of Patient Notes with Recurrent Neural Networks,2016
42,0.001316169,0.109136756,0.001315991,0.001316108,0.001316073,0.001316104,0.001316088,0.001316167,0.240166061,0.641484484,Topic10,"Hypothesis testing is an important cognitive process that supports human
reasoning. In this paper, we introduce a computational hypothesis testing
approach based on memory augmented neural networks. Our approach involves a
hypothesis testing loop that reconsiders and progressively refines a previously
formed hypothesis in order to generate new hypotheses to test. We apply the
proposed approach to language comprehension task by using Neural Semantic
Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an
absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by
single and ensemble systems on standard machine comprehension benchmarks such
as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.","Reasoning with Memory Augmented Neural Networks for Language
  Comprehension",2016
43,0.001333681,0.059575618,0.001333687,0.029236226,0.001333669,0.001333807,0.001333826,0.001333603,0.354362188,0.548823698,Topic10,"Although deep learning models have proven effective at solving problems in
natural language processing, the mechanism by which they come to their
conclusions is often unclear. As a result, these models are generally treated
as black boxes, yielding no insight of the underlying learned patterns. In this
paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new
approach for tracking the importance of a given input to the LSTM for a given
output. By identifying consistently important patterns of words, we are able to
distill state of the art LSTMs on sentiment analysis and question answering
into a set of representative phrases. This representation is then
quantitatively validated by using the extracted phrases to construct a simple,
rule-based classifier which approximates the output of the LSTM.",Automatic Rule Extraction from Long Short Term Memory Networks,2017
44,0.014213024,0.000629119,0.040129942,0.000629071,0.391992372,0.000629091,0.000629152,0.000629057,0.237409604,0.313109568,Topic5,"Objective: We investigate whether deep learning techniques for natural
language processing (NLP) can be used efficiently for patient phenotyping.
Patient phenotyping is a classification task for determining whether a patient
has a medical condition, and is a crucial part of secondary analysis of
healthcare data. We assess the performance of deep learning algorithms and
compare them with classical NLP approaches.
  Materials and Methods: We compare convolutional neural networks (CNNs),
n-gram models, and approaches based on cTAKES that extract pre-defined medical
concepts from clinical notes and use them to predict patient phenotypes. The
performance is tested on 10 different phenotyping tasks using 1,610 discharge
summaries extracted from the MIMIC-III database.
  Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The
average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our
model having an F1-score up to 37 points higher than alternative approaches. We
additionally assess the interpretability of our model by presenting a method
that extracts the most salient phrases for a particular prediction.
  Conclusion: We show that NLP methods based on deep learning improve the
performance of patient phenotyping. Our CNN-based algorithm automatically
learns the phrases associated with each patient phenotype. As such, it reduces
the annotation complexity for clinical domain experts, who are normally
required to develop task-specific annotation rules and identify relevant
phrases. Our method performs well in terms of both performance and
interpretability, which indicates that deep learning is an effective approach
to patient phenotyping based on clinicians' notes.",Comparing Rule-Based and Deep Learning Models for Patient Phenotyping,2017
45,0.03380503,0.002128255,0.002127834,0.002127891,0.002128322,0.002127865,0.002127915,0.002128196,0.218574592,0.732724099,Topic10,"Over 50 million scholarly articles have been published: they constitute a
unique repository of knowledge. In particular, one may infer from them
relations between scientific concepts, such as synonyms and hyponyms.
Artificial neural networks have been recently explored for relation extraction.
In this work, we continue this line of work and present a system based on a
convolutional neural network to extract relations. Our model ranked first in
the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific
articles (subtask C).","MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional
  Neural Networks",2017
46,0.001408838,0.001408721,0.001408708,0.001408662,0.123269009,0.001408729,0.001408732,0.001408736,0.619196402,0.247673463,Topic9,"Recent approaches based on artificial neural networks (ANNs) have shown
promising results for named-entity recognition (NER). In order to achieve high
performances, ANNs need to be trained on a large labeled dataset. However,
labels might be difficult to obtain for the dataset on which the user wants to
perform NER: label scarcity is particularly pronounced for patient note
de-identification, which is an instance of NER. In this work, we analyze to
what extent transfer learning may address this issue. In particular, we
demonstrate that transferring an ANN model trained on a large labeled dataset
to another dataset with a limited number of labels improves upon the
state-of-the-art results on two different datasets for patient note
de-identification.",Transfer Learning for Named-Entity Recognition with Neural Networks,2017
47,0.48444105,0.0012349,0.001234745,0.001234835,0.00123487,0.103877279,0.001234838,0.001234787,0.001234818,0.403037878,Topic1,"Generative Adversarial Networks (GANs) have gathered a lot of attention from
the computer vision community, yielding impressive results for image
generation. Advances in the adversarial generation of natural language from
noise however are not commensurate with the progress made in generating images,
and still lag far behind likelihood based methods. In this paper, we take a
step towards generating natural language with a GAN objective alone. We
introduce a simple baseline that addresses the discrete output space problem
without relying on gradient estimators and show that it is able to achieve
state-of-the-art results on a Chinese poem generation dataset. We present
quantitative results on generating sentences from context-free and
probabilistic context-free grammars, and qualitative language modeling results.
A conditional version is also described that can generate sequences conditioned
on sentence characteristics.",Adversarial Generation of Natural Language,2017
48,0.001370207,0.167512723,0.001370107,0.001370158,0.001370187,0.001370175,0.001370196,0.00137009,0.682128438,0.140767719,Topic9,"Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.",Explaining Recurrent Neural Network Predictions in Sentiment Analysis,2017
49,0.181425206,0.001087154,0.001087284,0.00108718,0.001087262,0.001087121,0.366991778,0.001087228,0.001087193,0.443972595,Topic10,"Can textual data be compressed intelligently without losing accuracy in
evaluating sentiment? In this study, we propose a novel evolutionary
compression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),
which makes use of Parts-of-Speech tags to compress text in a way that
sacrifices minimal classification accuracy when used in conjunction with
sentiment analysis algorithms. An analysis of PARSEC with eight commercial and
non-commercial sentiment analysis algorithms on twelve English sentiment data
sets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss
in sentiment classification accuracy for (20%, 50%, 75%) data compression with
PARSEC using LingPipe, the most accurate of the sentiment algorithms. Other
sentiment analysis algorithms are more severely affected by compression. We
conclude that significant compression of text data is possible for sentiment
analysis depending on the accuracy demands of the specific application and the
specific sentiment analysis algorithm used.",Text Compression for Sentiment Analysis via Evolutionary Algorithms,2017
50,0.000719608,0.000719572,0.000719627,0.00071955,0.000719554,0.000719668,0.000719601,0.000719546,0.110882284,0.88336099,Topic10,"Direct acoustics-to-word (A2W) models in the end-to-end paradigm have
received increasing attention compared to conventional sub-word based automatic
speech recognition models using phones, characters, or context-dependent hidden
Markov model states. This is because A2W models recognize words from speech
without any decoder, pronunciation lexicon, or externally-trained language
model, making training and decoding with such models simple. Prior work has
shown that A2W models require orders of magnitude more training data in order
to perform comparably to conventional models. Our work also showed this
accuracy gap when using the English Switchboard-Fisher data set. This paper
describes a recipe to train an A2W model that closes this gap and is at-par
with state-of-the-art sub-word based models. We achieve a word error rate of
8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder
or language model. We find that model initialization, training data order, and
regularization have the most impact on the A2W model performance. Next, we
present a joint word-character A2W model that learns to first spell the word
and then recognize it. This model provides a rich output to the user instead of
simple word hypotheses, making it especially useful in the case of words unseen
or rarely-seen during training.","Building competitive direct acoustics-to-word models for English
  conversational speech recognition",2017
51,0.426095861,0.000699485,0.00069939,0.000699434,0.000699439,0.000699463,0.000699432,0.000699446,0.251487301,0.317520748,Topic1,"We address the problem of Visual Question Answering (VQA), which requires
joint image and language understanding to answer a question about a given
photograph. Recent approaches have applied deep image captioning methods based
on convolutional-recurrent networks to this problem, but have failed to model
spatial inference. To remedy this, we propose a model we call the Spatial
Memory Network and apply it to the VQA task. Memory networks are recurrent
neural networks with an explicit attention mechanism that selects certain parts
of the information stored in memory. Our Spatial Memory Network stores neuron
activations from different spatial regions of the image in its memory, and uses
the question to choose relevant regions for computing the answer, a process of
which constitutes a single ""hop"" in the network. We propose a novel spatial
attention architecture that aligns words with image patches in the first hop,
and obtain improved results by adding a second attention hop which considers
the whole question to choose visual evidence based on the results of the first
hop. To better understand the inference process learned by the network, we
design synthetic questions that specifically require spatial inference and
visualize the attention weights. We evaluate our model on two published visual
question answering datasets, DAQUAR [1] and VQA [2], and obtain improved
results compared to a strong deep baseline model (iBOWIMG) which concatenates
image and question features to predict the answer [3].","Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
  Visual Question Answering",2015
52,0.422510399,0.000952663,0.000952608,0.000952601,0.000952671,0.000952613,0.095404511,0.000952572,0.235858288,0.240511075,Topic1,"Visual question answering (VQA) has witnessed great progress since May, 2015
as a classic problem unifying visual and textual data into a system. Many
enlightening VQA works explore deep into the image and question encodings and
fusing methods, of which attention is the most effective and infusive
mechanism. Current attention based methods focus on adequate fusion of visual
and textual features, but lack the attention to where people focus to ask
questions about the image. Traditional attention based methods attach a single
value to the feature at each spatial location, which losses many useful
information. To remedy these problems, we propose a general method to perform
saliency-like pre-selection on overlapped region features by the interrelation
of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication
based attention method to capture more competent correlation information
between visual and textual features. We conduct experiments on the large-scale
COCO-VQA dataset and analyze the effectiveness of our model demonstrated by
strong empirical results.","Task-driven Visual Saliency and Attention-based Visual Question
  Answering",2017
53,0.168246363,0.000980584,0.000980666,0.0009807,0.000980641,0.24772787,0.000980679,0.319341624,0.127986127,0.131794745,Topic8,"We present a systematic analysis on the performance of a phonetic recogniser
when the window of input features is not symmetric with respect to the current
frame. The recogniser is based on Context Dependent Deep Neural Networks
(CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the
latency of the system by reducing the number of future feature frames required
to estimate the current output. Our tests performed on the TIMIT database show
that the performance does not degrade when the input window is shifted up to 5
frames in the past compared to common practice (no future frame). This
corresponds to improving the latency by 50 ms in our settings. Our tests also
show that the best results are not obtained with the symmetric window commonly
employed, but with an asymmetric window with eight past and two future context
frames, although this observation should be confirmed on other data sets. The
reduction in latency suggested by our results is critical for specific
applications such as real-time lip synchronisation for tele-presence, but may
also be beneficial in general applications to improve the lag in human-machine
spoken interaction.","Optimising The Input Window Alignment in CD-DNN Based Phoneme
  Recognition for Low Latency Processing",2016
54,0.001492799,0.001493099,0.034080065,0.001492746,0.001492854,0.001492754,0.001492695,0.32162142,0.42272293,0.212618639,Topic9,"Recently, the long short-term memory neural network (LSTM) has attracted wide
interest due to its success in many tasks. LSTM architecture consists of a
memory cell and three gates, which looks similar to the neuronal networks in
the brain. However, there still lacks the evidence of the cognitive
plausibility of LSTM architecture as well as its working mechanism. In this
paper, we study the cognitive plausibility of LSTM by aligning its internal
architecture with the brain activity observed via fMRI when the subjects read a
story. Experiment results show that the artificial memory vector in LSTM can
accurately predict the observed sequential brain activities, indicating the
correlation between LSTM architecture and the cognitive process of story
reading.",Bridging LSTM Architecture and the Neural Dynamics during Reading,2016
55,0.001639709,0.001639834,0.0016398,0.001639737,0.001639697,0.070159301,0.001639765,0.001639975,0.607384055,0.310978127,Topic9,"This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform ""weight tuning"" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.",Feature Weight Tuning for Recursive Neural Networks,2014
56,0.041579428,0.000735574,0.000735482,0.000735485,0.211213667,0.000735507,0.000735493,0.000735462,0.335989614,0.406804287,Topic10,"One essential task in information extraction from the medical corpus is drug
name recognition. Compared with text sources come from other domains, the
medical text is special and has unique characteristics. In addition, the
medical text mining poses more challenges, e.g., more unstructured text, the
fast growing of new terms addition, a wide range of name variation for the same
drug. The mining is even more challenging due to the lack of labeled dataset
sources and external knowledge, as well as multiple token representations for a
single drug name that is more common in the real application setting. Although
many approaches have been proposed to overwhelm the task, some problems
remained with poor F-score performance (less than 0.75). This paper presents a
new treatment in data representation techniques to overcome some of those
challenges. We propose three data representation techniques based on the
characteristics of word distribution and word similarities as a result of word
embedding training. The first technique is evaluated with the standard NN
model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two
deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked
Denoising Encoders). The third technique represents the sentence as a sequence
that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term
Memory). In extracting the drug name entities, the third technique gives the
best F-score performance compared to the state of the art, with its average
F-score being 0.8645.","A New Data Representation Based on Training Data Characteristics to
  Extract Drug Named-Entity in Medical Text",2016
57,0.058257879,0.000877388,0.000877368,0.000877465,0.110275357,0.000877426,0.320460734,0.000877407,0.000877516,0.505741461,Topic10,"Writing rap lyrics requires both creativity to construct a meaningful,
interesting story and lyrical skills to produce complex rhyme patterns, which
form the cornerstone of good flow. We present a rap lyrics generation method
that captures both of these aspects. First, we develop a prediction model to
identify the next line of existing lyrics from a set of candidate next lines.
This model is based on two machine-learning techniques: the RankSVM algorithm
and a deep neural network model with a novel structure. Results show that the
prediction model can identify the true next line among 299 randomly selected
lines with an accuracy of 17%, i.e., over 50 times more likely than by random.
Second, we employ the prediction model to combine lines from existing songs,
producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics
shows that in terms of quantitative rhyme density, the method outperforms the
best human rappers by 21%. The rap lyrics generator has been deployed as an
online tool called DeepBeat, and the performance of the tool has been assessed
by analyzing its usage logs. This analysis shows that machine-learned rankings
correlate with user preferences.",DopeLearning: A Computational Approach to Rap Lyrics Generation,2015
58,0.000917692,0.026395244,0.044221164,0.279914952,0.247801448,0.000917726,0.00091771,0.000917677,0.09712157,0.300874818,Topic10,"Semantic matching, which aims to determine the matching degree between two
texts, is a fundamental problem for many NLP applications. Recently, deep
learning approach has been applied to this problem and significant improvements
have been achieved. In this paper, we propose to view the generation of the
global interaction between two texts as a recursive process: i.e. the
interaction of two texts at each position is a composition of the interactions
between their prefixes as well as the word level interaction at the current
position. Based on this idea, we propose a novel deep architecture, namely
Match-SRNN, to model the recursive matching structure. Firstly, a tensor is
constructed to capture the word level interactions. Then a spatial RNN is
applied to integrate the local interactions recursively, with importance
determined by four types of gates. Finally, the matching score is calculated
based on the global interaction. We show that, after degenerated to the exact
matching scenario, Match-SRNN can approximate the dynamic programming process
of longest common subsequence. Thus, there exists a clear interpretation for
Match-SRNN. Our experiments on two semantic matching tasks showed the
effectiveness of Match-SRNN, and its ability of visualizing the learned
matching structure.",Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN,2016
59,0.001111593,0.001111478,0.001111271,0.001111326,0.001111505,0.68588605,0.001111279,0.001111315,0.001111488,0.305222696,Topic6,"Advances in neural variational inference have facilitated the learning of
powerful directed graphical models with continuous latent variables, such as
variational autoencoders. The hope is that such models will learn to represent
rich, multi-modal latent factors in real-world data, such as natural language
text. However, current models often assume simplistic priors on the latent
variables - such as the uni-modal Gaussian distribution - which are incapable
of representing complex latent factors efficiently. To overcome this
restriction, we propose the simple, but highly flexible, piecewise constant
distribution. This distribution has the capacity to represent an exponential
number of modes of a latent target distribution, while remaining mathematically
tractable. Our results demonstrate that incorporating this new latent
distribution into different models yields substantial improvements in natural
language processing tasks such as document modeling and natural language
generation for dialogue.",Piecewise Latent Variables for Neural Variational Text Processing,2016
60,0.00126613,0.001266077,0.001266018,0.001266035,0.001266242,0.037086302,0.001266157,0.001266131,0.607328468,0.34672244,Topic9,"Recurrent Neural Networks (RNNs) have become increasingly popular for the
task of language understanding. In this task, a semantic tagger is deployed to
associate a semantic label to each word in an input sequence. The success of
RNN may be attributed to its ability to memorize long-term dependence that
relates the current-time semantic label prediction to the observations many
time instances away. However, the memory capacity of simple RNNs is limited
because of the gradient vanishing and exploding problem. We propose to use an
external memory to improve memorization capability of RNNs. We conducted
experiments on the ATIS dataset, and observed that the proposed model was able
to achieve the state-of-the-art results. We compare our proposed model with
alternative models and report analysis results that may provide insights for
future research.","Recurrent Neural Networks with External Memory for Language
  Understanding",2015
61,0.002084044,0.002083877,0.002083901,0.002083759,0.002083829,0.158957796,0.002083789,0.002083763,0.081180588,0.745274654,Topic10,"We present a novel response generation system that can be trained end to end
on large quantities of unstructured Twitter conversations. A neural network
architecture is used to address sparsity issues that arise when integrating
contextual information into classic statistical models, allowing the system to
take into account previous dialog utterances. Our dynamic-context generative
models show consistent gains over both context-sensitive and
non-context-sensitive Machine Translation and Information Retrieval baselines.","A Neural Network Approach to Context-Sensitive Generation of
  Conversational Responses",2015
62,0.001562889,0.001562775,0.001562799,0.028552775,0.001563055,0.001562805,0.043707244,0.001562876,0.134311775,0.784051008,Topic10,"This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.","The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured
  Multi-Turn Dialogue Systems",2015
63,0.102250316,0.001470908,0.001470747,0.001470832,0.001471304,0.001470889,0.001470825,0.001471288,0.088065348,0.799387542,Topic10,"We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.","Building End-To-End Dialogue Systems Using Generative Hierarchical
  Neural Network Models",2015
64,0.09295365,0.001000175,0.001000444,0.001000279,0.001000206,0.001000325,0.001000226,0.001000226,0.282457566,0.617586902,Topic10,"Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.",End-to-End Attention-based Large Vocabulary Speech Recognition,2015
65,0.001031129,0.471776567,0.00103103,0.001031073,0.001031152,0.001031097,0.001031084,0.001031097,0.394153907,0.126851865,Topic2,"We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.",Towards Neural Network-based Reasoning,2015
66,0.325877741,0.000961736,0.000961854,0.000961834,0.000961928,0.00096175,0.000961917,0.000961805,0.205960098,0.461429337,Topic10,"We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.","What to talk about and how? Selective Generation using LSTMs with
  Coarse-to-Fine Alignment",2015
67,0.000943616,0.051026405,0.000943503,0.000943508,0.000943544,0.000943526,0.000943551,0.000943516,0.382147004,0.560221827,Topic10,"While most approaches to automatically recognizing entailment relations have
used classifiers employing hand engineered features derived from complex
natural language processing pipelines, in practice their performance has been
only slightly better than bag-of-word pair classifiers using only lexical
similarity. The only attempt so far to build an end-to-end differentiable
neural network for entailment failed to outperform such a simple similarity
classifier. In this paper, we propose a neural model that reads two sentences
to determine entailment using long short-term memory units. We extend this
model with a word-by-word neural attention mechanism that encourages reasoning
over entailments of pairs of words and phrases. Furthermore, we present a
qualitative analysis of attention weights produced by this model, demonstrating
such reasoning capabilities. On a large entailment dataset this model
outperforms the previous best neural model and a classifier with engineered
features by a substantial margin. It is the first generic end-to-end
differentiable system that achieves state-of-the-art accuracy on a textual
entailment dataset.",Reasoning about Entailment with Neural Attention,2015
68,0.000980644,0.000980605,0.000980542,0.000980643,0.00098056,0.000980578,0.000980723,0.126149465,0.575945289,0.291040949,Topic9,"In this paper, we extend the deep long short-term memory (DLSTM) recurrent
neural networks by introducing gated direct connections between memory cells in
adjacent layers. These direct links, called highway connections, enable
unimpeded information flow across different layers and thus alleviate the
gradient vanishing problem when building deeper LSTMs. We further introduce the
latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
history while keeping the latency under control. Efficient algorithms are
proposed to train these novel networks using both frame and sequence
discriminative criteria. Experiments on the AMI distant speech recognition
(DSR) task indicate that we can train deeper LSTMs and achieve better
improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
$5.3\%$ relative improvement respectively.",Highway Long Short-Term Memory RNNs for Distant Speech Recognition,2015
69,0.000826664,0.08372938,0.000826596,0.000826576,0.000826634,0.000826618,0.000826786,0.18237814,0.329591457,0.399341151,Topic10,"We proposed Neural Enquirer as a neural network architecture to execute a
natural language (NL) query on a knowledge-base (KB) for answers. Basically,
Neural Enquirer finds the distributed representation of a query and then
executes it on knowledge-base tables to obtain the answer as one of the values
in the tables. Unlike similar efforts in end-to-end training of semantic
parsers, Neural Enquirer is fully ""neuralized"": it not only gives
distributional representation of the query and the knowledge-base, but also
realizes the execution of compositional queries as a series of differentiable
operations, with intermediate results (consisting of annotations of the tables
at different levels) saved on multiple layers of memory. Neural Enquirer can be
trained with gradient descent, with which not only the parameters of the
controlling components and semantic parsing component, but also the embeddings
of the tables and query words can be learned from scratch. The training can be
done in an end-to-end fashion, but it can take stronger guidance, e.g., the
step-by-step supervision for complicated queries, and benefit from it. Neural
Enquirer is one step towards building neural network systems which seek to
understand language by executing it on real-world. Our experiments show that
Neural Enquirer can learn to execute fairly complicated NL queries on tables
with rich structures.",Neural Enquirer: Learning to Query Tables with Natural Language,2015
70,0.001031212,0.028809002,0.001031106,0.001031074,0.001031163,0.001031177,0.040555284,0.001031165,0.122607875,0.801840942,Topic10,"We review the task of Sentence Pair Scoring, popular in the literature in
various forms - viewed as Answer Sentence Selection, Semantic Text Scoring,
Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a
component of Memory Networks.
  We argue that all such tasks are similar from the model perspective and
propose new baselines by comparing the performance of common IR metrics and
popular convolutional, recurrent and attention-based neural models across many
Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating
randomized models, propose a statistically grounded methodology, and attempt to
improve comparisons by releasing new datasets that are much harder than some of
the currently used well explored benchmarks. We introduce a unified open source
software framework with easily pluggable models and tasks, which enables us to
experiment with multi-task reusability of trained sentence model. We set a new
state-of-art in performance on the Ubuntu Dialogue dataset.",Sentence Pair Scoring: Towards Unified Framework for Text Comprehension,2016
71,0.000971175,0.075252842,0.000971112,0.000971105,0.056665427,0.000971212,0.000971163,0.000971094,0.271697014,0.590557855,Topic10,"We address an important problem in sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which certain segments in the input sequence are
selectively replicated in the output sequence. A similar phenomenon is
observable in human language communication. For example, humans tend to repeat
entity names or even long phrases in conversation. The challenge with regard to
copying in Seq2Seq is that new machinery is needed to decide when to perform
the operation. In this paper, we incorporate copying into neural network-based
Seq2Seq learning and propose a new model called CopyNet with encoder-decoder
structure. CopyNet can nicely integrate the regular way of word generation in
the decoder with the new copying mechanism which can choose sub-sequences in
the input sequence and put them at proper places in the output sequence. Our
empirical study on both synthetic data sets and real world data sets
demonstrates the efficacy of CopyNet. For example, CopyNet can outperform
regular RNN-based model with remarkable margins on text summarization tasks.",Incorporating Copying Mechanism in Sequence-to-Sequence Learning,2016
72,0.001123916,0.001123897,0.001123794,0.001123777,0.001123973,0.001123783,0.001123858,0.128821988,0.001123957,0.862187056,Topic10,"Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.","Generating Factoid Questions With Recurrent Neural Networks: The 30M
  Factoid Question-Answer Corpus",2016
73,0.001695435,0.001695284,0.001695275,0.001695136,0.001695426,0.001695186,0.00169516,0.001695268,0.001695224,0.984742606,Topic10,"We investigate evaluation metrics for dialogue response generation systems
where supervised labels, such as task completion, are not available. Recent
works in response generation have adopted metrics from machine translation to
compare a model's generated response to a single target response. We show that
these metrics correlate very weakly with human judgements in the non-technical
Twitter domain, and not at all in the technical Ubuntu domain. We provide
quantitative and qualitative results highlighting specific weaknesses in
existing metrics, and provide recommendations for future development of better
automatic evaluation metrics for dialogue systems.","How NOT To Evaluate Your Dialogue System: An Empirical Study of
  Unsupervised Evaluation Metrics for Dialogue Response Generation",2016
74,0.001515635,0.001515395,0.001515365,0.001515395,0.001515536,0.263936997,0.001515497,0.001515492,0.290921017,0.434533672,Topic10,"Sequential data often possesses a hierarchical structure with complex
dependencies between subsequences, such as found between the utterances in a
dialogue. In an effort to model this kind of generative process, we propose a
neural network-based generative architecture, with latent stochastic variables
that span a variable number of time steps. We apply the proposed model to the
task of dialogue response generation and compare it with recent neural network
architectures. We evaluate the model performance through automatic evaluation
metrics and by carrying out a human evaluation. The experiments demonstrate
that our model improves upon recently proposed models and that the latent
variables facilitate the generation of long outputs and maintain the context.","A Hierarchical Latent Variable Encoder-Decoder Model for Generating
  Dialogues",2016
75,0.001163156,0.001163148,0.001163034,0.001163024,0.001163013,0.001163115,0.041040066,0.001162991,0.639266667,0.311551785,Topic9,"Many important NLP problems can be posed as dual-sequence or
sequence-to-sequence modeling tasks. Recent advances in building end-to-end
neural architectures have been highly successful in solving such tasks. In this
work we propose a new architecture for dual-sequence modeling that is based on
associative memory. We derive AM-RNNs, a recurrent associative memory (AM)
which augments generic recurrent neural networks (RNN). This architecture is
extended to the Dual AM-RNN which operates on two AMs at once. Our models
achieve very competitive results on textual entailment. A qualitative analysis
demonstrates that long range dependencies between source and target-sequence
can be bridged effectively using Dual AM-RNNs. However, an initial experiment
on auto-encoding reveals that these benefits are not exploited by the system
when learning to solve sequence-to-sequence tasks which indicates that
additional supervision or regularization is needed.",Neural Associative Memory for Dual-Sequence Modeling,2016
76,0.000813173,0.000813284,0.000813214,0.000813127,0.000813199,0.345238437,0.000813175,0.000813155,0.442055202,0.207014033,Topic9,"We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural
Networks that replaces the softmax output layer by a log-linear output layer,
of which the softmax is a special case. This conceptually simple move has two
main advantages. First, it allows the learner to combat training data sparsity
by allowing it to model words (or more generally, output symbols) as complex
combinations of attributes without requiring that each combination is directly
observed in the training data (as the softmax does). Second, it permits the
inclusion of flexible prior knowledge in the form of a priori specified modular
features, where the neural network component learns to dynamically control the
weights of a log-linear distribution exploiting these features.
  We conduct experiments in the domain of language modelling of French, that
exploit morphological prior knowledge and show an important decrease in
perplexity relative to a baseline RNN.
  We provide other motivating iillustrations, and finally argue that the
log-linear and the neural-network components contribute complementary strengths
to the LL-RNN: the LL aspect allows the model to incorporate rich prior
knowledge, while the NN aspect, according to the ""representation learning""
paradigm, allows the model to discover novel combination of characteristics.","Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior
  Knowledge",2016
77,0.001408887,0.001408746,0.001408799,0.001408671,0.001408922,0.001408782,0.294563505,0.001408783,0.001408905,0.694166001,Topic10,"There is a practically unlimited amount of natural language data available.
Still, recent work in text comprehension has focused on datasets which are
small relative to current computing possibilities. This article is making a
case for the community to move to larger data and as a step in that direction
it is proposing the BookTest, a new dataset similar to the popular Children's
Book Test (CBT), however more than 60 times larger. We show that training on
the new data improves the accuracy of our Attention-Sum Reader model on the
original CBT test data by a much larger margin than many recent attempts to
improve the model architecture. On one version of the dataset our ensemble even
exceeds the human baseline provided by Facebook. We then show in our own human
study that there is still space for further improvement.",Embracing data abundance: BookTest Dataset for Reading Comprehension,2016
78,0.001136556,0.001136539,0.001136594,0.00113652,0.001136533,0.001136656,0.104880292,0.00113656,0.640350567,0.246813182,Topic9,"Recurrent neural networks are a powerful tool for modeling sequential data,
but the dependence of each timestep's computation on the previous timestep's
output limits parallelism and makes RNNs unwieldy for very long sequences. We
introduce quasi-recurrent neural networks (QRNNs), an approach to neural
sequence modeling that alternates convolutional layers, which apply in parallel
across timesteps, and a minimalist recurrent pooling function that applies in
parallel across channels. Despite lacking trainable recurrent layers, stacked
QRNNs have better predictive accuracy than stacked LSTMs of the same hidden
size. Due to their increased parallelism, they are up to 16 times faster at
train and test time. Experiments on language modeling, sentiment
classification, and character-level neural machine translation demonstrate
these advantages and underline the viability of QRNNs as a basic building block
for a variety of sequence tasks.",Quasi-Recurrent Neural Networks,2016
79,0.001282302,0.129680795,0.057971181,0.068485969,0.001282326,0.00128244,0.001282378,0.001282341,0.653327142,0.084123126,Topic9,"There exist many problem domains where the interpretability of neural network
models is essential for deployment. Here we introduce a recurrent architecture
composed of input-switched affine transformations - in other words an RNN
without any explicit nonlinearities, but with input-dependent recurrent
weights. This simple form allows the RNN to be analyzed via straightforward
linear methods: we can exactly characterize the linear contribution of each
input to the model predictions; we can use a change-of-basis to disentangle
input, output, and computational hidden unit subspaces; we can fully
reverse-engineer the architecture's solution to a simple task. Despite this
ease of interpretation, the input switched affine network achieves reasonable
performance on a text modeling tasks, and allows greater computational
efficiency than networks with standard nonlinearities.","Input Switched Affine Networks: An RNN Architecture Designed for
  Interpretability",2016
80,0.000840564,0.000840507,0.000840506,0.000840482,0.000840583,0.000840688,0.000840527,0.000840538,0.369637355,0.623638251,Topic10,"Neural language models predict the next token using a latent representation
of the immediate token history. Recently, various methods for augmenting neural
language models with an attention mechanism over a differentiable memory have
been proposed. For predicting the next token, these models query information
from a memory of the recent history which can facilitate learning mid- and
long-range dependencies. However, conventional attention mechanisms used in
memory-augmented neural language models produce a single output vector per time
step. This vector is used both for predicting the next token as well as for the
key and value of a differentiable memory of a token history. In this paper, we
propose a neural language model with a key-value attention mechanism that
outputs separate representations for the key and value of a differentiable
memory, as well as for encoding the next-word distribution. This model
outperforms existing memory-augmented neural language models on two corpora.
Yet, we found that our method mainly utilizes a memory of the five most recent
output representations. This led to the unexpected main finding that a much
simpler model based only on the concatenation of recent output representations
from previous time steps is on par with more sophisticated memory-augmented
neural language models.",Frustratingly Short Attention Spans in Neural Language Modeling,2017
81,0.001562902,0.001562824,0.156195909,0.001562833,0.001563078,0.001562799,0.001562829,0.001562873,0.001563031,0.831300922,Topic10,"This paper proposes a new model for extracting an interpretable sentence
embedding by introducing self-attention. Instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of the matrix attending on
a different part of the sentence. We also propose a self-attention mechanism
and a special regularization term for the model. As a side effect, the
embedding comes with an easy way of visualizing what specific parts of the
sentence are encoded into the embedding. We evaluate our model on 3 different
tasks: author profiling, sentiment classification, and textual entailment.
Results show that our model yields a significant performance gain compared to
other sentence embedding methods in all of the 3 tasks.",A Structured Self-attentive Sentence Embedding,2017
82,0.002128332,0.00212836,0.002128056,0.002128089,0.002128116,0.131567771,0.002128134,0.002128291,0.183447572,0.670087278,Topic10,"We introduce an attention-based Bi-LSTM for Chinese implicit discourse
relations and demonstrate that modeling argument pairs as a joint sequence can
outperform word order-agnostic approaches. Our model benefits from a partial
sampling scheme and is conceptually simple, yet achieves state-of-the-art
performance on the Chinese Discourse Treebank. We also visualize its attention
activity to illustrate the model's ability to selectively focus on the relevant
parts of an input sequence.","A Recurrent Neural Model with Attention for the Recognition of Chinese
  Implicit Discourse Relations",2017
83,0.16044889,0.001031311,0.001031255,0.00103116,0.001031176,0.00103121,0.001031248,0.001031199,0.001031229,0.831301323,Topic10,"Automated story generation is the problem of automatically selecting a
sequence of events, actions, or words that can be told as a story. We seek to
develop a system that can generate stories by learning everything it needs to
know from textual story corpora. To date, recurrent neural networks that learn
language models at character, word, or sentence levels have had little success
generating coherent stories. We explore the question of event representations
that provide a mid-level of abstraction between words and sentences in order to
retain the semantic information of the original data while minimizing event
sparsity. We present a technique for preprocessing textual story data into
event sequences. We then present a technique for automated story generation
whereby we decompose the problem into the generation of successive events
(event2event) and the generation of natural language sentences from events
(event2sentence). We give empirical results comparing different event
representations and their effects on event successor generation and the
translation of events to natural language.","Event Representations for Automated Story Generation with Deep Neural
  Nets",2017
84,0.001695616,0.001695268,0.001695211,0.001695247,0.130548892,0.001695439,0.001695317,0.001695374,0.00169554,0.855888094,Topic10,"We propose a generative machine comprehension model that learns jointly to
ask and answer questions based on documents. The proposed model uses a
sequence-to-sequence framework that encodes the document and generates a
question (answer) given an answer (question). Significant improvement in model
performance is observed empirically on the SQuAD corpus, confirming our
hypothesis that the model benefits from jointly learning to perform both tasks.
We believe the joint model's novelty offers a new perspective on machine
comprehension beyond architectural engineering, and serves as a first step
towards autonomous information seeking.",A Joint Model for Question Answering and Question Generation,2017
85,0.000709466,0.000709492,0.221166116,0.00070937,0.00070945,0.000709399,0.000709461,0.02651111,0.513040401,0.235025734,Topic9,"Model compression is significant for the wide adoption of Recurrent Neural
Networks (RNNs) in both user devices possessing limited resources and business
clusters requiring quick responses to large-scale service requests. This work
aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the
sizes of basic structures within LSTM units, including input updates, gates,
hidden states, cell states and outputs. Independently reducing the sizes of
basic structures can result in inconsistent dimensions among them, and
consequently, end up with invalid LSTM units. To overcome the problem, we
propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS
will simultaneously decrease the sizes of all basic structures by one and
thereby always maintain the dimension consistency. By learning ISS within LSTM
units, the obtained LSTMs remain regular while having much smaller basic
structures. Based on group Lasso regularization, our method achieves 10.59x
speedup without losing any perplexity of a language modeling of Penn TreeBank
dataset. It is also successfully evaluated through a compact model with only
2.69M weights for machine Question Answering of SQuAD dataset. Our approach is
successfully extended to non- LSTM RNNs, like Recurrent Highway Networks
(RHNs). Our source code is publicly available at
https://github.com/wenwei202/iss-rnns",Learning Intrinsic Sparse Structures within Long Short-Term Memory,2017
86,0.001010418,0.22090575,0.086900017,0.001010277,0.001010285,0.126904612,0.001010391,0.001010219,0.001010362,0.559227669,Topic10,"Representing the semantic relations that exist between two given words (or
entities) is an important first step in a wide-range of NLP applications such
as analogical reasoning, knowledge base completion and relational information
retrieval. A simple, yet surprisingly accurate method for representing a
relation between two words is to compute the vector offset (\PairDiff) between
their corresponding word embeddings. Despite the empirical success, it remains
unclear as to whether \PairDiff is the best operator for obtaining a relational
representation from word embeddings. We conduct a theoretical analysis of
generalised bilinear operators that can be used to measure the $\ell_{2}$
relational distance between two word-pairs. We show that, if the word
embeddings are standardised and uncorrelated, such an operator will be
independent of bilinear terms, and can be simplified to a linear form, where
\PairDiff is a special case. For numerous word embedding types, we empirically
verify the uncorrelation assumption, demonstrating the general applicability of
our theoretical result. Moreover, we experimentally discover \PairDiff from the
bilinear relation composition operator on several benchmark analogy datasets.","Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational
  Compositional Operators for Analogy Detection",2017
87,0.000952663,0.223671662,0.122306366,0.000952621,0.000952653,0.046169096,0.000952582,0.000952511,0.193514024,0.409575821,Topic10,"We propose Object-oriented Neural Programming (OONP), a framework for
semantically parsing documents in specific domains. Basically, OONP reads a
document and parses it into a predesigned object-oriented data structure
(referred to as ontology in this paper) that reflects the domain-specific
semantics of the document. An OONP parser models semantic parsing as a decision
process: a neural net-based Reader sequentially goes through the document, and
during the process it builds and updates an intermediate ontology to summarize
its partial understanding of the text it covers. OONP supports a rich family of
operations (both symbolic and differentiable) for composing the ontology, and a
big variety of forms (both symbolic and differentiable) for representing the
state and the document. An OONP parser can be trained with supervision of
different forms and strength, including supervised learning (SL) ,
reinforcement learning (RL) and hybrid of the two. Our experiments on both
synthetic and real-world document parsing tasks have shown that OONP can learn
to handle fairly complicated ontology with training data of modest sizes.",Object-oriented Neural Programming (OONP) for Document Understanding,2017
88,0.0460012,0.001111388,0.040633946,0.001111289,0.001111373,0.001111379,0.001111293,0.001111286,0.001111529,0.905585315,Topic10,"This paper proposes a novel neural machine reading model for open-domain
question answering at scale. Existing machine comprehension models typically
assume that a short piece of relevant text containing answers is already
identified and given to the models, from which the models are designed to
extract answers. This assumption, however, is not realistic for building a
large-scale open-domain question answering system which requires both deep text
understanding and identifying relevant text from corpus simultaneously.
  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates
both passage ranking and answer extraction in one single framework. A Q&A
system based on this framework allows users to issue an open-domain question
without needing to provide a piece of text that must contain the answer.
Experiments show that the unified NCR model is able to outperform the
states-of-the-art in both retrieval of relevant text and answer extraction.",A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering,2017
89,0.000854864,0.042221769,0.000854842,0.00085492,0.000854844,0.000854964,0.000854911,0.000854899,0.950938998,0.000854989,Topic9,"Speech recognition is largely taking advantage of deep learning, showing that
substantial benefits can be obtained by modern Recurrent Neural Networks
(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which
typically reach state-of-the-art performance in many tasks thanks to their
ability to learn long-term dependencies and robustness to vanishing gradients.
Nevertheless, LSTMs have a rather complex design with three multiplicative
gates, that might impair their efficient implementation. An attempt to simplify
LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just
two multiplicative gates.
  This paper builds on these efforts by further revising GRUs and proposing a
simplified architecture potentially more suitable for speech recognition. The
contribution of this work is two-fold. First, we suggest to remove the reset
gate in the GRU design, resulting in a more efficient single-gate architecture.
Second, we propose to replace tanh with ReLU activations in the state update
equations. Results show that, in our implementation, the revised architecture
reduces the per-epoch training time with more than 30% and consistently
improves recognition performance across different tasks, input features, and
noisy conditions when compared to a standard GRU.",Improving speech recognition by revising gated recurrent units,2017
90,0.001205175,0.034135622,0.001205177,0.001205067,0.420946389,0.335275422,0.001205086,0.001205087,0.00120517,0.202411805,Topic5,"Training a task-completion dialogue agent with real users via reinforcement
learning (RL) could be prohibitively expensive, because it requires many
interactions with users. One alternative is to resort to a user simulator,
while the discrepancy of between simulated and real users makes the learned
policy unreliable in practice. This paper addresses these challenges by
integrating planning into the dialogue policy learning based on Dyna-Q
framework, and provides a more sample-efficient approach to learn the dialogue
polices. The proposed agent consists of a planner trained on-line with limited
real user experience that can generate large amounts of simulated experience to
supplement with limited real user experience, and a policy model trained on
these hybrid experiences. The effectiveness of our approach is validated on a
movie-booking task in both a simulation setting and a human-in-the-loop
setting.",Integrating planning for task-completion dialogue policy learning,2018
91,0.000613617,0.000613619,0.000613596,0.000613595,0.000613632,0.00061375,0.061963872,0.000613617,0.523458819,0.410281882,Topic9,"Deep neural networks (DNNs) are now a central component of nearly all
state-of-the-art speech recognition systems. Building neural network acoustic
models requires several design decisions including network architecture, size,
and training loss function. This paper offers an empirical investigation on
which aspects of DNN acoustic model design are most important for speech
recognition system performance. We report DNN classifier performance and final
speech recognizer word error rates, and compare DNNs using several metrics to
quantify factors influencing differences in task performance. Our first set of
experiments use the standard Switchboard benchmark corpus, which contains
approximately 300 hours of conversational telephone speech. We compare standard
DNNs to convolutional networks, and present the first experiments using
locally-connected, untied neural networks for acoustic modeling. We
additionally build systems on a corpus of 2,100 hours of training data by
combining the Switchboard and Fisher corpora. This larger corpus allows us to
more thoroughly examine performance of large DNN models -- with up to ten times
more parameters than those typically used in speech recognition systems. Our
results suggest that a relatively simple DNN architecture and optimization
technique produces strong results. These findings, along with previous work,
help establish a set of best practices for building DNN hybrid speech
recognition systems with maximum likelihood training. Our experiments in DNN
optimization additionally serve as a case study for training DNNs with
discriminative loss functions for speech tasks, as well as DNN classifiers more
generally.",Building DNN Acoustic Models for Large Vocabulary Speech Recognition,2014
92,0.091024641,0.001351562,0.001351501,0.00135154,0.001351524,0.026990091,0.001351508,0.001351882,0.400507264,0.473368487,Topic10,"We present a novel deep Recurrent Neural Network (RNN) model for acoustic
modelling in Automatic Speech Recognition (ASR). We term our contribution as a
TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with
Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory
(BLSTM), and a final DNN. The first DNN acts as a feature processor to our
model, the BLSTM then generates a context from the sequence acoustic signal,
and the final DNN takes the context and models the posterior probabilities of
the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ)
eval92 task or more than 8% relative improvement over the baseline DNN models.",Deep Recurrent Neural Networks for Acoustic Modelling,2015
93,0.001282271,0.001282204,0.114253229,0.00128227,0.001282193,0.079505127,0.047466253,0.001282234,0.521021639,0.231342579,Topic9,"We stabilize the activations of Recurrent Neural Networks (RNNs) by
penalizing the squared distance between successive hidden states' norms.
  This penalty term is an effective regularizer for RNNs including LSTMs and
IRNNs, improving performance on character-level language modeling and phoneme
recognition, and outperforming weight noise and dropout.
  We achieve competitive performance (18.6\% PER) on the TIMIT phoneme
recognition task for RNNs evaluated without beam search or an RNN transducer.
  With this penalty term, IRNN can achieve similar performance to LSTM on
language modeling, although adding the penalty term to the LSTM results in
superior performance.
  Our penalty term also prevents the exponential growth of IRNN's activations
outside of their training horizon, allowing them to generalize to much longer
sequences.",Regularizing RNNs by Stabilizing Activations,2015
94,0.089837324,0.080770541,0.000840534,0.000840482,0.000840591,0.000840646,0.070511854,0.000840511,0.637043827,0.117633689,Topic9,"The capacity of a neural network to absorb information is limited by its
number of parameters. Conditional computation, where parts of the network are
active on a per-example basis, has been proposed in theory as a way of
dramatically increasing model capacity without a proportional increase in
computation. In practice, however, there are significant algorithmic and
performance challenges. In this work, we address these challenges and finally
realize the promise of conditional computation, achieving greater than 1000x
improvements in model capacity with only minor losses in computational
efficiency on modern GPU clusters. We introduce a Sparsely-Gated
Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward
sub-networks. A trainable gating network determines a sparse combination of
these experts to use for each example. We apply the MoE to the tasks of
language modeling and machine translation, where model capacity is critical for
absorbing the vast quantities of knowledge available in the training corpora.
We present model architectures in which a MoE with up to 137 billion parameters
is applied convolutionally between stacked LSTM layers. On large language
modeling and machine translation benchmarks, these models achieve significantly
better results than state-of-the-art at lower computational cost.","Outrageously Large Neural Networks: The Sparsely-Gated
  Mixture-of-Experts Layer",2017
95,0.288187665,0.00263203,0.002632233,0.002632138,0.002632108,0.002632457,0.002632411,0.002631983,0.237208102,0.456178871,Topic10,"This work presents a novel objective function for the unsupervised training
of neural network sentence encoders. It exploits signals from paragraph-level
discourse coherence to train these models to understand text. Our objective is
purely discriminative, allowing us to train models many times faster than was
possible under prior methods, and it yields models which perform well in
extrinsic evaluations.","Discourse-Based Objectives for Fast Unsupervised Sentence Representation
  Learning",2017
96,0.125690313,0.000730195,0.000730029,0.000730057,0.000730262,0.000730032,0.000730022,0.000730099,0.418476544,0.450722447,Topic10,"Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
""CNN Inception + Gate"" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.","Learning Convolutional Text Representations for Visual Question
  Answering",2017
97,0.181136646,0.001370187,0.001370129,0.001370051,0.001369989,0.097594149,0.00137009,0.001369989,0.166293043,0.546755728,Topic10,"In this paper, we propose a novel neural network model called RNN
Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN
encodes a sequence of symbols into a fixed-length vector representation, and
the other decodes the representation into another sequence of symbols. The
encoder and decoder of the proposed model are jointly trained to maximize the
conditional probability of a target sequence given a source sequence. The
performance of a statistical machine translation system is empirically found to
improve by using the conditional probabilities of phrase pairs computed by the
RNN Encoder-Decoder as an additional feature in the existing log-linear model.
Qualitatively, we show that the proposed model learns a semantically and
syntactically meaningful representation of linguistic phrases.","Learning Phrase Representations using RNN Encoder-Decoder for
  Statistical Machine Translation",2014
98,0.00093484,0.000934828,0.000934736,0.000934736,0.000934831,0.000934926,0.000934753,0.000934749,0.788646006,0.203875595,Topic9,"Recurrent neural networks (RNNs), particularly long short-term memory (LSTM),
have gained much attention in automatic speech recognition (ASR). Although some
successful stories have been reported, training RNNs remains highly
challenging, especially with limited training data. Recent research found that
a well-trained model can be used as a teacher to train other child models, by
using the predictions generated by the teacher model as supervision. This
knowledge transfer learning has been employed to train simple neural nets with
a complex one, so that the final performance can reach a level that is
infeasible to obtain by regular training. In this paper, we employ the
knowledge transfer learning approach to train RNNs (precisely LSTM) using a
deep neural network (DNN) model as the teacher. This is different from most of
the existing research on knowledge transfer learning, since the teacher (DNN)
is assumed to be weaker than the child (RNN); however, our experiments on an
ASR task showed that it works fairly well: without applying any tricks on the
learning scheme, this approach can train RNNs successfully even with limited
training data.",Recurrent Neural Network Training with Dark Knowledge Transfer,2015
99,0.000990431,0.000990285,0.000990187,0.000990303,0.00099024,0.000990308,0.000990326,0.000990244,0.737063354,0.25501432,Topic9,"Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)
architecture that has been designed to address the vanishing and exploding
gradient problems of conventional RNNs. Unlike feedforward neural networks,
RNNs have cyclic connections making them powerful for modeling sequences. They
have been successfully used for sequence labeling and sequence prediction
tasks, such as handwriting recognition, language modeling, phonetic labeling of
acoustic frames. However, in contrast to the deep neural networks, the use of
RNNs in speech recognition has been limited to phone recognition in small scale
tasks. In this paper, we present novel LSTM based RNN architectures which make
more effective use of model parameters to train acoustic models for large
vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at
various numbers of parameters and configurations. We show that LSTM models
converge quickly and give state of the art speech recognition performance for
relatively small sized models.","Long Short-Term Memory Based Recurrent Neural Network Architectures for
  Large Vocabulary Speech Recognition",2014
100,0.114707084,0.000980782,0.000980645,0.000980825,0.000980714,0.000980695,0.096444014,0.351641742,0.000980557,0.431322942,Topic10,"Based on the Aristotelian concept of potentiality vs. actuality allowing for
the study of energy and dynamics in language, we propose a field approach to
lexical analysis. Falling back on the distributional hypothesis to
statistically model word meaning, we used evolving fields as a metaphor to
express time-dependent changes in a vector space model by a combination of
random indexing and evolving self-organizing maps (ESOM). To monitor semantic
drifts within the observation period, an experiment was carried out on the term
space of a collection of 12.8 million Amazon book reviews. For evaluation, the
semantic consistency of ESOM term clusters was compared with their respective
neighbourhoods in WordNet, and contrasted with distances among term vectors by
random indexing. We found that at 0.05 level of significance, the terms in the
clusters showed a high level of semantic consistency. Tracking the drift of
distributional patterns in the term space across time periods, we found that
consistency decreased, but not at a statistically significant level. Our method
is highly scalable, with interpretations in philosophy.","Monitoring Term Drift Based on Semantic Consistency in an Evolving
  Vector Field",2015
101,0.001389277,0.001389844,0.001389547,0.001389165,0.001389351,0.00138925,0.034825473,0.001389176,0.250898704,0.704550214,Topic10,"The recently proposed Sequence-to-Sequence (seq2seq) framework advocates
replacing complex data processing pipelines, such as an entire automatic speech
recognition system, with a single neural network trained in an end-to-end
fashion. In this contribution, we analyse an attention-based seq2seq speech
recognition system that directly transcribes recordings into characters. We
observe two shortcomings: overconfidence in its predictions and a tendency to
produce incomplete transcriptions when language models are used. We propose
practical solutions to both problems achieving competitive speaker independent
word error rates on the Wall Street Journal dataset: without separate language
models we reach 10.6% WER, while together with a trigram language model, we
reach 6.7% WER.","Towards better decoding and language model integration in sequence to
  sequence models",2016
102,0.000952657,0.000952783,0.000952514,0.000952535,0.000952535,0.000952713,0.00095267,0.000952497,0.23115423,0.761224865,Topic10,"Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.",Neural Machine Translation by Jointly Learning to Align and Translate,2014
103,0.001562857,0.00156307,0.001562686,0.001562798,0.001562758,0.00156275,0.031724876,0.001562716,0.126455232,0.830880256,Topic10,"The authors of (Cho et al., 2014a) have shown that the recently introduced
neural network translation systems suffer from a significant drop in
translation quality when translating long sentences, unlike existing
phrase-based translation systems. In this paper, we propose a way to address
this issue by automatically segmenting an input sentence into phrases that can
be easily translated by the neural network translation model. Once each segment
has been independently translated by the neural machine translation model, the
translated clauses are concatenated to form a final translation. Empirical
results show a significant improvement in translation quality for long
sentences.","Overcoming the Curse of Sentence Length for Neural Machine Translation
  using Automatic Segmentation",2014
104,0.037983192,0.001020516,0.001020517,0.00102052,0.001020561,0.054833623,0.00102056,0.001020554,0.636364373,0.264695584,Topic9,"Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art
results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent
Neural Network (RNN) models have been shown to outperform DNNs counterparts.
However, state-of-the-art DNN and RNN models tend to be impractical to deploy
on embedded systems with limited computational capacity. Traditionally, the
approach for embedded platforms is to either train a small DNN directly, or to
train a small DNN that learns the output distribution of a large DNN. In this
paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We
use the RNN model to generate soft alignments and minimize the Kullback-Leibler
divergence against the small DNN. The small DNN trained on the soft RNN
alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task
compared to a baseline 4.54 WER or more than 13% relative improvement.",Transferring Knowledge from a RNN to a DNN,2015
105,0.24519398,0.000885139,0.325477796,0.000885131,0.000885175,0.055687464,0.000885169,0.000885156,0.329340829,0.039874161,Topic9,"Common Representation Learning (CRL), wherein different descriptions (or
views) of the data are embedded in a common subspace, is receiving a lot of
attention recently. Two popular paradigms here are Canonical Correlation
Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA
based approaches learn a joint representation by maximizing correlation of the
views when projected to the common subspace. AE based methods learn a common
representation by minimizing the error of reconstructing the two views. Each of
these approaches has its own advantages and disadvantages. For example, while
CCA based approaches outperform AE based approaches for the task of transfer
learning, they are not as scalable as the latter. In this work we propose an AE
based approach called Correlational Neural Network (CorrNet), that explicitly
maximizes correlation among the views when projected to the common subspace.
Through a series of experiments, we demonstrate that the proposed CorrNet is
better than the above mentioned approaches with respect to its ability to learn
correlated common representations. Further, we employ CorrNet for several cross
language tasks and show that the representations learned using CorrNet perform
better than the ones learned using other state of the art approaches.",Correlational Neural Networks,2015
106,0.249785713,0.001042,0.001041854,0.00104191,0.00104188,0.001041874,0.001041905,0.001041939,0.37931009,0.363610835,Topic9,"Recurrent sequence generators conditioned on input data through an attention
mechanism have recently shown very good performance on a range of tasks in-
cluding machine translation, handwriting synthesis and image caption gen-
eration. We extend the attention-mechanism with features needed for speech
recognition. We show that while an adaptation of the model used for machine
translation in reaches a competitive 18.7% phoneme error rate (PER) on the
TIMIT phoneme recognition task, it can only be applied to utterances which are
roughly as long as the ones it was trained on. We offer a qualitative
explanation of this failure and propose a novel and generic method of adding
location-awareness to the attention mechanism to alleviate this issue. The new
method yields a model that is robust to long inputs and achieves 18% PER in
single utterances and 20% in 10-times longer (repeated) utterances. Finally, we
propose a change to the at- tention mechanism that prevents it from
concentrating too much on single frames, which further reduces PER to 17.6%
level.",Attention-Based Models for Speech Recognition,2015
107,0.045412472,0.001136492,0.001136471,0.001136556,0.001136508,0.001136773,0.001136538,0.001136525,0.489427144,0.457204521,Topic9,"We have recently shown that deep Long Short-Term Memory (LSTM) recurrent
neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as
acoustic models for speech recognition. More recently, we have shown that the
performance of sequence trained context dependent (CD) hidden Markov model
(HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained
phone models initialized with connectionist temporal classification (CTC). In
this paper, we present techniques that further improve performance of LSTM RNN
acoustic models for large vocabulary speech recognition. We show that frame
stacking and reduced frame rate lead to more accurate models and faster
decoding. CD phone modeling leads to further improvements. We also present
initial results for LSTM RNN models outputting words directly.","Fast and Accurate Recurrent Neural Network Acoustic Models for Speech
  Recognition",2015
108,0.001099159,0.001099164,0.016343417,0.001099145,0.001099002,0.028202003,0.001099067,0.001099074,0.19285523,0.75600474,Topic10,"We present Listen, Attend and Spell (LAS), a neural network that learns to
transcribe speech utterances to characters. Unlike traditional DNN-HMM models,
this model learns all the components of a speech recognizer jointly. Our system
has two components: a listener and a speller. The listener is a pyramidal
recurrent network encoder that accepts filter bank spectra as inputs. The
speller is an attention-based recurrent network decoder that emits characters
as outputs. The network produces character sequences without making any
independence assumptions between the characters. This is the key improvement of
LAS over previous end-to-end CTC models. On a subset of the Google voice search
task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a
language model, and 10.3% with language model rescoring over the top 32 beams.
By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.","Listen, Attend and Spell",2015
109,0.00086231,0.000862172,0.000862243,0.000862272,0.000862214,0.00086232,0.418003299,0.00086218,0.317354324,0.258606664,Topic7,"We propose BlackOut, an approximation algorithm to efficiently train massive
recurrent neural network language models (RNNLMs) with million word
vocabularies. BlackOut is motivated by using a discriminative loss, and we
describe a new sampling strategy which significantly reduces computation while
improving stability, sample efficiency, and rate of convergence. One way to
understand BlackOut is to view it as an extension of the DropOut strategy to
the output layer, wherein we use a discriminative training loss and a weighted
sampling scheme. We also establish close connections between BlackOut,
importance sampling, and noise contrastive estimation (NCE). Our experiments,
on the recently released one billion word language modeling benchmark,
demonstrate scalability and accuracy of BlackOut; we outperform the
state-of-the art, and achieve the lowest perplexity scores on this dataset.
Moreover, unlike other established methods which typically require GPUs or CPU
clusters, we show that a carefully implemented version of BlackOut requires
only 1-10 days on a single machine to train a RNNLM with a million word
vocabulary and billions of parameters on one billion words. Although we
describe BlackOut in the context of RNNLM training, it can be used to any
networks with large softmax output layers.","BlackOut: Speeding up Recurrent Neural Network Language Models With Very
  Large Vocabularies",2015
110,0.001316075,0.001315943,0.001316003,0.00131605,0.001315974,0.001315921,0.023130782,0.001316109,0.240275388,0.727381754,Topic10,"Neural Machine Translation (MT) has reached state-of-the-art results.
However, one of the main challenges that neural MT still faces is dealing with
very large vocabularies and morphologically rich languages. In this paper, we
propose a neural MT system using character-based embeddings in combination with
convolutional and highway layers to replace the standard lookup-based word
representations. The resulting unlimited-vocabulary and affix-aware source word
embeddings are tested in a state-of-the-art neural MT based on an
attention-based bidirectional recurrent neural network. The proposed MT scheme
provides improved results even when the source language is not morphologically
rich. Improvements up to 3 BLEU points are obtained in the German-English WMT
task.",Character-based Neural Machine Translation,2016
111,0.001163034,0.001163024,0.001162957,0.001162954,0.001162956,0.001163233,0.001163042,0.001162916,0.220768062,0.769927821,Topic10,"This paper presents a novel latent variable recurrent neural network
architecture for jointly modeling sequences of words and (possibly latent)
discourse relations between adjacent sentences. A recurrent neural network
generates individual words, thus reaping the benefits of
discriminatively-trained vector representations. The discourse relations are
represented with a latent variable, which can be predicted or marginalized,
depending on the task. The resulting model can therefore employ a training
objective that includes not only discourse relation classification, but also
word prediction. As a result, it outperforms state-of-the-art alternatives for
two tasks: implicit discourse relation classification in the Penn Discourse
Treebank, and dialog act classification in the Switchboard corpus. Furthermore,
by marginalizing over latent discourse relations at test time, we obtain a
discourse informed language model, which improves over a strong LSTM baseline.","A Latent Variable Recurrent Neural Network for Discourse Relation
  Language Models",2016
112,0.001887153,0.001887215,0.001887083,0.001887061,0.001887407,0.001887203,0.001887072,0.001887149,0.469286095,0.515616562,Topic10,"Although highly correlated, speech and speaker recognition have been regarded
as two independent tasks and studied by two communities. This is certainly not
the way that people behave: we decipher both speech content and speaker traits
at the same time. This paper presents a unified model to perform speech and
speaker recognition simultaneously and altogether. The model is based on a
unified neural network where the output of one task is fed to the input of the
other, leading to a multi-task recurrent network. Experiments show that the
joint model outperforms the task-specific models on both the two tasks.",Multi-task Recurrent Model for Speech and Speaker Recognition,2016
113,0.000952591,0.000952628,0.00095255,0.000952526,0.000952563,0.000952704,0.17941492,0.000952569,0.608496868,0.205420081,Topic9,"Memory networks are neural networks with an explicit memory component that
can be both read and written to by the network. The memory is often addressed
in a soft way using a softmax function, making end-to-end training with
backpropagation possible. However, this is not computationally scalable for
applications which require the network to read from extremely large memories.
On the other hand, it is well known that hard attention mechanisms based on
reinforcement learning are challenging to train successfully. In this paper, we
explore a form of hierarchical memory network, which can be considered as a
hybrid between hard and soft attention memory networks. The memory is organized
in a hierarchical structure such that reading from it is done with less
computation than soft attention over a flat memory, while also being easier to
train than hard attention over a flat memory. Specifically, we propose to
incorporate Maximum Inner Product Search (MIPS) in the training and inference
procedures for our hierarchical memory network. We explore the use of various
state-of-the art approximate MIPS techniques and report results on
SimpleQuestions, a challenging large scale factoid question answering task.",Hierarchical Memory Networks,2016
114,0.001136689,0.001136667,0.001136617,0.001136683,0.001136584,0.055820874,0.001136731,0.001136554,0.326343255,0.609879345,Topic10,"Sequence-to-Sequence (seq2seq) modeling has rapidly become an important
general-purpose NLP tool that has proven effective for many text-generation and
sequence-labeling tasks. Seq2seq builds on deep neural language modeling and
inherits its remarkable accuracy in estimating local, next-word distributions.
In this work, we introduce a model and beam-search training scheme, based on
the work of Daume III and Marcu (2005), that extends seq2seq to learn global
sequence scores. This structured approach avoids classical biases associated
with local training and unifies the training loss with the test-time usage,
while preserving the proven model architecture of seq2seq and its efficient
training approach. We show that our system outperforms a highly-optimized
attention-based seq2seq system and other baselines on three different sequence
to sequence tasks: word ordering, parsing, and machine translation.",Sequence-to-Sequence Learning as Beam-Search Optimization,2016
115,0.001408762,0.001408863,0.001408818,0.001408649,0.309544973,0.001408776,0.001408755,0.001408777,0.369223542,0.311370086,Topic9,"In this work, we present the Grounded Recurrent Neural Network (GRNN), a
recurrent neural network architecture for multi-label prediction which
explicitly ties labels to specific dimensions of the recurrent hidden state (we
call this process ""grounding""). The approach is particularly well-suited for
extracting large numbers of concepts from text. We apply the new model to
address an important problem in healthcare of understanding what medical
concepts are discussed in clinical text. Using a publicly available dataset
derived from Intensive Care Units, we learn to label a patient's diagnoses and
procedures from their discharge summary. Our evaluation shows a clear advantage
to using our proposed architecture over a variety of strong baselines.",Grounded Recurrent Neural Networks,2017
116,0.001020733,0.001020678,0.001020533,0.001020568,0.298935191,0.292071298,0.001020584,0.001020604,0.0010207,0.40184911,Topic10,"Developing a dialogue agent that is capable of making autonomous decisions
and communicating by natural language is one of the long-term goals of machine
learning research. Traditional approaches either rely on hand-crafting a small
state-action set for applying reinforcement learning that is not scalable or
constructing deterministic models for learning dialogue sentences that fail to
capture natural conversational variability. In this paper, we propose a Latent
Intention Dialogue Model (LIDM) that employs a discrete latent variable to
learn underlying dialogue intentions in the framework of neural variational
inference. In a goal-oriented dialogue scenario, these latent intentions can be
interpreted as actions guiding the generation of machine responses, which can
be further refined autonomously by reinforcement learning. The experimental
evaluation of LIDM shows that the model out-performs published benchmarks for
both corpus-based and human evaluation, demonstrating the effectiveness of
discrete latent variable models for learning goal-oriented dialogues.",Latent Intention Dialogue Models,2017
117,0.055610672,0.001315996,0.001316063,0.001316017,0.086943121,0.001316039,0.001316232,0.00131612,0.515564915,0.333984824,Topic9,"End-to-end training of automated speech recognition (ASR) systems requires
massive data and compute resources. We explore transfer learning based on model
adaptation as an approach for training ASR models under constrained GPU memory,
throughput and training data. We conduct several systematic experiments
adapting a Wav2Letter convolutional neural network originally trained for
English ASR to the German language. We show that this technique allows faster
training on consumer-grade resources while requiring less training data in
order to achieve the same accuracy, thereby lowering the cost of training ASR
models in other languages. Model introspection revealed that small adaptations
to the network's weights were sufficient for good performance, especially for
inner layers.",Transfer Learning for Speech Recognition on a Budget,2017
118,0.039419052,0.000893144,0.000892958,0.000893055,0.000893083,0.255672995,0.140688145,0.000893296,0.000893164,0.558861107,Topic10,"State-level minimum Bayes risk (sMBR) training has become the de facto
standard for sequence-level training of speech recognition acoustic models. It
has an elegant formulation using the expectation semiring, and gives large
improvements in word error rate (WER) over models trained solely using
cross-entropy (CE) or connectionist temporal classification (CTC). sMBR
training optimizes the expected number of frames at which the reference and
hypothesized acoustic states differ. It may be preferable to optimize the
expected WER, but WER does not interact well with the expectation semiring, and
previous approaches based on computing expected WER exactly involve expanding
the lattices used during training. In this paper we show how to perform
optimization of the expected WER by sampling paths from the lattices used
during conventional sMBR training. The gradient of the expected WER is itself
an expectation, and so may be approximated using Monte Carlo sampling. We show
experimentally that optimizing WER during acoustic model training gives 5%
relative improvement in WER over a well-tuned sMBR baseline on a 2-channel
query recognition task (Google Home).",Optimizing expected word error rate via sampling for speech recognition,2017
119,0.091161574,0.001667076,0.190571326,0.001666999,0.001666999,0.1231859,0.001667182,0.108055158,0.24871686,0.231640926,Topic9,"In this paper, we consider several compression techniques for the language
modeling problem based on recurrent neural networks (RNNs). It is known that
conventional RNNs, e.g, LSTM-based networks in language modeling, are
characterized with either high space complexity or substantial inference time.
This problem is especially crucial for mobile applications, in which the
constant interaction with the remote server is inappropriate. By using the Penn
Treebank (PTB) dataset we compare pruning, quantization, low-rank
factorization, tensor train decomposition for LSTM networks in terms of model
size and suitability for fast inference.",Neural Networks Compression for Language Modeling,2017
120,0.138940389,0.000694542,0.0006946,0.000694575,0.051187634,0.000694642,0.138826657,0.00069455,0.598842535,0.068729877,Topic9,"Training deep neural networks requires massive amounts of training data, but
for many tasks only limited labeled data is available. This makes weak
supervision attractive, using weak or noisy signals like the output of
heuristic methods or user click-through data for training. In a semi-supervised
setting, we can use a large set of data with weak labels to pretrain a neural
network and then fine-tune the parameters with a small amount of data with true
labels. This feels intuitively sub-optimal as these two independent stages
leave the model unaware about the varying label quality. What if we could
somehow inform the model about the label quality? In this paper, we propose a
semi-supervised learning method where we train two neural networks in a
multi-task fashion: a ""target network"" and a ""confidence network"". The target
network is optimized to perform a given task and is trained using a large set
of unlabeled data that are weakly annotated. We propose to weight the gradient
updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model. We evaluate our learning strategy on two different
tasks: document ranking and sentiment classification. The results demonstrate
that our approach not only enhances the performance compared to the baselines
but also speeds up the learning process from weak labels.","Avoiding Your Teacher's Mistakes: Training Neural Networks with
  Controlled Weak Supervision",2017
121,0.001163038,0.001163092,0.001162945,0.001162996,0.00116306,0.558817469,0.001163096,0.063530712,0.275462001,0.095211592,Topic6,"In statistical dialogue management, the dialogue manager learns a policy that
maps a belief state to an action for the system to perform. Efficient
exploration is key to successful policy optimisation. Current deep
reinforcement learning methods are very promising but rely on epsilon-greedy
exploration, thus subjecting the user to a random choice of action during
learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA)
estimate uncertainties and are sample efficient, leading to better user
experience, but on the expense of a greater computational complexity. This
paper examines approaches to extract uncertainty estimates from deep Q-networks
(DQN) in the context of dialogue management. We perform an extensive benchmark
of deep Bayesian methods to extract uncertainty estimates, namely
Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and
alpha-divergences, combining it with DQN algorithm.","Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy
  Optimisation",2017
122,0.001190849,0.001190785,0.001190794,0.001190881,0.001190725,0.001190884,0.001190953,0.051989686,0.643739374,0.295935068,Topic9,"In this work, we investigate the memory capability of recurrent neural
networks (RNNs), where this capability is defined as a function that maps an
element in a sequence to the current output. We first analyze the system
function of a recurrent neural network (RNN) cell, and provide analytical
results for three RNNs. They are the simple recurrent neural network (SRN), the
long short-term memory (LSTM), and the gated recurrent unit (GRU). Based on the
analysis, we propose a new design to extend the memory length of a cell, and
call it the extended long short-term memory (ELSTM). Next, we present a
dependent bidirectional recurrent neural network (DBRNN) for the
sequence-in-sequence-out (SISO) problem, which is more robust to previous
erroneous predictions. Extensive experiments are carried out on different
language tasks to demonstrate the superiority of our proposed ELSTM and DBRNN
solutions.","On Extended Long Short-term Memory and Dependent Bidirectional Recurrent
  Neural Network",2018
123,0.367193537,0.001408648,0.001408592,0.001408611,0.001408671,0.001408596,0.001408587,0.001408575,0.327289215,0.295656968,Topic1,"In this paper, we propose to employ the convolutional neural network (CNN)
for the image question answering (QA). Our proposed CNN provides an end-to-end
framework with convolutional architectures for learning not only the image and
question representations, but also their inter-modal interactions to produce
the answer. More specifically, our model consists of three CNNs: one image CNN
to encode the image content, one sentence CNN to compose the words of the
question, and one multimodal convolution layer to learn their joint
representation for the classification in the space of candidate answer words.
We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA
datasets, which are two benchmark datasets for the image QA, with the
performances significantly outperforming the state-of-the-art.","Learning to Answer Questions From Image Using Convolutional Neural
  Network",2015
124,0.287029279,0.052765577,0.001351564,0.001351631,0.001351698,0.00135152,0.00135162,0.079727888,0.169910091,0.403809131,Topic10,"This paper presents stacked attention networks (SANs) that learn to answer
natural language questions from images. SANs use semantic representation of a
question as query to search for the regions in an image that are related to the
answer. We argue that image question answering (QA) often requires multiple
steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an
image multiple times to infer the answer progressively. Experiments conducted
on four image QA data sets demonstrate that the proposed SANs significantly
outperform previous state-of-the-art approaches. The visualization of the
attention layers illustrates the progress that the SAN locates the relevant
visual clues that lead to the answer of the question layer-by-layer.",Stacked Attention Networks for Image Question Answering,2015
125,0.187824672,0.001136751,0.001136545,0.001136805,0.001136806,0.001136562,0.001136541,0.001136681,0.407833051,0.396385586,Topic9,"Visual question answering is fundamentally compositional in nature---a
question like ""where is the dog?"" shares substructure with questions like ""what
color is the dog?"" and ""where is the cat?"" This paper seeks to simultaneously
exploit the representational capacity of deep networks and the compositional
linguistic structure of questions. We describe a procedure for constructing and
learning *neural module networks*, which compose collections of jointly-trained
neural ""modules"" into deep networks for question answering. Our approach
decomposes questions into their linguistic substructures, and uses these
structures to dynamically instantiate modular networks (with reusable
components for recognizing dogs, classifying colors, etc.). The resulting
compound networks are jointly trained. We evaluate our approach on two
challenging datasets for visual question answering, achieving state-of-the-art
results on both the VQA natural image dataset and a new dataset of complex
questions about abstract shapes.",Neural Module Networks,2015
126,0.234456701,0.299750285,0.000847767,0.000847711,0.000847746,0.000847727,0.000847774,0.152958479,0.061640599,0.24695521,Topic2,"In this paper, we extend a symbolic association framework for being able to
handle missing elements in multimodal sequences. The general scope of the work
is the symbolic associations of object-word mappings as it happens in language
development in infants. In other words, two different representations of the
same abstract concepts can associate in both directions. This scenario has been
long interested in Artificial Intelligence, Psychology, and Neuroscience. In
this work, we extend a recent approach for multimodal sequences (visual and
audio) to also cope with missing elements in one or both modalities. Our method
uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based
on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We
propose to include an extra step for the combination with the max operation for
exploiting the common elements between both sequences. The motivation behind is
that the combination acts as a condition selector for choosing the best
representation from both LSTMs. We evaluated the proposed extension in the
following scenarios: missing elements in one modality (visual or audio) and
missing elements in both modalities (visual and sound). The performance of our
extension reaches better results than the original model and similar results to
individual LSTM trained in each modality.","Symbol Grounding Association in Multimodal Sequences with Missing
  Elements",2015
127,0.451338974,0.001020641,0.072913922,0.00102073,0.284155635,0.001020721,0.001020697,0.001020591,0.15511862,0.031369469,Topic1,"The growing importance of massive datasets with the advent of deep learning
makes robustness to label noise a critical property for classifiers to have.
Sources of label noise include automatic labeling for large datasets,
non-expert labeling, and label corruption by data poisoning adversaries. In the
latter case, corruptions may be arbitrarily bad, even so bad that a classifier
predicts the wrong labels with high confidence. To protect against such sources
of noise, we leverage the fact that a small set of clean labels is often easy
to procure. We demonstrate that robustness to label noise up to severe
strengths can be achieved by using a set of trusted data with clean labels, and
propose a loss correction that utilizes trusted examples in a data-efficient
manner to mitigate the effects of label noise on deep neural network
classifiers. Across vision and natural language processing tasks, we experiment
with various label noises at several strengths, and show that our method
significantly outperforms existing methods.","Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe
  Noise",2018
128,0.12862468,0.001234942,0.001234753,0.001234763,0.00123484,0.062232948,0.001234865,0.001234854,0.556486229,0.245247126,Topic9,"Whereas deep neural networks were first mostly used for classification tasks,
they are rapidly expanding in the realm of structured output problems, where
the observed target is composed of multiple random variables that have a rich
joint distribution, given the input. We focus in this paper on the case where
the input also has a rich structure and the input and output structures are
somehow related. We describe systems that learn to attend to different places
in the input, for each element of the output, for a variety of tasks: machine
translation, image caption generation, video clip description and speech
recognition. All these systems are based on a shared set of building blocks:
gated recurrent neural networks and convolutional neural networks, along with
trained attention mechanisms. We report on experimental results with these
systems, showing impressively good performance and the advantage of the
attention mechanism.","Describing Multimedia Content using Attention-based Encoder--Decoder
  Networks",2015
129,0.431467181,0.001587637,0.001587576,0.001587556,0.001587578,0.001587584,0.001587497,0.001587532,0.00158767,0.555832188,Topic10,"In this paper we present an approach to multi-language image description
bringing together insights from neural machine translation and neural image
description. To create a description of an image for a given target language,
our sequence generation models condition on feature vectors from the image, the
description from the source language, and/or a multimodal vector computed over
the image and a description in the source language. In image description
experiments on the IAPR-TC12 dataset of images aligned with English and German
sentences, we find significant and substantial improvements in BLEU4 and Meteor
scores for models trained over multiple languages, compared to a monolingual
baseline.",Multilingual Image Description with Neural Sequence Models,2015
130,0.14607377,0.001111354,0.001111315,0.001111465,0.06342149,0.001111334,0.00111137,0.109847512,0.353763851,0.321336539,Topic9,"This paper introduces the visually informed embedding of word (VIEW), a
continuous vector representation for a word extracted from a deep neural model
trained using the Microsoft COCO data set to forecast the spatial arrangements
between visual objects, given a textual description. The model is composed of a
deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory
(LSTM) network, the latter being preceded by an embedding layer. The VIEW is
applied to transferring multimodal background knowledge to Spatial Role
Labeling (SpRL) algorithms, which recognize spatial relations between objects
mentioned in the text. This work also contributes with a new method to select
complementary features and a fine-tuning method for MLP that improves the $F1$
measure in classifying the words into spatial roles. The VIEW is evaluated with
the Task 3 of SemEval-2013 benchmark data set, SpaceEval.",Deep Embedding for Spatial Role Labeling,2016
131,0.31791824,0.04561819,0.113300737,0.001205085,0.001205108,0.001205116,0.001205126,0.001205161,0.231702547,0.285434689,Topic1,"We present a neural encoder-decoder model to convert images into
presentational markup based on a scalable coarse-to-fine attention mechanism.
Our method is evaluated in the context of image-to-LaTeX generation, and we
introduce a new dataset of real-world rendered mathematical expressions paired
with LaTeX markup. We show that unlike neural OCR techniques using CTC-based
models, attention-based approaches can tackle this non-standard OCR task. Our
approach outperforms classical mathematical OCR systems by a large margin on
in-domain rendered data, and, with pretraining, also performs well on
out-of-domain handwritten data. To reduce the inference complexity associated
with the attention-based approaches, we introduce a new coarse-to-fine
attention layer that selects a support region before applying attention.",Image-to-Markup Generation with Coarse-to-Fine Attention,2016
132,0.364520276,0.028663548,0.151834134,0.001428827,0.001428882,0.001428797,0.001428777,0.001428945,0.071976303,0.375861512,Topic10,"We present a deep recurrent neural network model with soft visual attention
that learns to generate LaTeX markup of real-world math formulas given their
images. Applying neural sequence generation techniques that have been very
successful in the fields of machine translation and image/handwriting/speech
captioning, recognition, transcription and synthesis, we construct an
image-to-markup model that learns to produce syntactically and semantically
correct LaTeX markup code of over 150 words long and achieves a BLEU score of
89%; the best reported so far for the Im2Latex problem. We also visually
demonstrate that the model learns to scan the image left-right / up-down much
as a human would read it.","Teaching Machines to Code: Neural Markup Generation with Visual
  Attention",2018
133,0.075131713,0.00056515,0.000565116,0.000565088,0.00056513,0.000565082,0.252449801,0.11008239,0.558945483,0.000565046,Topic9,"A promising paradigm for achieving highly efficient deep neural networks is
the idea of evolutionary deep intelligence, which mimics biological evolution
processes to progressively synthesize more efficient networks. A crucial design
factor in evolutionary deep intelligence is the genetic encoding scheme used to
simulate heredity and determine the architectures of offspring networks. In
this study, we take a deeper look at the notion of synaptic cluster-driven
evolution of deep neural networks which guides the evolution process towards
the formation of a highly sparse set of synaptic clusters in offspring
networks. Utilizing a synaptic cluster-driven genetic encoding, the
probabilistic encoding of synaptic traits considers not only individual
synaptic properties but also inter-synaptic relationships within a deep neural
network. This process results in highly sparse offspring networks which are
particularly tailored for parallel computational devices such as GPUs and deep
neural network accelerator chips. Comprehensive experimental results using four
well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and
DetectNet) on two different tasks (object categorization and object detection)
demonstrate the efficiency of the proposed method. Cluster-driven genetic
encoding scheme synthesizes networks that can achieve state-of-the-art
performance with significantly smaller number of synapses than that of the
original ancestor network. ($\sim$125-fold decrease in synapses for MNIST).
Furthermore, the improved cluster efficiency in the generated offspring
networks ($\sim$9.71-fold decrease in clusters for MNIST and a $\sim$8.16-fold
decrease in clusters for KITTI) is particularly useful for accelerated
performance on parallel computing hardware architectures such as those in GPUs
and deep neural network accelerator chips.","Evolution in Groups: A deeper look at synaptic cluster driven evolution
  of deep neural networks",2017
134,0.000714402,0.000714488,0.448102998,0.000714508,0.000714465,0.000714511,0.000714501,0.32285046,0.191724011,0.033035655,Topic3,"A relatively recent advance in cognitive neuroscience has been multi-voxel
pattern analysis (MVPA), which enables researchers to decode brain states
and/or the type of information represented in the brain during a cognitive
operation. MVPA methods utilize machine learning algorithms to distinguish
among types of information or cognitive states represented in the brain, based
on distributed patterns of neural activity. In the current investigation, we
propose a new approach for representation of neural data for pattern analysis,
namely a Mesh Learning Model. In this approach, at each time instant, a star
mesh is formed around each voxel, such that the voxel corresponding to the
center node is surrounded by its p-nearest neighbors. The arc weights of each
mesh are estimated from the voxel intensity values by least squares method. The
estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),
are then used to train a classifier, such as Neural Networks, k-Nearest
Neighbor, Na\""ive Bayes and Support Vector Machines. The proposed Mesh Model
was tested on neuroimaging data acquired via functional magnetic resonance
imaging (fMRI) during a recognition memory experiment using categorized word
lists, employing a previously established experimental paradigm (\""Oztekin &
Badre, 2011). Results suggest that the proposed Mesh Learning approach can
provide an effective algorithm for pattern analysis of brain activity during
cognitive processing.",Mesh Learning for Classifying Cognitive Processes,2012
135,0.001587975,0.001587881,0.001587607,0.001587615,0.001587863,0.320436131,0.001587753,0.001587992,0.666861594,0.001587587,Topic9,"In this work, we perform an exploratory study on synthesizing deep neural
networks using biological synaptic strength distributions, and the potential
influence of different distributions on modelling performance particularly for
the scenario associated with small data sets. Surprisingly, a CNN with
convolutional layer synaptic strengths drawn from biologically-inspired
distributions such as log-normal or correlated center-surround distributions
performed relatively well suggesting a possibility for designing deep neural
network architectures that do not require many data samples to learn, and can
sidestep current training procedures while maintaining or boosting modelling
performance.","Synthesizing Deep Neural Network Architectures using Biological Synaptic
  Strength Distributions",2017
136,0.001205007,0.001205118,0.001204996,0.437236705,0.001205045,0.001205074,0.553123,0.001205087,0.001205013,0.001204955,Topic7,"Addressing the issue of SVMs parameters optimization, this study proposes an
efficient memetic algorithm based on Particle Swarm Optimization algorithm
(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is
responsible for exploration of the search space and the detection of the
potential regions with optimum solutions, while pattern search (PS) is used to
produce an effective exploitation on the potential regions obtained by PSO.
Moreover, a novel probabilistic selection strategy is proposed to select the
appropriate individuals among the current population to undergo local
refinement, keeping a well balance between exploration and exploitation.
Experimental results confirm that the local refinement with PS and our proposed
selection strategy are effective, and finally demonstrate effectiveness and
robustness of the proposed PSO-PS based MA for SVMs parameters optimization.","A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters
  Optimization",2014
137,0.224043307,0.001470894,0.001471175,0.001470918,0.001471062,0.764189063,0.001470941,0.00147078,0.001470944,0.001470917,Topic6,"Unsupervised learning of probabilistic models is a central yet challenging
problem in machine learning. Specifically, designing models with tractable
learning, sampling, inference and evaluation is crucial in solving this task.
We extend the space of such models using real-valued non-volume preserving
(real NVP) transformations, a set of powerful invertible and learnable
transformations, resulting in an unsupervised learning algorithm with exact
log-likelihood computation, exact sampling, exact inference of latent
variables, and an interpretable latent space. We demonstrate its ability to
model natural images on four datasets through sampling, log-likelihood
evaluation and latent variable manipulations.",Density estimation using Real NVP,2016
138,0.001123936,0.001123807,0.001123759,0.001124422,0.001124337,0.318660382,0.495162284,0.17830935,0.001123942,0.001123782,Topic7,"We explore the use of Evolution Strategies (ES), a class of black box
optimization algorithms, as an alternative to popular MDP-based RL techniques
such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show
that ES is a viable solution strategy that scales extremely well with the
number of CPUs available: By using a novel communication strategy based on
common random numbers, our ES implementation only needs to communicate scalars,
making it possible to scale to over a thousand parallel workers. This allows us
to solve 3D humanoid walking in 10 minutes and obtain competitive results on
most Atari games after one hour of training. In addition, we highlight several
advantages of ES as a black box optimization technique: it is invariant to
action frequency and delayed rewards, tolerant of extremely long horizons, and
does not need temporal discounting or value function approximation.",Evolution Strategies as a Scalable Alternative to Reinforcement Learning,2017
139,0.001123831,0.128488043,0.001123768,0.001123824,0.001123807,0.076411471,0.224109588,0.001123781,0.564248067,0.001123819,Topic9,"This paper introduces the QMDP-net, a neural network architecture for
planning under partial observability. The QMDP-net combines the strengths of
model-free learning and model-based planning. It is a recurrent policy network,
but it represents a policy for a parameterized set of tasks by connecting a
model with a planning algorithm that solves the model, thus embedding the
solution structure of planning in a network learning architecture. The QMDP-net
is fully differentiable and allows for end-to-end training. We train a QMDP-net
on different tasks so that it can generalize to new ones in the parameterized
task set and ""transfer"" to other similar tasks beyond the set. In preliminary
experiments, QMDP-net showed strong performance on several robotic tasks in
simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it
sometimes outperforms the QMDP algorithm in the experiments, as a result of
end-to-end learning.",QMDP-Net: Deep Learning for Planning under Partial Observability,2017
140,0.211115314,0.071017305,0.000730034,0.000730071,0.000730103,0.429274,0.057594831,0.000730071,0.227348125,0.000730146,Topic6,"Combining deep model-free reinforcement learning with on-line planning is a
promising approach to building on the successes of deep RL. On-line planning
with look-ahead trees has proven successful in environments where transition
models are known a priori. However, in complex environments where transition
models need to be learned from data, the deficiencies of learned models have
limited their utility for planning. To address these challenges, we propose
TreeQN, a differentiable, recursive, tree-structured model that serves as a
drop-in replacement for any value function network in deep RL with discrete
actions. TreeQN dynamically constructs a tree by recursively applying a
transition model in a learned abstract state space and then aggregating
predicted rewards and state-values using a tree backup to estimate Q-values. We
also propose ATreeC, an actor-critic variant that augments TreeQN with a
softmax layer to form a stochastic policy network. Both approaches are trained
end-to-end, such that the learned model is optimised for its actual use in the
tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a
box-pushing task, as well as n-step DQN and value prediction networks (Oh et
al. 2017) on multiple Atari games. Furthermore, we present ablation studies
that demonstrate the effect of different auxiliary losses on learning
transition models.","TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep
  Reinforcement Learning",2017
141,0.001031146,0.001031187,0.001031205,0.001031124,0.001031222,0.199782101,0.189221874,0.001031148,0.603777797,0.001031194,Topic9,"A major drawback of backpropagation through time (BPTT) is the difficulty of
learning long-term dependencies, coming from having to propagate credit
information backwards through every single step of the forward computation.
This makes BPTT both computationally impractical and biologically implausible.
For this reason, full backpropagation through time is rarely used on long
sequences, and truncated backpropagation through time is used as a heuristic.
However, this usually leads to biased estimates of the gradient in which longer
term dependencies are ignored. Addressing this issue, we propose an alternative
algorithm, Sparse Attentive Backtracking, which might also be related to
principles used by brains to learn long-term dependencies. Sparse Attentive
Backtracking learns an attention mechanism over the hidden states of the past
and selectively backpropagates through paths with high attention weights. This
allows the model to learn long term dependencies while only backtracking for a
small number of time steps, not just from the recent past but also from
attended relevant past states.","Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent
  Networks",2017
142,0.050788838,0.001087195,0.001087493,0.001087369,0.001087141,0.067228238,0.156776069,0.409357106,0.310413406,0.001087144,Topic8,"We study the performance of stochastically trained deep neural networks
(DNNs) whose synaptic weights are implemented using emerging memristive devices
that exhibit limited dynamic range, resolution, and variability in their
programming characteristics. We show that a key device parameter to optimize
the learning efficiency of DNNs is the variability in its programming
characteristics. DNNs with such memristive synapses, even with dynamic range as
low as $15$ and only $32$ discrete levels, when trained based on stochastic
updates suffer less than $3\%$ loss in accuracy compared to floating point
software baseline. We also study the performance of stochastic memristive DNNs
when used as inference engines with noise corrupted data and find that if the
device variability can be minimized, the relative degradation in performance
for the Stochastic DNN is better than that of the software baseline. Hence, our
study presents a new optimization corner for memristive devices for building
large noise-immune deep learning systems.",Stochastic Deep Learning in Memristive Networks,2017
143,0.000943621,0.00094362,0.000943583,0.179894672,0.192864251,0.000943722,0.516496496,0.000943613,0.105082741,0.00094368,Topic7,"Multi-step-ahead time series prediction is one of the most challenging
research topics in the field of time series modeling and prediction, and is
continually under research. Recently, the multiple-input several
multiple-outputs (MISMO) modeling strategy has been proposed as a promising
alternative for multi-step-ahead time series prediction, exhibiting advantages
compared with the two currently dominating strategies, the iterated and the
direct strategies. Built on the established MISMO strategy, this study proposes
a particle swarm optimization (PSO)-based MISMO modeling strategy, which is
capable of determining the number of sub-models in a self-adaptive mode, with
varying prediction horizons. Rather than deriving crisp divides with equal-size
s prediction horizons from the established MISMO, the proposed PSO-MISMO
strategy, implemented with neural networks, employs a heuristic to create
flexible divides with varying sizes of prediction horizons and to generate
corresponding sub-models, providing considerable flexibility in model
construction, which has been validated with simulated and real datasets.",PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction,2013
144,0.008334213,0.008335273,0.008338535,0.008334328,0.008333907,0.563181763,0.008335678,0.008334037,0.370138351,0.008333913,Topic6,"We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.",Norm-Based Capacity Control in Neural Networks,2015
145,0.001695326,0.130842306,0.001695282,0.001695264,0.001695235,0.088414596,0.321968572,0.001695465,0.448602599,0.001695354,Topic9,"The method presented extends a given regression neural network to make its
performance improve. The modification affects the learning procedure only,
hence the extension may be easily omitted during evaluation without any change
in prediction. It means that the modified model may be evaluated as quickly as
the original one but tends to perform better.
  This improvement is possible because the modification gives better expressive
power, provides better behaved gradients and works as a regularization. The
knowledge gained by the temporarily extended neural network is contained in the
parameters shared with the original neural network.
  The only cost is an increase in learning time.","Improving the Performance of Neural Networks in Regression Tasks Using
  Drawering",2016
146,0.001010441,0.161840941,0.001010421,0.001010343,0.001010451,0.513386141,0.001010359,0.001010323,0.317700195,0.001010386,Topic6,"A key element in transfer learning is representation learning; if
representations can be developed that expose the relevant factors underlying
the data, then new tasks and domains can be learned readily based on mappings
of these salient factors. We propose that an important aim for these
representations are to be unbiased. Different forms of representation learning
can be derived from alternative definitions of unwanted bias, e.g., bias to
particular tasks, domains, or irrelevant underlying data dimensions. One very
useful approach to estimating the amount of bias in a representation comes from
maximum mean discrepancy (MMD) [5], a measure of distance between probability
distributions. We are not the first to suggest that MMD can be a useful
criterion in developing representations that apply across multiple domains or
tasks [1]. However, in this paper we describe a number of novel applications of
this criterion that we have devised, all based on the idea of developing
unbiased representations. These formulations include: a standard domain
adaptation framework; a method of learning invariant representations; an
approach based on noise-insensitive autoencoders; and a novel form of
generative model.",Learning unbiased features,2014
147,0.001205134,0.001204973,0.001205031,0.001205098,0.001205031,0.50793473,0.001205118,0.00120508,0.482424771,0.001205035,Topic6,"This paper proposes GProp, a deep reinforcement learning algorithm for
continuous policies with compatible function approximation. The algorithm is
based on two innovations. Firstly, we present a temporal-difference based
method for learning the gradient of the value-function. Secondly, we present
the deviator-actor-critic (DAC) model, which comprises three neural networks
that estimate the value function, its gradient, and determine the actor's
policy respectively. We evaluate GProp on two challenging tasks: a contextual
bandit problem constructed from nonparametric regression datasets that is
designed to probe the ability of reinforcement learning algorithms to
accurately estimate gradients; and the octopus arm, a challenging reinforcement
learning benchmark. GProp is competitive with fully supervised methods on the
bandit task and achieves the best performance to date on the octopus arm.","Compatible Value Gradients for Reinforcement Learning of Continuous Deep
  Policies",2015
148,0.001234745,0.031501855,0.001234872,0.00123481,0.001234781,0.535220101,0.001234882,0.279578495,0.146290696,0.001234764,Topic6,"We propose a particularly structured Boltzmann machine, which we refer to as
a dynamic Boltzmann machine (DyBM), as a stochastic model of a
multi-dimensional time-series. The DyBM can have infinitely many layers of
units but allows exact and efficient inference and learning when its parameters
have a proposed structure. This proposed structure is motivated by postulates
and observations, from biological neural networks, that the synaptic weight is
strengthened or weakened, depending on the timing of spikes (i.e., spike-timing
dependent plasticity or STDP). We show that the learning rule of updating the
parameters of the DyBM in the direction of maximizing the likelihood of given
time-series can be interpreted as STDP with long term potentiation and long
term depression. The learning rule has a guarantee of convergence and can be
performed in a distributed matter (i.e., local in space) with limited memory
(i.e., local in time).","Learning dynamic Boltzmann machines with spike-timing dependent
  plasticity",2015
149,0.001136594,0.183461322,0.00113657,0.001136623,0.246961753,0.001136677,0.079329272,0.001136554,0.37405136,0.110513276,Topic9,"Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures.",Gated Graph Sequence Neural Networks,2015
150,0.000952685,0.000952673,0.000952595,0.000952584,0.124938908,0.582535765,0.175421876,0.030343898,0.000952683,0.081996332,Topic6,"Being able to reason in an environment with a large number of discrete
actions is essential to bringing reinforcement learning to a larger class of
problems. Recommender systems, industrial plants and language models are only
some of the many real-world tasks involving large numbers of discrete actions
for which current methods are difficult or even often impossible to apply. An
ability to generalize over the set of actions as well as sub-linear complexity
relative to the size of the set are both necessary to handle such tasks.
Current approaches are not able to provide both of these, which motivates the
work in this paper. Our proposed approach leverages prior information about the
actions to embed them in a continuous space upon which it can generalize.
Additionally, approximate nearest-neighbor methods allow for logarithmic-time
lookup complexity relative to the number of actions, which is necessary for
time-wise tractable training. This combined approach allows reinforcement
learning methods to be applied to large-scale learning problems previously
intractable with current methods. We demonstrate our algorithm's abilities on a
series of tasks having up to one million actions.",Deep Reinforcement Learning in Large Discrete Action Spaces,2015
151,0.001449603,0.222775157,0.001449501,0.001449522,0.001449579,0.277816045,0.001449824,0.001449471,0.489261621,0.001449676,Topic9,"We introduce the value iteration network (VIN): a fully differentiable neural
network with a `planning module' embedded within. VINs can learn to plan, and
are suitable for predicting outcomes that involve planning-based reasoning,
such as policies for reinforcement learning. Key to our approach is a novel
differentiable approximation of the value-iteration algorithm, which can be
represented as a convolutional neural network, and trained end-to-end using
standard backpropagation. We evaluate VIN based policies on discrete and
continuous path-planning domains, and on a natural-language based search task.
We show that by learning an explicit planning computation, VIN policies
generalize better to new, unseen domains.",Value Iteration Networks,2016
152,0.001492852,0.286778057,0.028672551,0.001492819,0.001492958,0.00149296,0.198668606,0.09764452,0.380771758,0.001492919,Topic9,"Although RNNs have been shown to be powerful tools for processing sequential
data, finding architectures or optimization strategies that allow them to model
very long term dependencies is still an active area of research. In this work,
we carefully analyze two synthetic datasets originally outlined in (Hochreiter
and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store
information over many time steps. We explicitly construct RNN solutions to
these problems, and using these constructions, illuminate both the problems
themselves and the way in which RNNs store different types of information in
their hidden states. These constructions furthermore explain the success of
recent methods that specify unitary initializations or constraints on the
transition matrices.",Recurrent Orthogonal Networks and Long-Memory Tasks,2016
153,0.001562919,0.001562816,0.001562761,0.001563015,0.001562863,0.662581843,0.222899517,0.001562895,0.103578555,0.001562816,Topic6,"Most learning algorithms are not invariant to the scale of the function that
is being approximated. We propose to adaptively normalize the targets used in
learning. This is useful in value-based reinforcement learning, where the
magnitude of appropriate value approximations can change over time when we
update the policy of behavior. Our main motivation is prior work on learning to
play Atari games, where the rewards were all clipped to a predetermined range.
This clipping facilitates learning across many different games with a single
learning algorithm, but a clipped reward function can result in qualitatively
different behavior. Using the adaptive normalization we can remove this
domain-specific heuristic without diminishing overall performance.",Learning values across many orders of magnitude,2016
154,0.001299033,0.001298979,0.001299012,0.001298966,0.631000103,0.001299047,0.101286513,0.001298975,0.198111189,0.061808182,Topic5,"Each human genome is a 3 billion base pair set of encoding instructions.
Decoding the genome using deep learning fundamentally differs from most tasks,
as we do not know the full structure of the data and therefore cannot design
architectures to suit it. As such, architectures that fit the structure of
genomics should be learned not prescribed. Here, we develop a novel search
algorithm, applicable across domains, that discovers an optimal architecture
which simultaneously learns general genomic patterns and identifies the most
important sequence motifs in predicting functional genomic outcomes. The
architectures we find using this algorithm succeed at using only RNA expression
data to predict gene regulatory structure, learn human-interpretable
visualizations of key sequence motifs, and surpass state-of-the-art results on
benchmark genomics challenges.","Genetic Architect: Discovering Genomic Structure with Learned Neural
  Architectures",2016
155,0.092063637,0.00093492,0.000934794,0.025103451,0.000934832,0.635037176,0.000934742,0.000935069,0.242186559,0.000934819,Topic6,"Learning robust value functions given raw observations and rewards is now
possible with model-free and model-based deep reinforcement learning
algorithms. There is a third alternative, called Successor Representations
(SR), which decomposes the value function into two components -- a reward
predictor and a successor map. The successor map represents the expected future
state occupancy from any given state and the reward predictor maps states to
scalar rewards. The value function of a state can be computed as the inner
product between the successor map and the reward weights. In this paper, we
present DSR, which generalizes SR within an end-to-end deep reinforcement
learning framework. DSR has several appealing properties including: increased
sensitivity to distal reward changes due to factorization of reward and world
dynamics, and the ability to extract bottleneck states (subgoals) given
successor maps trained under a random policy. We show the efficacy of our
approach on two diverse environments given raw pixel observations -- simple
grid-world domains (MazeBase) and the Doom game engine.",Deep Successor Reinforcement Learning,2016
156,0.000746469,0.000746598,0.000746411,0.000746496,0.066064483,0.563247826,0.000746495,0.000746464,0.365462346,0.000746413,Topic6,"Deep reinforcement learning (deep RL) has been successful in learning
sophisticated behaviors automatically; however, the learning process requires a
huge number of trials. In contrast, animals can learn new tasks in just a few
trials, benefiting from their prior knowledge about the world. This paper seeks
to bridge this gap. Rather than designing a ""fast"" reinforcement learning
algorithm, we propose to represent it as a recurrent neural network (RNN) and
learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in
the weights of the RNN, which are learned slowly through a general-purpose
(""slow"") RL algorithm. The RNN receives all information a typical RL algorithm
would receive, including observations, actions, rewards, and termination flags;
and it retains its state across episodes in a given Markov Decision Process
(MDP). The activations of the RNN store the state of the ""fast"" RL algorithm on
the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both
small-scale and large-scale problems. On the small-scale side, we train it to
solve randomly generated multi-arm bandit problems and finite MDPs. After
RL$^2$ is trained, its performance on new MDPs is close to human-designed
algorithms with optimality guarantees. On the large-scale side, we test RL$^2$
on a vision-based navigation task and show that it scales up to
high-dimensional problems.",RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning,2016
157,0.001010307,0.001010267,0.001010271,0.001010264,0.001010339,0.087771215,0.001010351,0.001010316,0.9041463,0.001010369,Topic9,"Two potential bottlenecks on the expressiveness of recurrent neural networks
(RNNs) are their ability to store information about the task in their
parameters, and to store information about the input history in their units. We
show experimentally that all common RNN architectures achieve nearly the same
per-task and per-unit capacity bounds with careful training, for a variety of
tasks and stacking depths. They can store an amount of task information which
is linear in the number of parameters, and is approximately 5 bits per
parameter. They can additionally store approximately one real number from their
input history per hidden unit. We further find that for several tasks it is the
per-task parameter capacity bound that determines performance. These results
suggest that many previous results comparing RNN architectures are driven
primarily by differences in training effectiveness, rather than differences in
capacity. Supporting this observation, we compare training difficulty for
several architectures, and show that vanilla RNNs are far more difficult to
train, yet have slightly higher capacity. Finally, we propose two novel RNN
architectures, one of which is easier to train than the LSTM or GRU for deeply
stacked architectures.",Capacity and Trainability in Recurrent Neural Networks,2016
158,0.001250234,0.001250355,0.056365271,0.001250219,0.422731127,0.293571572,0.00125032,0.001250161,0.219830528,0.001250213,Topic5,"In application domains such as healthcare, we want accurate predictive models
that are also causally interpretable. In pursuit of such models, we propose a
causal regularizer to steer predictive models towards causally-interpretable
solutions and theoretically study its properties. In a large-scale analysis of
Electronic Health Records (EHR), our causally-regularized model outperforms its
L1-regularized counterpart in causal accuracy and is competitive in predictive
performance. We perform non-linear causality analysis by causally regularizing
a special neural network architecture. We also show that the proposed causal
regularizer can be used together with neural representation learning algorithms
to yield up to 20% improvement over multilayer perceptron in detecting
multivariate causation, a situation common in healthcare, where many causal
factors should occur simultaneously to have an effect on the target variable.",Causal Regularization,2017
159,0.084044219,0.148695305,0.000662498,0.000662486,0.000662469,0.000662456,0.000662433,0.000662479,0.667186369,0.096099288,Topic9,"Deep neural networks are representation learning techniques. During training,
a deep net is capable of generating a descriptive language of unprecedented
size and detail in machine learning. Extracting the descriptive language coded
within a trained CNN model (in the case of image data), and reusing it for
other purposes is a field of interest, as it provides access to the visual
descriptors previously learnt by the CNN after processing millions of images,
without requiring an expensive training phase. Contributions to this field
(commonly known as feature representation transfer or transfer learning) have
been purely empirical so far, extracting all CNN features from a single layer
close to the output and testing their performance by feeding them to a
classifier. This approach has provided consistent results, although its
relevance is limited to classification tasks. In a completely different
approach, in this paper we statistically measure the discriminative power of
every single feature found within a deep CNN, when used for characterizing
every class of 11 datasets. We seek to provide new insights into the behavior
of CNN features, particularly the ones from convolutional layers, as this can
be relevant for their application to knowledge representation and reasoning.
Our results confirm that low and middle level features may behave differently
to high level features, but only under certain conditions. We find that all CNN
features can be used for knowledge representation purposes both by their
presence or by their absence, doubling the information a single CNN feature may
provide. We also study how much noise these features may include, and propose a
thresholding approach to discard most of it. All these insights have a direct
application to the generation of CNN embedding spaces.",On the Behavior of Convolutional Nets for Feature Extraction,2017
160,0.615609606,0.001075702,0.001075434,0.001075417,0.001075569,0.375786061,0.001075468,0.001075489,0.001075649,0.001075606,Topic1,"Adversarial learning of probabilistic models has recently emerged as a
promising alternative to maximum likelihood. Implicit models such as generative
adversarial networks (GAN) often generate better samples compared to explicit
models trained by maximum likelihood. Yet, GANs sidestep the characterization
of an explicit density which makes quantitative evaluations challenging. To
bridge this gap, we propose Flow-GANs, a generative adversarial network for
which we can perform exact likelihood evaluation, thus supporting both
adversarial and maximum likelihood training. When trained adversarially,
Flow-GANs generate high-quality samples but attain extremely poor
log-likelihood scores, inferior even to a mixture model memorizing the training
data; the opposite is true when trained by maximum likelihood. Results on MNIST
and CIFAR-10 demonstrate that hybrid training can attain high held-out
likelihoods while retaining visual fidelity in the generated samples.","Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in
  Generative Models",2017
161,0.001316114,0.001316112,0.001315947,0.091552431,0.00131594,0.717146184,0.14155622,0.001315918,0.041849107,0.001316026,Topic6,"When used as a surrogate objective for maximum likelihood estimation in
latent variable models, the evidence lower bound (ELBO) produces
state-of-the-art results. Inspired by this, we consider the extension of the
ELBO to a family of lower bounds defined by a particle filter's estimator of
the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs
take the same arguments as the ELBO, but can exploit a model's sequential
structure to form tighter bounds. We present results that relate the tightness
of FIVO's bound to the variance of the particle filter's estimator by
considering the generic case of bounds defined as log-transformed likelihood
estimators. Experimentally, we show that training with FIVO results in
substantial improvements over training the same model architecture with the
ELBO on sequential data.",Filtering Variational Objectives,2017
162,0.001562866,0.001562742,0.001562859,0.001562791,0.125808501,0.682940782,0.001562824,0.001563064,0.180310658,0.001562913,Topic6,"Recent progress in variational inference has paid much attention to the
flexibility of variational posteriors. One promising direction is to use
implicit distributions, i.e., distributions without tractable densities as the
variational posterior. However, existing methods on implicit posteriors still
face challenges of noisy estimation and computational infeasibility when
applied to models with high-dimensional latent variables. In this paper, we
present a new approach named Kernel Implicit Variational Inference that
addresses these challenges. As far as we know, for the first time implicit
variational inference is successfully applied to Bayesian neural networks,
which shows promising results on both regression and classification tasks.",Kernel Implicit Variational Inference,2017
163,0.000787582,0.000787604,0.000787528,0.000787564,0.000787631,0.538139068,0.00078761,0.000787616,0.455560167,0.00078763,Topic6,"Partially observable environments present an important open challenge in the
domain of sequential control learning with delayed rewards. Despite numerous
attempts during the two last decades, the majority of reinforcement learning
algorithms and associated approximate models, applied to this context, still
assume Markovian state transitions. In this paper, we explore the use of a
recently proposed attention-based model, the Gated End-to-End Memory Network,
for sequential control. We call the resulting model the Gated End-to-End Memory
Policy Network. More precisely, we use a model-free value-based algorithm to
learn policies for partially observed domains using this memory-enhanced neural
network. This model is end-to-end learnable and it features unbounded memory.
Indeed, because of its attention mechanism and associated non-parametric
memory, the proposed model allows us to define an attention mechanism over the
observation stream unlike recurrent models. We show encouraging results that
illustrate the capability of our attention-based model in the context of the
continuous-state non-stationary control problem of stock trading. We also
present an OpenAI Gym environment for simulated stock exchange and explain its
relevance as a benchmark for the field of non-Markovian decision process
learning.",Non-Markovian Control with Gated End-to-End Memory Policy Networks,2017
164,0.000645301,0.000645337,0.000645297,0.000645264,0.115630535,0.000645323,0.337773318,0.000645352,0.388667597,0.154056675,Topic9,"Regression or classification? This is perhaps the most basic question faced
when tackling a new supervised learning problem. We present an Evolutionary
Deep Learning (EDL) algorithm that automatically solves this by identifying the
question type with high accuracy, along with a proposed deep architecture.
Typically, a significant amount of human insight and preparation is required
prior to executing machine learning algorithms. For example, when creating deep
neural networks, the number of parameters must be selected in advance and
furthermore, a lot of these choices are made based upon pre-existing knowledge
of the data such as the use of a categorical cross entropy loss function.
Humans are able to study a dataset and decide whether it represents a
classification or a regression problem, and consequently make decisions which
will be applied to the execution of the neural network. We propose the
Automated Problem Identification (API) algorithm, which uses an evolutionary
algorithm interface to TensorFlow to manipulate a deep neural network to decide
if a dataset represents a classification or a regression problem. We test API
on 16 different classification, regression and sentiment analysis datasets with
up to 10,000 features and up to 17,000 unique target values. API achieves an
average accuracy of $96.3\%$ in identifying the problem type without hardcoding
any insights about the general characteristics of regression or classification
problems. For example, API successfully identifies classification problems even
with 1000 target values. Furthermore, the algorithm recommends which loss
function to use and also recommends a neural network architecture. Our work is
therefore a step towards fully automated machine learning.","Automated Problem Identification: Regression vs Classification via
  Evolutionary Deep Networks",2017
165,0.000877402,0.000877486,0.000877363,0.000877333,0.165978811,0.036765446,0.000877463,0.000877371,0.791113895,0.000877431,Topic9,"Deep neural networks excel in regimes with large amounts of data, but tend to
struggle when data is scarce or when they need to adapt quickly to changes in
the task. In response, recent work in meta-learning proposes training a
meta-learner on a distribution of similar tasks, in the hopes of generalization
to novel but related tasks by learning a high-level strategy that captures the
essence of the problem it is asked to solve. However, many recent meta-learning
approaches are extensively hand-designed, either using architectures
specialized to a particular application, or hard-coding algorithmic components
that constrain how the meta-learner solves the task. We propose a class of
simple and generic meta-learner architectures that use a novel combination of
temporal convolutions and soft attention; the former to aggregate information
from past experience and the latter to pinpoint specific pieces of information.
In the most extensive set of meta-learning experiments to date, we evaluate the
resulting Simple Neural AttentIve Learner (or SNAIL) on several
heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement
learning, SNAIL attains state-of-the-art performance by significant margins.",A Simple Neural Attentive Meta-Learner,2017
166,0.000826626,0.09775392,0.00082672,0.000826672,0.000826734,0.466708378,0.07605536,0.00082671,0.354522185,0.000826695,Topic6,"Neural networks are generally built by interleaving (adaptable) linear layers
with (fixed) nonlinear activation functions. To increase their flexibility,
several authors have proposed methods for adapting the activation functions
themselves, endowing them with varying degrees of flexibility. None of these
approaches, however, have gained wide acceptance in practice, and research in
this topic remains open. In this paper, we introduce a novel family of flexible
activation functions that are based on an inexpensive kernel expansion at every
neuron. Leveraging over several properties of kernel-based models, we propose
multiple variations for designing and initializing these kernel activation
functions (KAFs), including a multidimensional scheme allowing to nonlinearly
combine information from different paths in the network. The resulting KAFs can
approximate any mapping defined over a subset of the real line, either convex
or nonconvex. Furthermore, they are smooth over their entire domain, linear in
their parameters, and they can be regularized using any known scheme, including
the use of $\ell_1$ penalties to enforce sparseness. To the best of our
knowledge, no other known model satisfies all these properties simultaneously.
In addition, we provide a relatively complete overview on alternative
techniques for adapting the activation functions, which is currently lacking in
the literature. A large set of experiments validates our proposal.","Kafnets: kernel-based non-parametric activation functions for neural
  networks",2017
167,0.257113729,0.438511281,0.000813139,0.000813169,0.000813253,0.298682645,0.000813193,0.000813205,0.000813206,0.000813181,Topic2,"Conventional wisdom holds that model-based planning is a powerful approach to
sequential decision-making. It is often very challenging in practice, however,
because while a model can be used to evaluate a plan, it does not prescribe how
to construct a plan. Here we introduce the ""Imagination-based Planner"", the
first model-based, sequential decision-making agent that can learn to
construct, evaluate, and execute plans. Before any action, it can perform a
variable number of imagination steps, which involve proposing an imagined
action and evaluating it with its model-based imagination. All imagined actions
and outcomes are aggregated, iteratively, into a ""plan context"" which
conditions future real and imagined actions. The agent can even decide how to
imagine: testing out alternative imagined actions, chaining sequences of
actions together, or building a more complex ""imagination tree"" by navigating
flexibly among the previously imagined states using a learned policy. And our
agent can learn to plan economically, jointly optimizing for external rewards
and computational costs associated with using its imagination. We show that our
architecture can learn to solve a challenging continuous control problem, and
also learn elaborate planning strategies in a discrete maze-solving task. Our
work opens a new direction toward learning the components of a model-based
planning system and how to use them.",Learning model-based planning from scratch,2017
168,0.176492791,0.001299192,0.001299004,0.001299083,0.001299036,0.214868723,0.001299007,0.001298985,0.599545178,0.001299,Topic9,"We propose a recurrent extension of the Ladder networks whose structure is
motivated by the inference required in hierarchical latent variable models. We
demonstrate that the recurrent Ladder is able to handle a wide variety of
complex learning tasks that benefit from iterative inference and temporal
modeling. The architecture shows close-to-optimal results on temporal modeling
of video data, competitive results on music modeling, and improved perceptual
grouping based on higher order abstractions, such as stochastic textures and
motion cues. We present results for fully supervised, semi-supervised, and
unsupervised tasks. The results suggest that the proposed architecture and
principles are powerful tools for learning a hierarchy of abstractions,
learning iterative inference and handling temporal information.",Recurrent Ladder Networks,2017
169,0.001639862,0.351398174,0.001639611,0.001639625,0.001639852,0.192317158,0.208848282,0.001639637,0.237597944,0.001639854,Topic2,"With a direct analysis of neural networks, this paper presents a
mathematically tight generalization theory to partially address an open problem
regarding the generalization of deep learning. Unlike previous bound-based
theory, our main theory is quantitatively as tight as possible for every
dataset individually, while producing qualitative insights competitively. Our
results give insight into why and how deep learning can generalize well,
despite its large capacity, complexity, possible algorithmic instability,
nonrobustness, and sharp minima, answering to an open question in the
literature. We also discuss limitations of our results and propose additional
open problems.",Generalization in Deep Learning,2017
170,0.120975725,0.001639736,0.001639785,0.001639689,0.131507379,0.143648633,0.001639677,0.001639668,0.594029773,0.001639935,Topic9,"It is commonly agreed that the use of relevant invariances as a good
statistical bias is important in machine-learning. However, most approaches
that explicitly incorporate invariances into a model architecture only make use
of very simple transformations, such as translations and rotations. Hence,
there is a need for methods to model and extract richer transformations that
capture much higher-level invariances. To that end, we introduce a tool
allowing to parametrize the set of filters of a trained convolutional neural
network with the latent space of a generative adversarial network. We then show
that the method can capture highly non-linear invariances of the data by
visualizing their effect in the data space.",Parametrizing filters of a CNN with a GAN,2017
171,0.001250274,0.00125023,0.055144951,0.001250261,0.001250156,0.001250247,0.001250433,0.001250219,0.797539499,0.138563729,Topic9,"Long Short-Term Memory (LSTM) is a popular approach to boosting the ability
of Recurrent Neural Networks to store longer term temporal information. The
capacity of an LSTM network can be increased by widening and adding layers.
However, usually the former introduces additional parameters, while the latter
increases the runtime. As an alternative we propose the Tensorized LSTM in
which the hidden states are represented by tensors and updated via a
cross-layer convolution. By increasing the tensor size, the network can be
widened efficiently without additional parameters since the parameters are
shared across different locations in the tensor; by delaying the output, the
network can be deepened implicitly with little additional runtime since deep
computations for each timestep are merged into temporal computations of the
sequence. Experiments conducted on five challenging sequence learning tasks
show the potential of the proposed model.","Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence
  Learning",2017
172,0.000819896,0.000819801,0.000819799,0.000819864,0.000819862,0.033960701,0.00081994,0.256426943,0.703873327,0.000819869,Topic9,"We describe a novel spiking neural network (SNN) for automated, real-time
handwritten digit classification and its implementation on a GP-GPU platform.
Information processing within the network, from feature extraction to
classification is implemented by mimicking the basic aspects of neuronal spike
initiation and propagation in the brain. The feature extraction layer of the
SNN uses fixed synaptic weight maps to extract the key features of the image
and the classifier layer uses the recently developed NormAD approximate
gradient descent based supervised learning algorithm for spiking neural
networks to adjust the synaptic weights. On the standard MNIST database images
of handwritten digits, our network achieves an accuracy of 99.80% on the
training set and 98.06% on the test set, with nearly 7x fewer parameters
compared to the state-of-the-art spiking networks. We further use this network
in a GPU based user-interface system demonstrating real-time SNN simulation to
infer digits written by different users. On a test set of 500 such images, this
real-time platform achieves an accuracy exceeding 97% while making a prediction
within an SNN emulation time of less than 100ms.","Learning and Real-time Classification of Hand-written Digits With
  Spiking Neural Networks",2017
173,0.001149852,0.001149825,0.001149655,0.001149653,0.001149822,0.001149733,0.19955825,0.114288133,0.678105386,0.001149691,Topic9,"Catastrophic forgetting occurs when a neural network loses the information
learned in a previous task after training on subsequent tasks. This problem
remains a hurdle for artificial intelligence systems with sequential learning
capabilities. In this paper, we propose a task-based hard attention mechanism
that preserves previous tasks' information without affecting the current task's
learning. A hard attention mask is learned concurrently to every task, through
stochastic gradient descent, and previous masks are exploited to condition such
learning. We show that the proposed mechanism is effective for reducing
catastrophic forgetting, cutting current rates by 45 to 80%. We also show that
it is robust to different hyperparameter choices, and that it offers a number
of monitoring capabilities. The approach features the possibility to control
both the stability and compactness of the learned knowledge, which we believe
makes it also attractive for online learning or network compression
applications.",Overcoming catastrophic forgetting with hard attention to the task,2018
174,0.001075564,0.001075416,0.001075676,0.062074181,0.148476685,0.472054891,0.001075508,0.001075468,0.310941155,0.001075457,Topic6,"Faced with distribution shift between training and test set, we wish to
detect and quantify the shift, and to correct our classifiers without test set
labels. Motivated by medical diagnosis, where diseases (targets), cause
symptoms (observations), we focus on label shift, where the label marginal
$p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box
Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits
arbitrary black box predictors to reduce dimensionality prior to shift
correction. While better predictors give tighter estimates, BBSE works even
when predictors are biased, inaccurate, or uncalibrated, so long as their
confusion matrices are invertible. We prove BBSE's consistency, bound its
error, and introduce a statistical test that uses BBSE to detect shift. We also
leverage BBSE to correct classifiers. Experiments demonstrate accurate
estimates and improved prediction, even on high-dimensional datasets of natural
images.",Detecting and Correcting for Label Shift with Black Box Predictors,2018
175,0.001000295,0.199230844,0.001000213,0.00100021,0.001000261,0.527254915,0.001000305,0.00100017,0.266512599,0.00100019,Topic6,"This paper introduces a novel measure-theoretic learning theory to analyze
generalization behaviors of practical interest. The proposed learning theory
has the following abilities: 1) to utilize the qualities of each learned
representation on the path from raw inputs to outputs in representation
learning, 2) to guarantee good generalization errors possibly with arbitrarily
rich hypothesis spaces (e.g., arbitrarily large capacity and Rademacher
complexity) and non-stable/non-robust learning algorithms, and 3) to clearly
distinguish each individual problem instance from each other. Our
generalization bounds are relative to a representation of the data, and hold
true even if the representation is learned. We discuss several consequences of
our results on deep learning, one-shot learning and curriculum learning. Unlike
statistical learning theory, the proposed learning theory analyzes each problem
instance individually via measure theory, rather than a set of problem
instances via statistics. Because of the differences in the assumptions and the
objectives, the proposed learning theory is meant to be complementary to
previous learning theory and is not designed to compete with it.",Generalization in Machine Learning via Analytical Learning Theory,2018
176,0.000943692,0.028341584,0.056801451,0.000943594,0.016420051,0.439758458,0.000943685,0.00094355,0.453960336,0.000943599,Topic9,"In practice it is often found that large over-parameterized neural networks
generalize better than their smaller counterparts, an observation that appears
to conflict with classical notions of function complexity, which typically
favor smaller models. In this work, we investigate this tension between
complexity and generalization through an extensive empirical exploration of two
natural metrics of complexity related to sensitivity to input perturbations.
Our experiments survey thousands of models with various fully-connected
architectures, optimizers, and other hyper-parameters, as well as four
different image classification datasets.
  We find that trained neural networks are more robust to input perturbations
in the vicinity of the training data manifold, as measured by the norm of the
input-output Jacobian of the network, and that it correlates well with
generalization. We further establish that factors associated with poor
generalization $-$ such as full-batch training or using random labels $-$
correspond to lower robustness, while factors associated with good
generalization $-$ such as data augmentation and ReLU non-linearities $-$ give
rise to more robust functions. Finally, we demonstrate how the input-output
Jacobian norm can be predictive of generalization at the level of individual
test points.",Sensitivity and Generalization in Neural Networks: an Empirical Study,2018
177,0.000952598,0.000952556,0.036026671,0.000952697,0.000952653,0.000952681,0.134282277,0.00095271,0.802916493,0.021058663,Topic9,"Despite their ability to memorize large datasets, deep neural networks often
achieve good generalization performance. However, the differences between the
learned solutions of networks which generalize and those which do not remain
unclear. Additionally, the tuning properties of single directions (defined as
the activation of a single unit or some linear combination of units in response
to some input) have been highlighted, but their importance has not been
evaluated. Here, we connect these lines of inquiry to demonstrate that a
network's reliance on single directions is a good predictor of its
generalization performance, across networks trained on datasets with different
fractions of corrupted labels, across ensembles of networks trained on datasets
with unmodified labels, across different hyperparameters, and over the course
of training. While dropout only regularizes this quantity up to a point, batch
normalization implicitly discourages single direction reliance, in part by
decreasing the class selectivity of individual units. Finally, we find that
class selectivity is a poor predictor of task importance, suggesting not only
that networks which generalize well minimize their dependence on individual
units by reducing their selectivity, but also that individually selective units
may not be necessary for strong network performance.",On the importance of single directions for generalization,2018
178,0.000901201,0.000901029,0.000901178,0.291662891,0.323536146,0.000901276,0.378492942,0.000901032,0.000901188,0.00090112,Topic7,"Images can be segmented by first using a classifier to predict an affinity
graph that reflects the degree to which image pixels must be grouped together
and then partitioning the graph to yield a segmentation. Machine learning has
been applied to the affinity classifier to produce affinity graphs that are
good in the sense of minimizing edge misclassification rates. However, this
error measure is only indirectly related to the quality of segmentations
produced by ultimately partitioning the affinity graph. We present the first
machine learning algorithm for training a classifier to produce affinity graphs
that are good in the sense of producing segmentations that directly minimize
the Rand index, a well known segmentation performance measure. The Rand index
measures segmentation performance by quantifying the classification of the
connectivity of image pixel pairs after segmentation. By using the simple graph
partitioning algorithm of finding the connected components of the thresholded
affinity graph, we are able to train an affinity classifier to directly
minimize the Rand index of segmentations resulting from the graph partitioning.
Our learning algorithm corresponds to the learning of maximin affinities
between image pixel pairs, which are predictive of the pixel-pair connectivity.",Maximin affinity learning of image segmentation,2009
179,0.001064067,0.001064212,0.001064013,0.098684485,0.001064199,0.001064089,0.001064103,0.606905071,0.286961723,0.001064038,Topic8,"This study is focused on the development of the cortex-like visual object
recognition system. We propose a general framework, which consists of three
hierarchical levels (modules). These modules functionally correspond to the V1,
V4 and IT areas. Both bottom-up and top-down connections between the
hierarchical levels V4 and IT are employed. The higher the degree of matching
between the input and the preferred stimulus, the shorter the response time of
the neuron. Therefore information about a single stimulus is distributed in
time and is transmitted by the waves of spikes. The reciprocal connections and
waves of spikes implement predictive coding: an initial hypothesis is generated
on the basis of information delivered by the first wave of spikes and is tested
with the information carried by the consecutive waves. The development is
considered as extraction and accumulation of features in V4 and objects in IT.
Once stored a feature can be disposed, if rarely activated. This cause update
of feature repository. Consequently, objects in IT are also updated. This
illustrates the growing process and dynamical change of topological structures
of V4, IT and connections between these areas.","A General Framework for Development of the Cortex-like Visual Object
  Recognition System: Waves of Spikes, Predictive Coding and Universal
  Dictionary of Features",2011
180,0.002128176,0.002128135,0.002127999,0.129622117,0.002128472,0.002128106,0.002128131,0.002128313,0.631996285,0.223484267,Topic9,"The competitive MNIST handwritten digit recognition benchmark has a long
history of broken records since 1998. The most recent substantial improvement
by others dates back 7 years (error rate 0.4%) . Recently we were able to
significantly improve this result, using graphics cards to greatly speed up
training of simple but deep MLPs, which achieved 0.35%, outperforming all the
previous more complex methods. Here we report another substantial improvement:
0.31% obtained using a committee of MLPs.","Handwritten Digit Recognition with a Committee of Deep Neural Nets on
  GPUs",2011
181,0.001064042,0.187830135,0.001064038,0.001064044,0.181786052,0.001064076,0.306179543,0.001064083,0.317819824,0.001064163,Topic9,"Artificial Neural Network is among the most popular algorithm for supervised
learning. However, Neural Networks have a well-known drawback of being a ""Black
Box"" learner that is not comprehensible to the Users. This lack of transparency
makes it unsuitable for many high risk tasks such as medical diagnosis that
requires a rational justification for making a decision. Rule Extraction
methods attempt to curb this limitation by extracting comprehensible rules from
a trained Network. Many such extraction algorithms have been developed over the
years with their respective strengths and weaknesses. They have been broadly
categorized into three types based on their approach to use internal model of
the Network. Eclectic Methods are hybrid algorithms that combine the other
approaches to attain more performance. In this paper, we present an Eclectic
method called HERETIC. Our algorithm uses Inductive Decision Tree learning
combined with information of the neural network structure for extracting
logical rules. Experiments and theoretical analysis show HERETIC to be better
in terms of speed and performance.",Eclectic Extraction of Propositional Rules from Neural Networks,2011
182,0.541326498,0.000990426,0.000990246,0.000990256,0.138114631,0.249552122,0.065064587,0.000990365,0.000990432,0.000990437,Topic1,"Communicating and sharing intelligence among agents is an important facet of
achieving Artificial General Intelligence. As a first step towards this
challenge, we introduce a novel framework for image generation: Message Passing
Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have
recently been shown to be very effective for image generation and other tasks,
these networks have been limited to mostly single generator-discriminator
networks. We show that we can obtain multi-agent GANs that communicate through
message passing to achieve better image generation. The objectives of the
individual agents in this framework are two fold: a co-operation objective and
a competing objective. The co-operation objective ensures that the message
sharing mechanism guides the other generator to generate better than itself
while the competing objective encourages each generator to generate better than
its counterpart. We analyze and visualize the messages that these GANs share
among themselves in various scenarios. We quantitatively show that the message
sharing formulation serves as a regularizer for the adversarial training.
Qualitatively, we show that the different generators capture different traits
of the underlying data distribution.",Message Passing Multi-Agent GANs,2016
183,0.33553443,0.001408808,0.081398251,0.001408922,0.150050086,0.333196976,0.001408947,0.001408701,0.092776199,0.001408681,Topic1,"Although Generative Adversarial Networks achieve state-of-the-art results on
a variety of generative tasks, they are regarded as highly unstable and prone
to miss modes. We argue that these bad behaviors of GANs are due to the very
particular functional shape of the trained discriminators in high dimensional
spaces, which can easily make training stuck or push probability mass in the
wrong direction, towards that of higher concentration than that of the data
generating distribution. We introduce several ways of regularizing the
objective, which can dramatically stabilize the training of GAN models. We also
show that our regularizers can help the fair distribution of probability mass
across the modes of the data generating distribution, during the early phases
of training and thus providing a unified solution to the missing modes problem.",Mode Regularized Generative Adversarial Networks,2016
184,0.000840537,0.000840396,0.000840651,0.000840576,0.000840464,0.000840499,0.34484805,0.000840468,0.648427918,0.00084044,Topic9,"The increasing complexity of deep learning architectures is resulting in
training time requiring weeks or even months. This slow training is due in part
to vanishing gradients, in which the gradients used by back-propagation are
extremely large for weights connecting deep layers (layers near the output
layer), and extremely small for shallow layers (near the input layer); this
results in slow learning in the shallow layers. Additionally, it has also been
shown that in highly non-convex problems, such as deep neural networks, there
is a proliferation of high-error low curvature saddle points, which slows down
learning dramatically. In this paper, we attempt to overcome the two above
problems by proposing an optimization method for training deep neural networks
which uses learning rates which are both specific to each layer in the network
and adaptive to the curvature of the function, increasing the learning rate at
low curvature points. This enables us to speed up learning in the shallow
layers of the network and quickly escape high-error low curvature saddle
points. We test our method on standard image classification datasets such as
MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy
as well as reduces the required training time over standard algorithms.",Layer-Specific Adaptive Learning Rates for Deep Networks,2015
185,0.254847897,0.00112381,0.001123827,0.092635148,0.00112404,0.076293892,0.001123938,0.001123964,0.569479602,0.001123881,Topic9,"Unlike human learning, machine learning often fails to handle changes between
training (source) and test (target) input distributions. Such domain shifts,
common in practical scenarios, severely damage the performance of conventional
machine learning methods. Supervised domain adaptation methods have been
proposed for the case when the target data have labels, including some that
perform very well despite being ""frustratingly easy"" to implement. However, in
practice, the target domain is often unlabeled, requiring unsupervised
adaptation. We propose a simple, effective, and efficient method for
unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and
target distributions, without requiring any target labels. Even though it is
extraordinarily simple--it can be implemented in four lines of Matlab
code--CORAL performs remarkably well in extensive evaluations on standard
benchmark datasets.",Return of Frustratingly Easy Domain Adaptation,2015
186,0.426272517,0.001250158,0.00125029,0.00125035,0.001250235,0.00125017,0.001250319,0.379444029,0.185531751,0.00125018,Topic1,"An ever increasing number of computer vision and image/video processing
challenges are being approached using deep convolutional neural networks,
obtaining state-of-the-art results in object recognition and detection,
semantic segmentation, action recognition, optical flow and superresolution.
Hardware acceleration of these algorithms is essential to adopt these
improvements in embedded and mobile computer vision systems. We present a new
architecture, design and implementation as well as the first reported silicon
measurements of such an accelerator, outperforming previous work in terms of
power-, area- and I/O-efficiency. The manufactured device provides up to 196
GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power
efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it
the first architecture scalable to TOp/s performance.",Origami: A 803 GOp/s/W Convolutional Network Accelerator,2015
187,0.026774821,0.23146992,0.0007695,0.000769463,0.173332248,0.330991134,0.000769503,0.000769433,0.233584556,0.000769423,Topic6,"This paper introduces an automated skill acquisition framework in
reinforcement learning which involves identifying a hierarchical description of
the given task in terms of abstract states and extended actions between
abstract states. Identifying such structures present in the task provides ways
to simplify and speed up reinforcement learning algorithms. These structures
also help to generalize such algorithms over multiple tasks without relearning
policies from scratch. We use ideas from dynamical systems to find metastable
regions in the state space and associate them with abstract states. The
spectral clustering algorithm PCCA+ is used to identify suitable abstractions
aligned to the underlying structure. Skills are defined in terms of the
sequence of actions that lead to transitions between such abstract states. The
connectivity information from PCCA+ is used to generate these skills or
options. These skills are independent of the learning task and can be
efficiently reused across a variety of tasks defined over the same model. This
approach works well even without the exact model of the environment by using
sample trajectories to construct an approximate estimate. We also present our
approach to scaling the skill acquisition framework to complex tasks with large
state spaces for which we perform state aggregation using the representation
learned from an action conditional video prediction network and use the skill
acquisition framework on the aggregated state space.","Option Discovery in Hierarchical Reinforcement Learning using
  Spatio-Temporal Clustering",2016
188,0.001052823,0.059712554,0.001052768,0.001052942,0.001052934,0.001052963,0.001052967,0.00105286,0.80177674,0.131140449,Topic9,"In this work we propose a novel interpretation of residual networks showing
that they can be seen as a collection of many paths of differing length.
Moreover, residual networks seem to enable very deep networks by leveraging
only the short paths during training. To support this observation, we rewrite
residual networks as an explicit collection of paths. Unlike traditional
models, paths through residual networks vary in length. Further, a lesion study
reveals that these paths show ensemble-like behavior in the sense that they do
not strongly depend on each other. Finally, and most surprising, most paths are
shorter than one might expect, and only the short paths are needed during
training, as longer paths do not contribute any gradient. For example, most of
the gradient in a residual network with 110 layers comes from paths that are
only 10-34 layers deep. Our results reveal one of the key characteristics that
seem to enable the training of very deep networks: Residual networks avoid the
vanishing gradient problem by introducing short paths which can carry gradient
throughout the extent of very deep networks.",Residual Networks Behave Like Ensembles of Relatively Shallow Networks,2016
189,0.296077023,0.000943643,0.000943612,0.000943621,0.140784506,0.076416128,0.000943675,0.00094363,0.481060597,0.000943564,Topic9,"Deep neural networks (DNNs) have demonstrated state-of-the-art results on
many pattern recognition tasks, especially vision classification problems.
Understanding the inner workings of such computational brains is both
fascinating basic science that is interesting in its own right - similar to why
we study the human brain - and will enable researchers to further improve DNNs.
One path to understanding how a neural network functions internally is to study
what each of its neurons has learned to detect. One such method is called
activation maximization (AM), which synthesizes an input (e.g. an image) that
highly activates a neuron. Here we dramatically improve the qualitative state
of the art of activation maximization by harnessing a powerful, learned prior:
a deep generator network (DGN). The algorithm (1) generates qualitatively
state-of-the-art synthetic images that look almost real, (2) reveals the
features learned by each neuron in an interpretable way, (3) generalizes well
to new datasets and somewhat well to different network architectures without
requiring the prior to be relearned, and (4) can be considered as a
high-quality generative method (in this case, by generating novel, creative,
interesting, recognizable images).","Synthesizing the preferred inputs for neurons in neural networks via
  deep generator networks",2016
190,0.001923645,0.001923315,0.073975991,0.00192349,0.001923248,0.081038197,0.001923555,0.033229004,0.800216298,0.001923258,Topic9,"We derive a relationship between network representation in energy-efficient
neuromorphic architectures and block Toplitz convolutional matrices. Inspired
by this connection, we develop deep convolutional networks using a family of
structured convolutional matrices and achieve state-of-the-art trade-off
between energy efficiency and classification accuracy for well-known image
recognition tasks. We also put forward a novel method to train binary
convolutional networks by utilising an existing connection between
noisy-rectified linear units and binary activations.",Structured Convolution Matrices for Energy-efficient Deep learning,2016
191,0.085928144,0.001389143,0.001389192,0.00138914,0.001389106,0.092853551,0.001389202,0.001389093,0.811494316,0.001389114,Topic9,"Deep neural networks are able to learn powerful representations from large
quantities of labeled input data, however they cannot always generalize well
across changes in input distributions. Domain adaptation algorithms have been
proposed to compensate for the degradation in performance due to domain shift.
In this paper, we address the case when the target domain is unlabeled,
requiring unsupervised adaptation. CORAL is a ""frustratingly easy"" unsupervised
domain adaptation method that aligns the second-order statistics of the source
and target distributions with a linear transformation. Here, we extend CORAL to
learn a nonlinear transformation that aligns correlations of layer activations
in deep neural networks (Deep CORAL). Experiments on standard benchmark
datasets show state-of-the-art performance.",Deep CORAL: Correlation Alignment for Deep Domain Adaptation,2016
192,0.2295258,0.063454227,0.001010351,0.001010445,0.214289393,0.001010423,0.001010373,0.091053378,0.396625206,0.001010405,Topic9,"3D action recognition - analysis of human actions based on 3D skeleton data -
becomes popular recently due to its succinctness, robustness, and
view-invariant representation. Recent attempts on this problem suggested to
develop RNN-based learning methods to model the contextual dependency in the
temporal domain. In this paper, we extend this idea to spatio-temporal domains
to analyze the hidden sources of action-related information within the input
data over both domains concurrently. Inspired by the graphical structure of the
human skeleton, we further propose a more powerful tree-structure based
traversal method. To handle the noise and occlusion in 3D skeleton data, we
introduce new gating mechanism within LSTM to learn the reliability of the
sequential input data and accordingly adjust its effect on updating the
long-term context information stored in the memory cell. Our method achieves
state-of-the-art performance on 4 challenging benchmark datasets for 3D human
action analysis.",Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition,2016
193,0.001562746,0.001562896,0.001562784,0.001562696,0.001562772,0.414523226,0.001562887,0.001562677,0.572974567,0.001562748,Topic9,"Deep Neural Networks often require good regularizers to generalize well.
Dropout is one such regularizer that is widely used among Deep Learning
practitioners. Recent work has shown that Dropout can also be viewed as
performing Approximate Bayesian Inference over the network parameters. In this
work, we generalize this notion and introduce a rich family of regularizers
which we call Generalized Dropout. One set of methods in this family, called
Dropout++, is a version of Dropout with trainable parameters. Classical Dropout
emerges as a special case of this method. Another member of this family selects
the width of neural network layers. Experiments show that these methods help in
improving generalization performance over Dropout.",Generalized Dropout,2016
194,0.001205211,0.001205181,0.001205213,0.001205165,0.001205176,0.120864083,0.152023614,0.295633851,0.424247466,0.00120504,Topic9,"A new, radical CNN design approach is presented in this paper, considering
the reduction of the total computational load during inference. This is
achieved by a new holistic intervention on both the CNN architecture and the
training procedure, which targets to the parsimonious inference by learning to
exploit or remove the redundant capacity of a CNN architecture. This is
accomplished, by the introduction of a new structural element that can be
inserted as an add-on to any contemporary CNN architecture, whilst preserving
or even improving its recognition accuracy. Our approach formulates a
systematic and data-driven method for developing CNNs that are trained to
eventually change size and form in real-time during inference, targeting to the
smaller possible computational footprint. Results are provided for the optimal
implementation on a few modern, high-end mobile computing platforms indicating
a significant speed-up of up to x3 times.","Parsimonious Inference on Convolutional Neural Networks: Learning and
  applying on-line kernel activation rules",2017
195,0.001099261,0.001099063,0.001099075,0.001099095,0.001099088,0.275850265,0.001099276,0.001099017,0.715356755,0.001099106,Topic9,"We propose an algorithm for meta-learning that is model-agnostic, in the
sense that it is compatible with any model trained with gradient descent and
applicable to a variety of different learning problems, including
classification, regression, and reinforcement learning. The goal of
meta-learning is to train a model on a variety of learning tasks, such that it
can solve new learning tasks using only a small number of training samples. In
our approach, the parameters of the model are explicitly trained such that a
small number of gradient steps with a small amount of training data from a new
task will produce good generalization performance on that task. In effect, our
method trains the model to be easy to fine-tune. We demonstrate that this
approach leads to state-of-the-art performance on two few-shot image
classification benchmarks, produces good results on few-shot regression, and
accelerates fine-tuning for policy gradient reinforcement learning with neural
network policies.",Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,2017
196,0.325623534,0.000943619,0.023851522,0.000943648,0.000943545,0.000943624,0.047513931,0.000943674,0.597349309,0.000943594,Topic9,"For computer vision applications, prior works have shown the efficacy of
reducing the numeric precision of model parameters (network weights) in deep
neural networks but also that reducing the precision of activations hurts model
accuracy much more than reducing the precision of model parameters. We study
schemes to train networks from scratch using reduced-precision activations
without hurting the model accuracy. We reduce the precision of activation maps
(along with model parameters) using a novel quantization scheme and increase
the number of filter maps in a layer, and find that this scheme compensates or
surpasses the accuracy of the baseline full-precision network. As a result, one
can significantly reduce the dynamic memory footprint, memory bandwidth,
computational energy and speed up the training and inference process with
appropriate hardware support. We call our scheme WRPN - wide reduced-precision
networks. We report results using our proposed schemes and show that our
results are better than previously reported accuracies on ILSVRC-12 dataset
while being computationally less expensive compared to previously reported
reduced-precision networks.",WRPN: Training and Inference using Wide Reduced-Precision Networks,2017
197,0.140411984,0.000980582,0.000980759,0.000980637,0.22098049,0.062824134,0.000980687,0.000980583,0.569899548,0.000980596,Topic9,"Deep neural networks trained on large supervised datasets have led to
impressive results in image classification and other tasks. However,
well-annotated datasets can be time-consuming and expensive to collect, lending
increased interest to larger but noisy datasets that are more easily obtained.
In this paper, we show that deep neural networks are capable of generalizing
from training data for which true labels are massively outnumbered by incorrect
labels. We demonstrate remarkably high test performance after training on
corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain
test accuracy above 90 percent even after each clean training example has been
diluted with 100 randomly-labeled examples. Such behavior holds across multiple
patterns of label noise, even when erroneous labels are biased towards
confusing classes. We show that training in this regime requires a significant
but manageable increase in dataset size that is related to the factor by which
correct labels have been diluted. Finally, we provide an analysis of our
results that shows how increasing noise decreases the effective batch size.",Deep Learning is Robust to Massive Label Noise,2017
198,0.51097112,0.0014498,0.189560233,0.101122546,0.001449587,0.001449659,0.001449577,0.001449516,0.18964815,0.001449813,Topic1,"Content-invariance in mapping codes learned by GAEs is a useful feature for
various relation learning tasks. In this paper we show that the
content-invariance of mapping codes for images of 2D and 3D rotated objects can
be substantially improved by extending the standard GAE loss (symmetric
reconstruction error) with a regularization term that penalizes the symmetric
cross-reconstruction error. This error term involves reconstruction of pairs
with mapping codes obtained from other pairs exhibiting similar
transformations. Although this would principally require knowledge of the
transformations exhibited by training pairs, our experiments show that a
bootstrapping approach can sidestep this issue, and that the regularization
term can effectively be used in an unsupervised setting.","Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object
  Rotation",2017
199,0.000980663,0.000980564,0.000980616,0.000980592,0.271056734,0.018623719,0.017899232,0.294279604,0.393237656,0.000980622,Topic9,"Sensor-based activity recognition seeks the profound high-level knowledge
about human activities from multitudes of low-level sensor readings.
Conventional pattern recognition approaches have made tremendous progress in
the past years. However, those methods often heavily rely on heuristic
hand-crafted feature extraction, which could hinder their generalization
performance. Additionally, existing methods are undermined for unsupervised and
incremental learning tasks. Recently, the recent advancement of deep learning
makes it possible to perform automatic high-level feature extraction thus
achieves promising performance in many areas. Since then, deep learning based
methods have been widely adopted for the sensor-based activity recognition
tasks. This paper surveys the recent advance of deep learning based
sensor-based activity recognition. We summarize existing literature from three
aspects: sensor modality, deep model, and application. We also present detailed
insights on existing work and propose grand challenges for future research.",Deep Learning for Sensor-based Activity Recognition: A Survey,2017
200,0.0962746,0.362592267,0.001219873,0.001219766,0.001219874,0.001219825,0.139384721,0.001219803,0.394429546,0.001219726,Topic9,"We explain that the difficulties of training deep neural networks come from a
syndrome of three consistency issues. This paper describes our efforts in their
analysis and treatment. The first issue is the training speed inconsistency in
different layers. We propose to address it with an intuitive,
simple-to-implement, low footprint second-order method. The second issue is the
scale inconsistency between the layer inputs and the layer residuals. We
explain how second-order information provides favorable convenience in removing
this roadblock. The third and most challenging issue is the inconsistency in
residual propagation. Based on the fundamental theorem of linear algebra, we
provide a mathematical characterization of the famous vanishing gradient
problem. Thus, an important design principle for future optimization and neural
network design is derived. We conclude this paper with the construction of a
novel contractive neural network.",On the Importance of Consistency in Training Deep Neural Networks,2017
201,0.000961807,0.000961732,0.000961755,0.13808176,0.463445653,0.000961755,0.000961747,0.000961863,0.350081557,0.04262037,Topic5,"For complex segmentation tasks, fully automatic systems are inherently
limited in their achievable accuracy for extracting relevant objects.
Especially in cases where only few data sets need to be processed for a highly
accurate result, semi-automatic segmentation techniques exhibit a clear benefit
for the user. One area of application is medical image processing during an
intervention for a single patient. We propose a learning-based cooperative
segmentation approach which includes the computing entity as well as the user
into the task. Our system builds upon a state-of-the-art fully convolutional
artificial neural network (FCN) as well as an active user model for training.
During the segmentation process, a user of the trained system can iteratively
add additional hints in form of pictorial scribbles as seed points into the FCN
system to achieve an interactive and precise segmentation result. The
segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches
can yield superior results compared to networks without the user input channel
component, due to a consistent improvement in segmentation quality after each
interaction.","UI-Net: Interactive Artificial Neural Networks for Iterative Image
  Segmentation Based on a User Model",2017
202,0.001492943,0.001493686,0.001492827,0.001492853,0.00149292,0.001493044,0.365078981,0.001492801,0.622977154,0.00149279,Topic9,"Most of the weights in a Lightweight Neural Network have a value of zero,
while the remaining ones are either +1 or -1. These universal approximators
require approximately 1.1 bits/weight of storage, posses a quick forward pass
and achieve classification accuracies similar to conventional continuous-weight
networks. Their training regimen focuses on error reduction initially, but
later emphasizes discretization of weights. They ignore insignificant inputs,
remove unnecessary weights, and drop unneeded hidden neurons. We have
successfully tested them on the MNIST, credit card fraud, and credit card
defaults data sets using networks having 2 to 16 hidden layers and up to 4.4
million weights.",Lightweight Neural Networks,2017
203,0.001234785,0.001235027,0.100265607,0.471247546,0.03789718,0.140403408,0.001234814,0.001234945,0.244011775,0.001234913,Topic4,"We introduce tensor field networks, which are locally equivariant to 3D
rotations, translations, and permutations of points at every layer. 3D rotation
equivariance removes the need for data augmentation to identify features in
arbitrary orientations. Our network uses filters built from spherical
harmonics; due to the mathematical consequences of this filter choice, each
layer accepts as input (and guarantees as output) scalars, vectors, and
higher-order tensors, in the geometric sense of these terms. We demonstrate how
tensor field networks learn to model simple physics (Newtonian gravitation and
moment of inertia), classify simple 3D shapes (trained on one orientation and
tested on shapes in arbitrary orientations), and, given a small organic
molecule with an atom removed, replace the correct element at the correct
location in space.","Tensor Field Networks: Rotation- and Translation-Equivariant Neural
  Networks for 3D Point Clouds",2018
204,0.302253307,0.000699563,0.000699405,0.000699538,0.08762662,0.000699543,0.185721496,0.000699461,0.420201581,0.000699486,Topic9,"We explore the effect of introducing prior information into the intermediate
level of neural networks for a learning task on which all the state-of-the-art
machine learning algorithms tested failed to learn. We motivate our work from
the hypothesis that humans learn such intermediate concepts from other
individuals via a form of supervision or guidance using a curriculum. The
experiments we have conducted provide positive evidence in favor of this
hypothesis. In our experiments, a two-tiered MLP architecture is trained on a
dataset with 64x64 binary inputs images, each image with three sprites. The
final task is to decide whether all the sprites are the same or one of them is
different. Sprites are pentomino tetris shapes and they are placed in an image
with different locations using scaling and rotation transformations. The first
part of the two-tiered MLP is pre-trained with intermediate-level targets being
the presence of sprites at each location, while the second part takes the
output of the first part as input and predicts the final task's target binary
event. The two-tiered MLP architecture, with a few tens of thousand examples,
was able to learn the task perfectly, whereas all other algorithms (include
unsupervised pre-training, but also traditional algorithms like SVMs, decision
trees and boosting) all perform no better than chance. We hypothesize that the
optimization difficulty involved when the intermediate pre-training is not
performed is due to the {\em composition} of two highly non-linear tasks. Our
findings are also consistent with hypotheses on cultural learning inspired by
the observations of optimization problems with deep learning, presumably
because of effective local minima.",Knowledge Matters: Importance of Prior Information for Optimization,2013
205,0.001351691,0.001351871,0.231409737,0.001351541,0.062666468,0.237494015,0.00135163,0.001351487,0.460319829,0.00135173,Topic9,"Regularized training of an autoencoder typically results in hidden unit
biases that take on large negative values. We show that negative biases are a
natural result of using a hidden layer whose responsibility is to both
represent the input data and act as a selection mechanism that ensures sparsity
of the representation. We then show that negative biases impede the learning of
data distributions whose intrinsic dimensionality is high. We also propose a
new activation function that decouples the two roles of the hidden layer and
that allows us to learn representations on data with very high intrinsic
dimensionality, where standard autoencoders typically fail. Since the decoupled
activation function acts like an implicit regularizer, the model can be trained
by minimizing the reconstruction error of training data, without requiring any
additional regularization.",Zero-bias autoencoders and the benefits of co-adapting features,2014
206,0.000909357,0.054648192,0.000909293,0.000909266,0.000909358,0.058536267,0.000909387,0.187489123,0.693870415,0.000909343,Topic9,"Deep convolutional neural networks (CNNs) have shown great potential for
numerous real-world machine learning applications, but performing inference in
large CNNs in real-time remains a challenge. We have previously demonstrated
that traditional CNNs can be converted into deep spiking neural networks
(SNNs), which exhibit similar accuracy while reducing both latency and
computational load as a consequence of their data-driven, event-based style of
computing. Here we provide a novel theory that explains why this conversion is
successful, and derive from it several new tools to convert a larger and more
powerful class of deep networks into SNNs. We identify the main sources of
approximation errors in previous conversion methods, and propose simple
mechanisms to fix these issues. Furthermore, we develop spiking implementations
of common CNN operations such as max-pooling, softmax, and batch-normalization,
which allow almost loss-less conversion of arbitrary CNN architectures into the
spiking domain. Empirical evaluation of different network architectures on the
MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.","Theory and Tools for the Conversion of Analog to Spiking Convolutional
  Neural Networks",2016
207,0.912822413,0.000885197,0.000885141,0.000885107,0.000885097,0.048069081,0.000885086,0.000885081,0.032912667,0.00088513,Topic1,"In this paper, we propose a novel generative model named Stacked Generative
Adversarial Networks (SGAN), which is trained to invert the hierarchical
representations of a bottom-up discriminative network. Our model consists of a
top-down stack of GANs, each learned to generate lower-level representations
conditioned on higher-level representations. A representation discriminator is
introduced at each feature hierarchy to encourage the representation manifold
of the generator to align with that of the bottom-up discriminative network,
leveraging the powerful discriminative representations to guide the generative
model. In addition, we introduce a conditional loss that encourages the use of
conditional information from the layer above, and a novel entropy loss that
maximizes a variational lower bound on the conditional entropy of generator
outputs. We first train each stack independently, and then train the whole
model end-to-end. Unlike the original GAN that uses a single noise vector to
represent all the variations, our SGAN decomposes variations into multiple
levels and gradually resolves uncertainties in the top-down generative process.
Based on visual inspection, Inception scores and visual Turing test, we
demonstrate that SGAN is able to generate images of much higher quality than
GANs without stacking.",Stacked Generative Adversarial Networks,2016
208,0.190987221,0.001351706,0.001351659,0.001351708,0.001351789,0.001351643,0.040354113,0.04128861,0.69042706,0.030184491,Topic9,"We study the problem of large scale, multi-label visual recognition with a
large number of possible classes. We propose a method for augmenting a trained
neural network classifier with auxiliary capacity in a manner designed to
significantly improve upon an already well-performing model, while minimally
impacting its computational footprint. Using the predictions of the network
itself as a descriptor for assessing visual similarity, we define a
partitioning of the label space into groups of visually similar entities. We
then augment the network with auxilliary hidden layer pathways with
connectivity only to these groups of label units. We report a significant
improvement in mean average precision on a large-scale object recognition task
with the augmented model, while increasing the number of multiply-adds by less
than 3%.",Self-informed neural network structure learning,2014
209,0.001724482,0.001724392,0.001724462,0.001724554,0.001724443,0.172510934,0.001724638,0.001724488,0.813693303,0.001724304,Topic9,"Artificial neural networks typically have a fixed, non-linear activation
function at each neuron. We have designed a novel form of piecewise linear
activation function that is learned independently for each neuron using
gradient descent. With this adaptive activation function, we are able to
improve upon deep neural network architectures composed of static rectified
linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),
CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs
boson decay modes.",Learning Activation Functions to Improve Deep Neural Networks,2014
210,0.187079556,0.040564482,0.001163067,0.054786729,0.001163109,0.001163155,0.001163081,0.001163165,0.710590558,0.001163098,Topic9,"Suitable lateral connections between encoder and decoder are shown to allow
higher layers of a denoising autoencoder (dAE) to focus on invariant
representations. In regular autoencoders, detailed information needs to be
carried through the highest layers but lateral connections from encoder to
decoder relieve this pressure. It is shown that abstract invariant features can
be translated to detailed reconstructions when invariant features are allowed
to modulate the strength of the lateral connection. Three dAE structures with
modulated and additive lateral connections, and without lateral connections
were compared in experiments using real-world images. The experiments verify
that adding modulated lateral connections to the model 1) improves the accuracy
of the probability model for inputs, as measured by denoising performance; 2)
results in representations whose degree of invariance grows faster towards the
higher layers; and 3) supports the formation of diverse invariant poolings.","Denoising autoencoder with modulated lateral connections learns
  invariant representations of natural images",2014
211,0.092329193,0.000854959,0.00085484,0.058314957,0.171782368,0.197159086,0.000854907,0.000854939,0.371538377,0.105456373,Topic9,"A grand challenge in machine learning is the development of computational
algorithms that match or outperform humans in perceptual inference tasks that
are complicated by nuisance variation. For instance, visual object recognition
involves the unknown object position, orientation, and scale in object
recognition while speech recognition involves the unknown voice pronunciation,
pitch, and speed. Recently, a new breed of deep learning algorithms have
emerged for high-nuisance inference tasks that routinely yield pattern
recognition systems with near- or super-human capabilities. But a fundamental
question remains: Why do they work? Intuitions abound, but a coherent framework
for understanding, analyzing, and synthesizing deep learning architectures has
remained elusive. We answer this question by developing a new probabilistic
framework for deep learning based on the Deep Rendering Model: a generative
probabilistic model that explicitly captures latent nuisance variation. By
relaxing the generative model to a discriminative one, we can recover two of
the current leading deep learning systems, deep convolutional neural networks
and random decision forests, providing insights into their successes and
shortcomings, as well as a principled route to their improvement.",A Probabilistic Theory of Deep Learning,2015
212,0.000847691,0.00084773,0.000847653,0.000847648,0.111951455,0.194253415,0.159141056,0.000847689,0.470672381,0.059743283,Topic9,"Tackling pattern recognition problems in areas such as computer vision,
bioinformatics, speech or text recognition is often done best by taking into
account task-specific statistical relations between output variables. In
structured prediction, this internal structure is used to predict multiple
outputs simultaneously, leading to more accurate and coherent predictions.
Structural support vector machines (SSVMs) are nonprobabilistic models that
optimize a joint input-output function through margin-based learning. Because
SSVMs generally disregard the interplay between unary and interaction factors
during the training phase, final parameters are suboptimal. Moreover, its
factors are often restricted to linear combinations of input features, limiting
its generalization power. To improve prediction accuracy, this paper proposes:
(i) Joint inference and learning by integration of back-propagation and
loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM
factors to neural networks that form highly nonlinear functions of input
features. Image segmentation benchmark results demonstrate improvements over
conventional SSVM training methods in terms of accuracy, highlighting the
feasibility of end-to-end SSVM training with neural factors.","Integrated Inference and Learning of Neural Factors in Structural
  Support Vector Machines",2015
213,0.387803105,0.085046073,0.000813176,0.098265773,0.000813274,0.000813261,0.00081316,0.000813204,0.42400573,0.000813243,Topic9,"Top-down information plays a central role in human perception, but plays
relatively little role in many current state-of-the-art deep networks, such as
Convolutional Neural Networks (CNNs). This work seeks to explore a path by
which top-down information can have a direct impact within current deep
networks. We explore this path by learning and using ""generators"" corresponding
to the network internal effects of three types of transformation (each a
restriction of a general affine transformation): rotation, scaling, and
translation. We demonstrate how these learned generators can be used to
transfer top-down information to novel settings, as mediated by the ""feature
flows"" that the transformations (and the associated generators) correspond to
inside the network. Specifically, we explore three aspects: 1) using generators
as part of a method for synthesizing transformed images --- given a previously
unseen image, produce versions of that image corresponding to one or more
specified transformations, 2) ""zero-shot learning"" --- when provided with a
feature flow corresponding to the effect of a transformation of unknown amount,
leverage learned generators as part of a method by which to perform an accurate
categorization of the amount of transformation, even for amounts never observed
during training, and 3) (inside-CNN) ""data augmentation"" --- improve the
classification performance of an existing network by using the learned
generators to directly provide additional training ""inside the CNN"".","What Happened to My Dog in That Network: Unraveling Top-down Generators
  in Convolutional Neural Networks",2015
214,0.230631366,0.00080663,0.000806589,0.07031425,0.6364664,0.000806611,0.000806638,0.000806674,0.057748256,0.000806587,Topic5,"Modern computer vision algorithms typically require expensive data
acquisition and accurate manual labeling. In this work, we instead leverage the
recent progress in computer graphics to generate fully labeled, dynamic, and
photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual
world cloning method, and validate our approach by building and publicly
releasing a new video dataset, called Virtual KITTI (see
http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),
automatically labeled with accurate ground truth for object detection,
tracking, scene and instance segmentation, depth, and optical flow. We provide
quantitative experimental evidence suggesting that (i) modern deep learning
algorithms pre-trained on real data behave similarly in real and virtual
worlds, and (ii) pre-training on virtual data improves performance. As the gap
between real and virtual worlds is small, virtual worlds enable measuring the
impact of various weather and imaging conditions on recognition performance,
all other things being equal. We show these factors may affect drastically
otherwise high-performing deep models for tracking.",Virtual Worlds as Proxy for Multi-Object Tracking Analysis,2016
215,0.499744608,0.001099109,0.001099062,0.00109921,0.001099074,0.231014061,0.00109915,0.261547315,0.001099318,0.001099095,Topic1,"Video sequences contain rich dynamic patterns, such as dynamic texture
patterns that exhibit stationarity in the temporal domain, and action patterns
that are non-stationary in either spatial or temporal domain. We show that a
spatial-temporal generative ConvNet can be used to model and synthesize dynamic
patterns. The model defines a probability distribution on the video sequence,
and the log probability is defined by a spatial-temporal ConvNet that consists
of multiple layers of spatial-temporal filters to capture spatial-temporal
patterns of different scales. The model can be learned from the training video
sequences by an ""analysis by synthesis"" learning algorithm that iterates the
following two steps. Step 1 synthesizes video sequences from the currently
learned model. Step 2 then updates the model parameters based on the difference
between the synthesized video sequences and the observed training sequences. We
show that the learning algorithm can synthesize realistic dynamic patterns.",Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet,2016
216,0.103208806,0.000793943,0.00079385,0.00079378,0.000794009,0.000793932,0.191540539,0.000793885,0.699693471,0.000793784,Topic9,"Taking inspiration from biological evolution, we explore the idea of ""Can
deep neural networks evolve naturally over successive generations into highly
efficient deep neural networks?"" by introducing the notion of synthesizing new
highly efficient, yet powerful deep neural networks over successive generations
via an evolutionary process from ancestor deep neural networks. The
architectural traits of ancestor deep neural networks are encoded using
synaptic probability models, which can be viewed as the `DNA' of these
networks. New descendant networks with differing network architectures are
synthesized based on these synaptic probability models from the ancestor
networks and computational environmental factor models, in a random manner to
mimic heredity, natural selection, and random mutation. These offspring
networks are then trained into fully functional networks, like one would train
a newborn, and have more efficient, more diverse network architectures than
their ancestor networks, while achieving powerful modeling capabilities.
Experimental results for the task of visual saliency demonstrated that the
synthesized `evolved' offspring networks can achieve state-of-the-art
performance while having network architectures that are significantly more
efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth
generation) compared to the original ancestor network.","Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural
  Networks",2016
217,0.155793256,0.001282332,0.001282545,0.00128231,0.00128231,0.710403711,0.001282411,0.001282302,0.124826425,0.001282398,Topic6,"This paper proposes an alternating back-propagation algorithm for learning
the generator network model. The model is a non-linear generalization of factor
analysis. In this model, the mapping from the continuous latent factors to the
observed signal is parametrized by a convolutional neural network. The
alternating back-propagation algorithm iterates the following two steps: (1)
Inferential back-propagation, which infers the latent factors by Langevin
dynamics or gradient descent. (2) Learning back-propagation, which updates the
parameters given the inferred latent factors by gradient descent. The gradient
computations in both steps are powered by back-propagation, and they share most
of their code in common. We show that the alternating back-propagation
algorithm can learn realistic generator models of natural images, video
sequences, and sounds. Moreover, it can also be used to learn from incomplete
or indirect training data.",Alternating Back-Propagation for Generator Network,2016
218,0.14625907,0.001149562,0.001149682,0.001149656,0.001149706,0.107136447,0.300853699,0.001149622,0.43885291,0.001149648,Topic9,"Recently, several optimization methods have been successfully applied to the
hyperparameter optimization of deep neural networks (DNNs). The methods work by
modeling the joint distribution of hyperparameter values and corresponding
error. Those methods become less practical when applied to modern DNNs whose
training may take a few days and thus one cannot collect sufficient
observations to accurately model the distribution. To address this challenging
issue, we propose a method that learns to transfer optimal hyperparameter
values for a small source dataset to hyperparameter values with comparable
performance on a dataset of interest. As opposed to existing transfer learning
methods, our proposed method does not use hand-designed features. Instead, it
uses surrogates to model the hyperparameter-error distributions of the two
datasets and trains a neural network to learn the transfer function. Extensive
experiments on three CV benchmark datasets clearly demonstrate the efficiency
of our method.","Hyperparameter Transfer Learning through Surrogate Alignment for
  Efficient Deep Neural Network Training",2016
219,0.000869821,0.076613427,0.000869662,0.000869723,0.317301365,0.22801403,0.000869682,0.000869782,0.263442139,0.110280369,Topic5,"While perception tasks such as visual object recognition and text
understanding play an important role in human intelligence, the subsequent
tasks that involve inference, reasoning and planning require an even higher
level of intelligence. The past few years have seen major advances in many
perception tasks using deep learning models. For higher-level inference,
however, probabilistic graphical models with their Bayesian nature are still
more powerful and flexible. To achieve integrated intelligence that involves
both perception and inference, it is naturally desirable to tightly integrate
deep learning and Bayesian models within a principled probabilistic framework,
which we call Bayesian deep learning. In this unified framework, the perception
of text or images using deep learning can boost the performance of higher-level
inference and in return, the feedback from the inference process is able to
enhance the perception of text or images. This paper proposes a general
framework for Bayesian deep learning and reviews its recent applications on
recommender systems, topic models, and control. In this paper, we also discuss
the relationship and differences between Bayesian deep learning and other
related topics like Bayesian treatment of neural networks.",Towards Bayesian Deep Learning: A Framework and Some Existing Methods,2016
220,0.001887598,0.001887246,0.001887029,0.001887316,0.001887558,0.001887214,0.100489014,0.001887177,0.884412692,0.001887157,Topic9,"We propose and systematically evaluate three strategies for training
dynamically-routed artificial neural networks: graphs of learned
transformations through which different input signals may take different paths.
Though some approaches have advantages over others, the resulting networks are
often qualitatively similar. We find that, in dynamically-routed networks
trained to classify images, layers and branches become specialized to process
distinct categories of images. Additionally, given a fixed computational
budget, dynamically-routed networks tend to perform better than comparable
statically-routed networks.",Deciding How to Decide: Dynamic Routing in Artificial Neural Networks,2017
221,0.309314352,0.174893185,0.00087736,0.175180477,0.000877396,0.000877473,0.000877387,0.000877354,0.335347667,0.000877349,Topic9,"Deconvolutional layers have been widely used in a variety of deep models for
up-sampling, including encoder-decoder networks for semantic segmentation and
deep generative models for unsupervised learning. One of the key limitations of
deconvolutional operations is that they result in the so-called checkerboard
problem. This is caused by the fact that no direct relationship exists among
adjacent pixels on the output feature map. To address this problem, we propose
the pixel deconvolutional layer (PixelDCL) to establish direct relationships
among adjacent pixels on the up-sampled feature map. Our method is based on a
fresh interpretation of the regular deconvolution operation. The resulting
PixelDCL can be used to replace any deconvolutional layer in a plug-and-play
manner without compromising the fully trainable capabilities of original
models. The proposed PixelDCL may result in slight decrease in efficiency, but
this can be overcome by an implementation trick. Experimental results on
semantic segmentation demonstrate that PixelDCL can consider spatial features
such as edges and shapes and yields more accurate segmentation outputs than
deconvolutional layers. When used in image generation tasks, our PixelDCL can
largely overcome the checkerboard problem suffered by regular deconvolution
operations.",Pixel Deconvolutional Networks,2017
222,0.412053999,0.000943691,0.078215573,0.000943613,0.000943578,0.205027796,0.000943656,0.000943543,0.299040948,0.000943602,Topic1,"We propose a novel architecture for $k$-shot classification on the Omniglot
dataset. Building on prototypical networks, we extend their architecture to
what we call Gaussian prototypical networks. Prototypical networks learn a map
between images and embedding vectors, and use their clustering for
classification. In our model, a part of the encoder output is interpreted as a
confidence region estimate about the embedding point, and expressed as a
Gaussian covariance matrix. Our network then constructs a direction and class
dependent distance metric on the embedding space, using uncertainties of
individual data points as weights. We show that Gaussian prototypical networks
are a preferred architecture over vanilla prototypical networks with an
equivalent number of parameters. We report state-of-the-art performance in
1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot
5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.
We explore artificially down-sampling a fraction of images in the training set,
which improves our performance even further. We therefore hypothesize that
Gaussian prototypical networks might perform better in less homogeneous,
noisier datasets, which are commonplace in real world applications.",Gaussian Prototypical Networks for Few-Shot Learning on Omniglot,2017
223,0.16869277,0.001176778,0.001176661,0.00117667,0.001176861,0.192001154,0.269309574,0.001176644,0.362936088,0.0011768,Topic9,"In this paper, we show a phenomenon, which we named ""super-convergence"",
where residual networks can be trained using an order of magnitude fewer
iterations than is used with standard training methods. The existence of
super-convergence is relevant to understanding why deep networks generalize
well. One of the key elements of super-convergence is training with cyclical
learning rates and a large maximum learning rate. Furthermore, we present
evidence that training with large learning rates improves performance by
regularizing the network. In addition, we show that super-convergence provides
a greater boost in performance relative to standard training when the amount of
labeled training data is limited. We also derive a simplification of the
Hessian Free optimization method to compute an estimate of the optimal learning
rate. The architectures and code to replicate the figures in this paper are
available at github.com/lnsmith54/super-convergence.","Super-Convergence: Very Fast Training of Residual Networks Using Large
  Learning Rates",2017
224,0.198171269,0.001010387,0.001010264,0.001010256,0.001010312,0.336297771,0.001010355,0.001010312,0.458458751,0.001010323,Topic9,"Learning, taking into account full distribution of the data, referred to as
generative, is not feasible with deep neural networks (DNNs) because they model
only the conditional distribution of the outputs given the inputs. Current
solutions are either based on joint probability models facing difficult
estimation problems or learn two separate networks, mapping inputs to outputs
(recognition) and vice-versa (generation). We propose an intermediate approach.
First, we show that forward computation in DNNs with logistic sigmoid
activations corresponds to a simplified approximate Bayesian inference in a
directed probabilistic multi-layer model. This connection allows to interpret
DNN as a probabilistic model of the output and all hidden units given the
input. Second, we propose that in order for the recognition and generation
networks to be more consistent with the joint model of the data, weights of the
recognition and generator network should be related by transposition. We
demonstrate in a tentative experiment that such a coupled pair can be learned
generatively, modelling the full distribution of the data, and has enough
capacity to perform well in both recognition and generation.",Generative learning for deep networks,2017
225,0.001250342,0.00125022,0.001250195,0.001250219,0.001250397,0.001250227,0.309153513,0.00125037,0.680844246,0.001250273,Topic9,"We explore efficient neural architecture search methods and show that a
simple yet powerful evolutionary algorithm can discover new architectures with
excellent performance. Our approach combines a novel hierarchical genetic
representation scheme that imitates the modularized design pattern commonly
adopted by human experts, and an expressive search space that supports complex
topologies. Our algorithm efficiently discovers architectures that outperform a
large number of manually designed models for image classification, obtaining
top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which
is competitive with the best existing neural architecture search approaches. We
also present results using random search, achieving 0.3% less top-1 accuracy on
CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36
hours down to 1 hour.",Hierarchical Representations for Efficient Architecture Search,2017
226,0.298007539,0.000781361,0.073654428,0.000781365,0.109340738,0.000781471,0.000781407,0.000781429,0.475975537,0.039114725,Topic9,"Effective training of neural networks requires much data. In the low-data
regime, parameters are underdetermined, and learnt networks generalise poorly.
Data Augmentation alleviates this by using existing data more effectively.
However standard data augmentation produces only limited plausible alternative
data. Given there is potential to generate a much broader set of augmentations,
we design and train a generative model to do data augmentation. The model,
based on image conditional Generative Adversarial Networks, takes data from a
source domain and learns to take any data item and generalise it to generate
other within-class data items. As this generative process does not depend on
the classes themselves, it can be applied to novel unseen classes of data. We
show that a Data Augmentation Generative Adversarial Network (DAGAN) augments
standard vanilla classifiers well. We also show a DAGAN can enhance few-shot
learning systems such as Matching Networks. We demonstrate these approaches on
Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In
our experiments we can see over 13% increase in accuracy in the low-data regime
experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face
(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%
(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).",Data Augmentation Generative Adversarial Networks,2017
227,0.002041484,0.243052978,0.002041288,0.279623009,0.002041256,0.002041196,0.002041526,0.002041193,0.463034754,0.002041316,Topic9,"This paper introduces the first deep neural network-based estimation metric
for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network
predicts whether or not they should be adjacent in the correct assembly of the
puzzle, using nothing but the pixels of each piece. The proposed metric
exhibits an extremely high precision even though no manual feature extraction
is performed. When incorporated into an existing puzzle solver, the solution's
accuracy increases significantly, achieving thereby a new state-of-the-art
standard.","DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the
  Jigsaw Puzzle Problem",2017
228,0.156702727,0.001369982,0.001370124,0.001370181,0.001370089,0.001370029,0.001370114,0.001370157,0.832336417,0.00137018,Topic9,"In this paper we describe the problem of painter classification, and propose
a novel approach based on deep convolutional autoencoder neural networks. While
previous approaches relied on image processing and manual feature extraction
from paintings, our approach operates on the raw pixel level, without any
preprocessing or manual feature extraction. We first train a deep convolutional
autoencoder on a dataset of paintings, and subsequently use it to initialize a
supervised convolutional neural network for the classification phase.
  The proposed approach substantially outperforms previous methods, improving
the previous state-of-the-art for the 3-painter classification problem from
90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63%
reduction in error rate.","DeepPainter: Painter Classification Using Deep Convolutional
  Autoencoders",2017
229,0.287498695,0.001111744,0.267295382,0.001111389,0.001111415,0.0011113,0.001111372,0.09908529,0.339452057,0.001111354,Topic9,"This paper presents a novel deep learning-based method for learning a
functional representation of mammalian neural images. The method uses a deep
convolutional denoising autoencoder (CDAE) for generating an invariant, compact
representation of in situ hybridization (ISH) images. While most existing
methods for bio-imaging analysis were not developed to handle images with
highly complex anatomical structures, the results presented in this paper show
that functional representation extracted by CDAE can help learn features of
functional gene ontology categories for their classification in a highly
accurate manner. Using this CDAE representation, our method outperforms the
previous state-of-the-art classification rate, by improving the average AUC
from 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates
on input images that were downsampled significantly with respect to the
original ones to make it computationally feasible.","DeepBrain: Functional Representation of Neural In-Situ Hybridization
  Images for Gene Ontology Classification Using Deep Convolutional Autoencoders",2017
230,0.918470089,0.001176976,0.001177682,0.001176733,0.001176703,0.001177028,0.001176667,0.001176635,0.072114804,0.001176684,Topic1,"In this paper, we propose novel generative models for creating adversarial
examples, slightly perturbed images resembling natural images but maliciously
crafted to fool pre-trained models. We present trainable deep neural networks
for transforming images to adversarial perturbations. Our proposed models can
produce image-agnostic and image-dependent perturbations for both targeted and
non-targeted attacks. We also demonstrate that similar architectures can
achieve impressive results in fooling classification and semantic segmentation
models, obviating the need for hand-crafting attack methods for each task.
Using extensive experiments on challenging high-resolution datasets such as
ImageNet and Cityscapes, we show that our perturbations achieve high fooling
rates with small perturbation norms. Moreover, our attacks are considerably
faster than current iterative methods at inference time.",Generative Adversarial Perturbations,2017
231,0.429440968,0.002041457,0.00204119,0.002041489,0.002041347,0.002041458,0.046432472,0.002041129,0.268235665,0.243642824,Topic1,"We show that simple transformations, namely translations and rotations alone,
are sufficient to fool neural network-based vision models on a significant
fraction of inputs. This is in sharp contrast to previous work that relied on
more complicated optimization approaches that are unlikely to appear outside of
a truly adversarial setting. Moreover, fooling rotations and translations are
easy to find and require only a few black-box queries to the target model.
Overall, our findings emphasize the need for designing robust classifiers even
in natural, benign contexts.","A Rotation and a Translation Suffice: Fooling CNNs with Simple
  Transformations",2017
232,0.001205183,0.001205263,0.001205065,0.001205143,0.161267958,0.001205195,0.001205269,0.001205115,0.737781022,0.092514787,Topic9,"The quest for performant networks has been a significant force that drives
the advancements of deep learning in recent years. While rewarding, improving
network design has never been an easy journey. The large design space combined
with the tremendous cost required for network training poses a major obstacle
to this endeavor. In this work, we propose a new approach to this problem,
namely, predicting the performance of a network before training, based on its
architecture. Specifically, we develop a unified way to encode individual
layers into vectors and bring them together to form an integrated description
via LSTM. Taking advantage of the recurrent network's strong expressive power,
this method can reliably predict the performances of various network
architectures. Our empirical studies showed that it not only achieved accurate
predictions but also produced consistent rankings across datasets -- a key
desideratum in performance prediction.",Peephole: Predicting Network Performance Before Training,2017
233,0.083717827,0.063922279,0.017511175,0.000671237,0.000671273,0.000671318,0.000671352,0.000671305,0.8308209,0.000671334,Topic9,"Convolutional neural networks (CNNs) are similar to ""ordinary"" neural
networks in the sense that they are made up of hidden layers consisting of
neurons with ""learnable"" parameters. These neurons receive inputs, performs a
dot product, and then follows it with a non-linearity. The whole network
expresses the mapping between raw image pixels and their class scores.
Conventionally, the Softmax function is the classifier used at the last layer
of this network. However, there have been studies (Alalshekmubarak and Smith,
2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited
studies introduce the usage of linear support vector machine (SVM) in an
artificial neural network architecture. This project is yet another take on the
subject, and is inspired by (Tang, 2013). Empirical data has shown that the
CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST
dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax
was able to achieve a test accuracy of ~99.23% using the same dataset. Both
models were also tested on the recently-published Fashion-MNIST dataset (Xiao,
Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image
classification dataset than MNIST (Zalandoresearch, 2017). This proved to be
the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax
reached a test accuracy of ~91.86%. The said results may be improved if data
preprocessing techniques were employed on the datasets, and if the base CNN
model was a relatively more sophisticated than the one used in this study.","An Architecture Combining Convolutional Neural Network (CNN) and Support
  Vector Machine (SVM) for Image Classification",2017
234,0.000909243,0.000909345,0.084283675,0.000909247,0.000909385,0.000909303,0.301405311,0.132778174,0.47607706,0.000909257,Topic9,"Artifical Neural Networks are a particular class of learning systems modeled
after biological neural functions with an interesting penchant for Hebbian
learning, that is ""neurons that wire together, fire together"". However, unlike
their natural counterparts, artificial neural networks have a close and
stringent coupling between the modules of neurons in the network. This coupling
or locking imposes upon the network a strict and inflexible structure that
prevent layers in the network from updating their weights until a full
feed-forward and backward pass has occurred. Such a constraint though may have
sufficed for a while, is now no longer feasible in the era of very-large-scale
machine learning, coupled with the increased desire for parallelization of the
learning process across multiple computing infrastructures. To solve this
problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are
introduced as a viable alternative to the backpropagation algorithm. This paper
performs a speed benchmark to compare the speed and accuracy capabilities of
SG-DNI as opposed to a standard neural interface using multilayer perceptron
MLP. SG-DNI shows good promise, in that it not only captures the learning
problem, it is also over 3-fold faster due to it asynchronous learning
capabilities.",Benchmarking Decoupled Neural Interfaces with Synthetic Gradients,2017
235,0.107618601,0.001695303,0.001695251,0.467426482,0.413087808,0.001695302,0.001695305,0.001695273,0.001695423,0.001695252,Topic4,"Image segmentation is the process of partitioning an image into a set of
meaningful regions according to some criteria. Hierarchical segmentation has
emerged as a major trend in this regard as it favors the emergence of important
regions at different scales. On the other hand, many methods allow us to have
prior information on the position of structures of interest in the images. In
this paper, we present a versatile hierarchical segmentation method that takes
into account any prior spatial information and outputs a hierarchical
segmentation that emphasizes the contours or regions of interest while
preserving the important structures in the image. An application of this method
to the weakly-supervised segmentation problem is presented.",Segmentation hirarchique faiblement supervise,2018
236,0.185816264,0.000568262,0.000568299,0.00056831,0.000568279,0.000568314,0.128229404,0.047554386,0.634990204,0.000568278,Topic9,"For fast and energy-efficient deployment of trained deep neural networks on
resource-constrained embedded hardware, each learned weight parameter should
ideally be represented and stored using a single bit. Error-rates usually
increase when this requirement is imposed. Here, we report large improvements
in error rates on multiple datasets, for deep convolutional neural networks
deployed with 1-bit-per-weight. Using wide residual networks as our main
baseline, our approach simplifies existing methods that binarize weights by
applying the sign function in training; we apply scaling factors for each layer
with constant unlearned values equal to the layer-specific standard deviations
used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with
1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve
error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We
also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test
results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error
rates halve previously reported values, and are within about 1% of our
error-rates for the same network with full-precision weights. For networks that
overfit, we also show significant improvements in error rate by not learning
batch normalization scale and offset parameters. This applies to both full
precision and 1-bit-per-weight networks. Using a warm-restart learning-rate
schedule, we found that training for 1-bit-per-weight is just as fast as
full-precision networks, with better accuracy than standard schedules, and
achieved about 98%-99% of peak performance in just 62 training epochs for
CIFAR-10/100. For full training code and trained models in MATLAB, Keras and
PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .","Training wide residual networks for deployment using a single bit for
  each weight",2018
237,0.001666898,0.001666871,0.001666892,0.001666842,0.001666873,0.15287774,0.043944165,0.001666825,0.791510025,0.00166687,Topic9,"We introduce the use of rectified linear units (ReLU) as the classification
function in a deep neural network (DNN). Conventionally, ReLU is used as an
activation function in DNNs, with Softmax function as their classification
function. However, there have been several studies on using a classification
function other than Softmax, and this study is an addition to those. We
accomplish this by taking the activation of the penultimate layer $h_{n - 1}$
in a neural network, then multiply it by weight parameters $\theta$ to get the
raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$,
i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide
class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.",Deep Learning using Rectified Linear Units (ReLU),2018
238,0.000826728,0.000826689,0.342439483,0.000826636,0.28722758,0.122190376,0.000826688,0.000826601,0.243182616,0.000826604,Topic3,"We propose rectified factor networks (RFNs) to efficiently construct very
sparse, non-linear, high-dimensional representations of the input. RFN models
identify rare and small events in the input, have a low interference between
code units, have a small reconstruction error, and explain the data covariance
structure. RFN learning is a generalized alternating minimization algorithm
derived from the posterior regularization method which enforces non-negative
and normalized posterior means. We proof convergence and correctness of the RFN
learning algorithm. On benchmarks, RFNs are compared to other unsupervised
methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to
previous sparse coding methods, RFNs yield sparser codes, capture the data's
covariance structure more precisely, and have a significantly smaller
reconstruction error. We test RFNs as pretraining technique for deep networks
on different vision datasets, where RFNs were superior to RBMs and
autoencoders. On gene expression data from two pharmaceutical drug discovery
studies, RFNs detected small and rare gene modules that revealed highly
relevant new biological insights which were so far missed by other unsupervised
methods.",Rectified Factor Networks,2015
239,0.001087241,0.085098676,0.001087268,0.001087169,0.001087277,0.066943344,0.00108718,0.001087175,0.840347523,0.001087148,Topic9,"Motivated by an important insight from neural science, we propose a new
framework for understanding the success of the recently proposed ""maxout""
networks. The framework is based on encoding information on sparse pathways and
recognizing the correct pathway at inference time. Elaborating further on this
insight, we propose a novel deep network architecture, called ""channel-out""
network, which takes a much better advantage of sparse pathway encoding. In
channel-out networks, pathways are not only formed a posteriori, but they are
also actively selected according to the inference outputs from the lower
layers. From a mathematical perspective, channel-out networks can represent a
wider class of piece-wise continuous functions, thereby endowing the network
with more expressive power than that of maxout networks. We test our
channel-out networks on several well-known image classification benchmarks,
setting new state-of-the-art performance on CIFAR-100 and STL-10, which
represent some of the ""harder"" image classification benchmarks.",From Maxout to Channel-Out: Encoding Information on Sparse Pathways,2013
240,0.002778353,0.00277856,0.002778474,0.002778366,0.002778142,0.00277814,0.002778111,0.002778202,0.974995363,0.002778289,Topic9,"We propose a novel learning method for multilayered neural networks which
uses feedforward supervisory signal and associates classification of a new
input with that of pre-trained input. The proposed method effectively uses rich
input information in the earlier layer for robust leaning and revising internal
representation in a multilayer neural network.","Competitive Learning with Feedforward Supervisory Signal for Pre-trained
  Multilayered Networks",2013
241,0.001000306,0.001000229,0.001000164,0.001000214,0.001000259,0.00100025,0.110490132,0.001000171,0.881508052,0.001000223,Topic9,"Our proposed deeply-supervised nets (DSN) method simultaneously minimizes
classification error while making the learning process of hidden layers direct
and transparent. We make an attempt to boost the classification performance by
studying a new formulation in deep networks. Three aspects in convolutional
neural networks (CNN) style architectures are being looked at: (1) transparency
of the intermediate layers to the overall classification; (2)
discriminativeness and robustness of learned features, especially in the early
layers; (3) effectiveness in training due to the presence of the exploding and
vanishing gradients. We introduce ""companion objective"" to the individual
hidden layers, in addition to the overall objective at the output layer (a
different strategy to layer-wise pre-training). We extend techniques from
stochastic gradient methods to analyze our algorithm. The advantage of our
method is evident and our experimental result on benchmark datasets shows
significant performance gain over existing methods (e.g. all state-of-the-art
results on MNIST, CIFAR-10, CIFAR-100, and SVHN).",Deeply-Supervised Nets,2014
242,0.00212796,0.002128092,0.143076992,0.002128275,0.002127921,0.0021282,0.543095872,0.002127896,0.298930887,0.002127905,Topic7,"We revisit the choice of SGD for training deep neural networks by
reconsidering the appropriate geometry in which to optimize the weights. We
argue for a geometry invariant to rescaling of weights that does not affect the
output of the network, and suggest Path-SGD, which is an approximate steepest
descent method with respect to a path-wise regularizer related to max-norm
regularization. Path-SGD is easy and efficient to implement and leads to
empirical gains over SGD and AdaGrad.",Path-SGD: Path-Normalized Optimization in Deep Neural Networks,2015
243,0.001562755,0.001562791,0.001562791,0.001562807,0.00156274,0.001562805,0.225541478,0.001562797,0.761956302,0.001562733,Topic9,"The Resilient Propagation (Rprop) algorithm has been very popular for
backpropagation training of multilayer feed-forward neural networks in various
applications. The standard Rprop however encounters difficulties in the context
of deep neural networks as typically happens with gradient-based learning
algorithms. In this paper, we propose a modification of the Rprop that combines
standard Rprop steps with a special drop out technique. We apply the method for
training Deep Neural Networks as standalone components and in ensemble
formulations. Results on the MNIST dataset show that the proposed modification
alleviates standard Rprop's problems demonstrating improved learning speed and
accuracy.",Adapting Resilient Propagation for Deep Learning,2015
244,0.000971082,0.000971105,0.229610215,0.000971128,0.227963391,0.000971114,0.000971059,0.362899808,0.173700062,0.000971036,Topic8,"Autism Spectrum Disorders (ASDs) are often associated with specific atypical
postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have
a specific visibility. While the identification and the quantification of SMM
patterns remain complex, its automation would provide support to accurate
tuning of the intervention in the therapy of autism. Therefore, it is essential
to develop automatic SMM detection systems in a real world setting, taking care
of strong inter-subject and intra-subject variability. Wireless accelerometer
sensing technology can provide a valid infrastructure for real-time SMM
detection, however such variability remains a problem also for machine learning
methods, in particular whenever handcrafted features extracted from
accelerometer signal are considered. Here, we propose to employ the deep
learning paradigm in order to learn discriminating features from multi-sensor
accelerometer signals. Our results provide preliminary evidence that feature
learning and transfer learning embedded in the deep architecture achieve higher
accurate SMM detectors in longitudinal scenarios.","Convolutional Neural Network for Stereotypical Motor Movement Detection
  in Autism",2015
245,0.145665695,0.002174229,0.002174106,0.002174231,0.002174235,0.002174373,0.002174393,0.002174246,0.836940296,0.002174196,Topic9,"Residual networks (ResNets) have recently achieved state-of-the-art on
challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep
dual-stream architecture that generalizes ResNets and standard CNNs and is
easily implemented with no computational overhead. RiR consistently improves
performance over ResNets, outperforms architectures with similar amounts of
augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.",Resnet in Resnet: Generalizing Residual Architectures,2016
246,0.049179765,0.000934858,0.000934778,0.000934733,0.000934895,0.000934773,0.294992497,0.097090538,0.553128429,0.000934735,Topic9,"There has been significant recent interest towards achieving highly efficient
deep neural network architectures. A promising paradigm for achieving this is
the concept of evolutionary deep intelligence, which attempts to mimic
biological evolution processes to synthesize highly-efficient deep neural
networks over successive generations. An important aspect of evolutionary deep
intelligence is the genetic encoding scheme used to mimic heredity, which can
have a significant impact on the quality of offspring deep neural networks.
Motivated by the neurobiological phenomenon of synaptic clustering, we
introduce a new genetic encoding scheme where synaptic probability is driven
towards the formation of a highly sparse set of synaptic clusters. Experimental
results for the task of image classification demonstrated that the synthesized
offspring networks using this synaptic cluster-driven genetic encoding scheme
can achieve state-of-the-art performance while having network architectures
that are not only significantly more efficient (with a ~125-fold decrease in
synapses for MNIST) compared to the original ancestor network, but also
tailored for GPU-accelerated machine learning applications.","Evolutionary Synthesis of Deep Neural Networks via Synaptic
  Cluster-driven Genetic Encoding",2016
247,0.699995218,0.001234752,0.001235138,0.001234837,0.00123481,0.001234861,0.001234931,0.042794067,0.248566512,0.001234874,Topic1,"The increasingly photorealistic sample quality of generative image models
suggests their feasibility in applications beyond image generation. We present
the Neural Photo Editor, an interface that leverages the power of generative
neural networks to make large, semantically coherent changes to existing
images. To tackle the challenge of achieving accurate reconstructions without
loss of feature quality, we introduce the Introspective Adversarial Network, a
novel hybridization of the VAE and GAN. Our model efficiently captures
long-range dependencies through use of a computational block based on
weight-shared dilated convolutions, and improves generalization performance
with Orthogonal Regularization, a novel weight regularization method. We
validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples
and reconstructions with high visual fidelity.",Neural Photo Editing with Introspective Adversarial Networks,2016
248,0.094719631,0.000806657,0.000806586,0.000806663,0.000806641,0.041772984,0.331939515,0.00080659,0.52672815,0.000806581,Topic9,"We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of accuracy. Rather
than attempting to redesign or approximate existing networks, we propose two
schemes that adaptively utilize networks. We first pose an adaptive network
evaluation scheme, where we learn a system to adaptively choose the components
of a deep network to be evaluated for each example. By allowing examples
correctly classified using early layers of the system to exit, we avoid the
computational time associated with full evaluation of the network. We extend
this to learn a network selection system that adaptively selects the network to
be evaluated for each example. We show that computational time can be
dramatically reduced by exploiting the fact that many examples can be correctly
classified using relatively efficient networks and that complex,
computationally costly networks are only necessary for a small fraction of
examples. We pose a global objective for learning an adaptive early exit or
network selection policy and solve it by reducing the policy learning problem
to a layer-by-layer weighted binary classification problem. Empirically, these
approaches yield dramatic reductions in computational cost, with up to a 2.8x
speedup on state-of-the-art networks from the ImageNet image recognition
challenge with minimal (<1%) loss of top5 accuracy.",Adaptive Neural Networks for Efficient Inference,2017
249,0.627297964,0.001041893,0.092671147,0.001041863,0.001041834,0.213775637,0.00104184,0.060003926,0.001041976,0.00104192,Topic1,"The key idea of variational auto-encoders (VAEs) resembles that of
traditional auto-encoder models in which spatial information is supposed to be
explicitly encoded in the latent space. However, the latent variables in VAEs
are vectors, which are commonly interpreted as multiple feature maps of size
1x1. Such representations can only convey spatial information implicitly when
coupled with powerful decoders. In this work, we propose spatial VAEs that use
latent variables as feature maps of larger size to explicitly capture spatial
information. This is achieved by allowing the latent variables to be sampled
from matrix-variate normal (MVN) distributions whose parameters are computed
from the encoder network. To increase dependencies among locations on latent
feature maps and reduce the number of parameters, we further propose spatial
VAEs via low-rank MVN distributions. Experimental results show that the
proposed spatial VAEs outperform original VAEs in capturing rich structural and
spatial information.","Spatial Variational Auto-Encoding via Matrix-Variate Normal
  Distributions",2017
250,0.345232157,0.001075536,0.001075479,0.16693383,0.001075644,0.001075546,0.00107555,0.001075538,0.48030508,0.001075639,Topic9,"The key idea of current deep learning methods for dense prediction is to
apply a model on a regular patch centered on each pixel to make pixel-wise
predictions. These methods are limited in the sense that the patches are
determined by network architecture instead of learned from data. In this work,
we propose the dense transformer networks, which can learn the shapes and sizes
of patches from data. The dense transformer networks employ an encoder-decoder
architecture, and a pair of dense transformer modules are inserted into each of
the encoder and decoder paths. The novelty of this work is that we provide
technical solutions for learning the shapes and sizes of patches from data and
efficiently restoring the spatial correspondence required for dense prediction.
The proposed dense transformer modules are differentiable, thus the entire
network can be trained. We apply the proposed networks on natural and
biological image segmentation tasks and show superior performance is achieved
in comparison to baseline methods.",Dense Transformer Networks,2017
251,0.001111275,0.001111329,0.060550473,0.001111305,0.001111388,0.08417264,0.435233425,0.001111374,0.41337552,0.00111127,Topic7,"We develop an algorithm for systematic design of a large artificial neural
network using a progression property. We find that some non-linear functions,
such as the rectifier linear unit and its derivatives, hold the property. The
systematic design addresses the choice of network size and regularization of
parameters. The number of nodes and layers in network increases in progression
with the objective of consistently reducing an appropriate cost. Each layer is
optimized at a time, where appropriate parameters are learned using convex
optimization. Regularization parameters for convex optimization do not need a
significant manual effort for tuning. We also use random instances for some
weight matrices, and that helps to reduce the number of parameters we learn.
The developed network is expected to show good generalization power due to
appropriate regularization and use of random weights in the layers. This
expectation is verified by extensive experiments for classification and
regression problems, using standard databases.",Progressive Learning for Systematic Design of Large Neural Networks,2017
252,0.600626765,0.001111395,0.001111365,0.001111316,0.306937904,0.08465537,0.001111464,0.001111358,0.001111471,0.001111592,Topic1,"A fundamental, and still largely unanswered, question in the context of
Generative Adversarial Networks (GANs) is whether GANs are actually able to
capture the key characteristics of the datasets they are trained on. The
current approaches to examining this issue require significant human
supervision, such as visual inspection of sampled images, and often offer only
fairly limited scalability. In this paper, we propose new techniques that
employ a classification-based perspective to evaluate synthetic GAN
distributions and their capability to accurately reflect the essential
properties of the training data. These techniques require only minimal human
supervision and can easily be scaled and adapted to evaluate a variety of
state-of-the-art GANs on large, popular datasets. Our analysis indicates that
GANs have significant problems in reproducing the more distributional
properties of the training dataset. In particular, when seen through the lens
of classification, the diversity of GAN data is orders of magnitude less than
that of the original data.",A Classification-Based Perspective on GAN Distributions,2017
253,0.308168567,0.315142392,0.000934757,0.000934772,0.000934858,0.000934844,0.000934824,0.000934802,0.370145297,0.000934886,Topic9,"Achieving artificial visual reasoning - the ability to answer image-related
questions which require a multi-step, high-level process - is an important step
towards artificial general intelligence. This multi-modal task requires
learning a question-dependent, structured reasoning process over images from
language. Standard deep learning approaches tend to exploit biases in the data
rather than learn this underlying structure, while leading methods learn to
visually reason successfully but are hand-crafted for reasoning. We show that a
general-purpose, Conditional Batch Normalization approach achieves
state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%
error rate. We outperform the next best end-to-end method (4.5%) and even
methods that use extra supervision (3.1%). We probe our model to shed light on
how it reasons, showing it has learned a question-dependent, multi-step
process. Previous work has operated under the assumption that visual reasoning
calls for a specialized architecture, but we show that a general architecture
with proper conditioning can learn to visually reason effectively.",Learning Visual Reasoning Without Strong Priors,2017
254,0.234044785,0.000909302,0.000909361,0.000909355,0.28761666,0.000909365,0.119495261,0.000909317,0.144700289,0.209596303,Topic5,"Language is increasingly being used to define rich visual recognition
problems with supporting image collections sourced from the web. Structured
prediction models are used in these tasks to take advantage of correlations
between co-occurring labels and visual input but risk inadvertently encoding
social biases found in web corpora. In this work, we study data and models
associated with multilabel object classification and visual semantic role
labeling. We find that (a) datasets for these tasks contain significant gender
bias and (b) models trained on these datasets further amplify existing bias.
For example, the activity cooking is over 33% more likely to involve females
than males in a training set, and a trained model further amplifies the
disparity to 68% at test time. We propose to inject corpus-level constraints
for calibrating existing structured prediction models and design an algorithm
based on Lagrangian relaxation for collective inference. Our method results in
almost no performance loss for the underlying recognition task but decreases
the magnitude of bias amplification by 47.5% and 40.5% for multilabel
classification and visual semantic role labeling, respectively.","Men Also Like Shopping: Reducing Gender Bias Amplification using
  Corpus-level Constraints",2017
255,0.19294039,0.055899816,0.000666781,0.079015123,0.318475063,0.000666888,0.00066679,0.098862449,0.000666863,0.252139837,Topic5,"Spatial understanding is a fundamental problem with wide-reaching real-world
applications. The representation of spatial knowledge is often modeled with
spatial templates, i.e., regions of acceptability of two objects under an
explicit spatial relationship (e.g., ""on"", ""below"", etc.). In contrast with
prior work that restricts spatial templates to explicit spatial prepositions
(e.g., ""glass on table""), here we extend this concept to implicit spatial
language, i.e., those relationships (generally actions) for which the spatial
arrangement of the objects is only implicitly implied (e.g., ""man riding
horse""). In contrast with explicit relationships, predicting spatial
arrangements from implicit spatial language requires significant common sense
spatial understanding. Here, we introduce the task of predicting spatial
templates for two objects under a relationship, which can be seen as a spatial
question-answering task with a (2D) continuous output (""where is the man w.r.t.
a horse when the man is walking the horse?""). We present two simple
neural-based models that leverage annotated images and structured text to learn
this task. The good performance of these models reveals that spatial locations
are to a large extent predictable from implicit spatial language. Crucially,
the models attain similar performance in a challenging generalized setting,
where the object-relation-object combinations (e.g.,""man walking dog"") have
never been seen before. Next, we go one step further by presenting the models
with unseen objects (e.g., ""dog""). In this scenario, we show that leveraging
word embeddings enables the models to output accurate spatial predictions,
proving that the models acquire solid common sense spatial knowledge allowing
for such generalization.","Acquiring Common Sense Spatial Knowledge through Implicit Spatial
  Templates",2017
256,0.001235005,0.167419768,0.001234898,0.001235095,0.001234867,0.001234844,0.001234779,0.001234856,0.822700916,0.001234971,Topic9,"We introduce a general-purpose conditioning method for neural networks called
FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network
computation via a simple, feature-wise affine transformation based on
conditioning information. We show that FiLM layers are highly effective for
visual reasoning - answering image-related questions which require a
multi-step, high-level process - a task which has proven difficult for standard
deep learning methods that do not explicitly model reasoning. Specifically, we
show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error
for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are
robust to ablations and architectural modifications, and 4) generalize well to
challenging, new data from few examples or even zero-shot.",FiLM: Visual Reasoning with a General Conditioning Layer,2017
257,0.099461859,0.001408839,0.165188125,0.001408847,0.001408756,0.001408817,0.001408708,0.001408622,0.001408739,0.725488688,Topic10,"We introduce a new approach to unsupervised estimation of feature-rich
semantic role labeling models. Our model consists of two components: (1) an
encoding component: a semantic role labeling model which predicts roles given a
rich set of syntactic and lexical features; (2) a reconstruction component: a
tensor factorization model which relies on roles to predict argument fillers.
When the components are estimated jointly to minimize errors in argument
reconstruction, the induced roles largely correspond to roles defined in
annotated resources. Our method performs on par with most accurate role
induction methods on English and German, even though, unlike these previous
approaches, we do not incorporate any prior linguistic knowledge about the
languages.","Unsupervised Induction of Semantic Roles within a Reconstruction-Error
  Minimization Framework",2014
258,0.000769411,0.000769413,0.000769469,0.000769425,0.465230584,0.000769405,0.108724883,0.000769393,0.000769398,0.42065862,Topic5,"The blind application of machine learning runs the risk of amplifying biases
present in data. Such a danger is facing us with word embedding, a popular
framework to represent text data as vectors which has been used in many machine
learning and natural language processing tasks. We show that even word
embeddings trained on Google News articles exhibit female/male gender
stereotypes to a disturbing extent. This raises concerns because their
widespread use, as we describe, often tends to amplify these biases.
Geometrically, gender bias is first shown to be captured by a direction in the
word embedding. Second, gender neutral words are shown to be linearly separable
from gender definition words in the word embedding. Using these properties, we
provide a methodology for modifying an embedding to remove gender stereotypes,
such as the association between between the words receptionist and female,
while maintaining desired associations such as between the words queen and
female. We define metrics to quantify both direct and indirect gender biases in
embeddings, and develop algorithms to ""debias"" the embedding. Using
crowd-worker evaluation as well as standard benchmarks, we empirically
demonstrate that our algorithms significantly reduce gender bias in embeddings
while preserving the its useful properties such as the ability to cluster
related concepts and to solve analogy tasks. The resulting embeddings can be
used in applications without amplifying gender bias.","Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
  Embeddings",2016
259,0.000719593,0.000719556,0.00071955,0.088302387,0.000719557,0.000719613,0.000719563,0.000719569,0.171631097,0.735029514,Topic10,"In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based
language model designed to directly capture the global semantic meaning
relating words in a document via latent topics. Because of their sequential
nature, RNNs are good at capturing the local structure of a word sequence -
both semantic and syntactic - but might face difficulty remembering long-range
dependencies. Intuitively, these long-range dependencies are of semantic
nature. In contrast, latent topic models are able to capture the global
underlying semantic structure of a document but do not account for word
ordering. The proposed TopicRNN model integrates the merits of RNNs and latent
topic models: it captures local (syntactic) dependencies using an RNN and
global (semantic) dependencies using latent topics. Unlike previous work on
contextual RNN language modeling, our model is learned end-to-end. Empirical
results on word prediction show that TopicRNN outperforms existing contextual
RNN baselines. In addition, TopicRNN can be used as an unsupervised feature
extractor for documents. We do this for sentiment analysis on the IMDB movie
review dataset and report an error rate of $6.28\%$. This is comparable to the
state-of-the-art $5.91\%$ resulting from a semi-supervised approach. Finally,
TopicRNN also yields sensible topics, making it a useful alternative to
document models such as latent Dirichlet allocation.",TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,2016
260,0.001219846,0.134299717,0.001219754,0.00121993,0.001219865,0.29835303,0.001219758,0.037636546,0.046082343,0.477529211,Topic10,"We propose the Gaussian attention model for content-based neural memory
access. With the proposed attention model, a neural network has the additional
degree of freedom to control the focus of its attention from a laser sharp
attention to a broad attention. It is applicable whenever we can assume that
the distance in the latent space reflects some notion of semantics. We use the
proposed attention model as a scoring function for the embedding of a knowledge
base into a continuous vector space and then train a model that performs
question answering about the entities in the knowledge base. The proposed
attention model can handle both the propagation of uncertainty when following a
series of relations and also the conjunction of conditions in a natural way. On
a dataset of soccer players who participated in the FIFA World Cup 2014, we
demonstrate that our model can handle both path queries and conjunctive queries
well.","Gaussian Attention Model and Its Application to Knowledge Base Embedding
  and Question Answering",2016
261,0.033351177,0.001176751,0.001176709,0.001176764,0.001176894,0.171926555,0.001176846,0.001176762,0.600674248,0.186987296,Topic9,"Recurrent neural networks (RNNs) have been used extensively and with
increasing success to model various types of sequential data. Much of this
progress has been achieved through devising recurrent units and architectures
with the flexibility to capture complex statistics in the data, such as long
range dependency or localized attention phenomena. However, while many
sequential data (such as video, speech or language) can have highly variable
information flow, most recurrent models still consume input features at a
constant rate and perform a constant number of computations per time step,
which can be detrimental to both speed and model capacity. In this paper, we
explore a modification to existing recurrent units which allows them to learn
to vary the amount of computation they perform at each step, without prior
knowledge of the sequence's time structure. We show experimentally that not
only do our models require fewer operations, they also lead to better
performance overall on evaluation tasks.",Variable Computation in Recurrent Neural Networks,2016
262,0.162533423,0.001428814,0.001428917,0.001428848,0.001428947,0.102814331,0.00142902,0.0014288,0.724650008,0.001428891,Topic9,"In this paper, we propose a method for training neural networks when we have
a large set of data with weak labels and a small amount of data with true
labels. In our proposed model, we train two neural networks: a target network,
the learner and a confidence network, the meta-learner. The target network is
optimized to perform a given task and is trained using a large set of unlabeled
data that are weakly annotated. We propose to control the magnitude of the
gradient updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model.",Learning to Learn from Weak Supervision by Full Supervision,2017
263,0.083575061,0.09814198,0.000806574,0.000806605,0.278671843,0.000806694,0.000806806,0.091569833,0.378236961,0.066577643,Topic9,"Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2vec, a deep RNN
that automatically learns features from SMILES to predict chemical properties,
without the need for additional explicit feature engineering. Using Bayesian
optimization methods to tune the network architecture, we show that an
optimized SMILES2vec model can serve as a general-purpose neural network for
predicting distinct chemical properties including toxicity, activity,
solubility and solvation energy, while also outperforming contemporary MLP
neural networks that uses engineered features. Furthermore, we demonstrate
proof-of-concept of interpretability by developing an explanation mask that
localizes on the most important characters used in making a prediction. When
tested on the solubility dataset, it identified specific parts of a chemical
that is consistent with established first-principles knowledge with an accuracy
of 88%. Our work demonstrates that neural networks can learn technically
accurate chemical concept and provide state-of-the-art accuracy, making
interpretable deep neural networks a useful tool of relevance to the chemical
industry.","SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for
  Predicting Chemical Properties",2017
264,0.000819893,0.000819877,0.000819827,0.00081985,0.153635173,0.521016049,0.000819937,0.000819882,0.140153913,0.180275598,Topic6,"In spoken dialogue systems, we aim to deploy artificial intelligence to build
automated dialogue agents that can converse with humans. A part of this effort
is the policy optimisation task, which attempts to find a policy describing how
to respond to humans, in the form of a function taking the current state of the
dialogue and returning the response of the system. In this paper, we
investigate deep reinforcement learning approaches to solve this problem.
Particular attention is given to actor-critic methods, off-policy reinforcement
learning with experience replay, and various methods aimed at reducing the bias
and variance of estimators. When combined, these methods result in the
previously proposed ACER algorithm that gave competitive results in gaming
environments. These environments however are fully observable and have a
relatively small action set so in this paper we examine the application of ACER
to dialogue policy optimisation. We show that this method beats the current
state-of-the-art in deep learning approaches for spoken dialogue systems. This
not only leads to a more sample efficient algorithm that can train faster, but
also allows us to apply the algorithm in more difficult environments than
before. We thus experiment with learning in a very large action space, which
has two orders of magnitude more actions than previously considered. We find
that ACER trains significantly faster than the current state-of-the-art.","Sample Efficient Deep Reinforcement Learning for Dialogue Systems with
  Large Action Spaces",2018
265,0.002564495,0.234172858,0.332209933,0.002564633,0.002564472,0.002564911,0.002564898,0.002564423,0.002564471,0.415664905,Topic10,"In this paper we explore the ""vector semantics"" problem from the perspective
of ""almost orthogonal"" property of high-dimensional random vectors. We show
that this intriguing property can be used to ""memorize"" random vectors by
simply adding them, and we provide an efficient probabilistic solution to the
set membership problem. Also, we discuss several applications to word context
vector embeddings, document sentences similarity, and spam filtering.",High-Dimensional Vector Semantics,2018
266,0.001786265,0.231748959,0.001786319,0.001786302,0.001786102,0.001786448,0.001786346,0.001786197,0.001786281,0.753960782,Topic10,"Induction of common sense knowledge about prototypical sequences of events
has recently received much attention. Instead of inducing this knowledge in the
form of graphs, as in much of the previous work, in our method, distributed
representations of event realizations are computed based on distributed
representations of predicates and their arguments, and then these
representations are used to predict prototypical event orderings. The
parameters of the compositional process for computing the event representations
and the ranking component of the model are jointly estimated from texts. We
show that this approach results in a substantial boost in ordering performance
with respect to previous methods.",Learning Semantic Script Knowledge with Event Embeddings,2013
267,0.000645291,0.026109393,0.000645288,0.000645313,0.383346367,0.000645394,0.371655368,0.000645316,0.030247104,0.185415167,Topic5,"While computer and communication technologies have provided effective means
to scale up many aspects of education, the submission and grading of
assessments such as homework assignments and tests remains a weak link. In this
paper, we study the problem of automatically grading the kinds of open response
mathematical questions that figure prominently in STEM (science, technology,
engineering, and mathematics) courses. Our data-driven framework for
mathematical language processing (MLP) leverages solution data from a large
number of learners to evaluate the correctness of their solutions, assign
partial-credit scores, and provide feedback to each learner on the likely
locations of any errors. MLP takes inspiration from the success of natural
language processing for text data and comprises three main steps. First, we
convert each solution to an open response mathematical question into a series
of numerical features. Second, we cluster the features from several solutions
to uncover the structures of correct, partially correct, and incorrect
solutions. We develop two different clustering approaches, one that leverages
generic clustering algorithms and one based on Bayesian nonparametrics. Third,
we automatically grade the remaining (potentially large number of) solutions
based on their assigned cluster and one instructor-provided grade per cluster.
As a bonus, we can track the cluster assignment of each step of a multistep
solution and determine when it departs from a cluster of correct solutions,
which enables us to indicate the likely locations of errors to learners. We
test and validate MLP on real-world MOOC data to demonstrate how it can
substantially reduce the human effort required in large-scale educational
platforms.","Mathematical Language Processing: Automatic Grading and Feedback for
  Open Response Mathematical Questions",2015
268,0.00052643,0.000526433,0.000526506,0.224011242,0.00052653,0.352676785,0.000526412,0.000526436,0.000526442,0.419626783,Topic10,"Human infants can discover words directly from unsegmented speech signals
without any explicitly labeled data. In this paper, we develop a novel machine
learning method called nonparametric Bayesian double articulation analyzer
(NPB-DAA) that can directly acquire language and acoustic models from observed
continuous speech signals. For this purpose, we propose an integrative
generative model that combines a language model and an acoustic model into a
single generative model called the ""hierarchical Dirichlet process hidden
language model"" (HDP-HLM). The HDP-HLM is obtained by extending the
hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by
Johnson et al. An inference procedure for the HDP-HLM is derived using the
blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure
enables the simultaneous and direct inference of language and acoustic models
from continuous speech signals. Based on the HDP-HLM and its inference
procedure, we developed a novel double articulation analyzer. By assuming
HDP-HLM as a generative model of observed time series data, and by inferring
latent variables of the model, the method can analyze latent double
articulation structure, i.e., hierarchically organized latent words and
phonemes, of the data in an unsupervised manner. The novel unsupervised double
articulation analyzer is called NPB-DAA.
  The NPB-DAA can automatically estimate double articulation structure embedded
in speech signals. We also carried out two evaluation experiments using
synthetic data and actual human continuous speech signals representing Japanese
vowel sequences. In the word acquisition and phoneme categorization tasks, the
NPB-DAA outperformed a conventional double articulation analyzer (DAA) and
baseline automatic speech recognition system whose acoustic model was trained
in a supervised manner.","Nonparametric Bayesian Double Articulation Analyzer for Direct Language
  Acquisition from Continuous Speech Signals",2015
269,0.059863662,0.25358067,0.00149284,0.001492782,0.001492768,0.001492796,0.001492816,0.001492781,0.600836783,0.076762101,Topic9,"Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.",Harnessing Deep Neural Networks with Logic Rules,2016
270,0.693544362,0.00123488,0.001234796,0.00123472,0.001234829,0.050398135,0.001234819,0.001234755,0.001234858,0.247413846,Topic1,"Generic generation and manipulation of text is challenging and has limited
success compared to recent deep generative modeling in visual domain. This
paper aims at generating plausible natural language sentences, whose attributes
are dynamically controlled by learning disentangled latent representations with
designated semantics. We propose a new neural generative model which combines
variational auto-encoders and holistic attribute discriminators for effective
imposition of semantic structures. With differentiable approximation to
discrete text samples, explicit constraints on independent attribute controls,
and efficient collaborative learning of generator and discriminators, our model
learns highly interpretable representations from even only word annotations,
and produces realistic sentences with desired attributes. Quantitative
evaluation validates the accuracy of sentence and attribute generation.",Toward Controlled Generation of Text,2017
271,0.156202432,0.001538826,0.001538661,0.001538714,0.08953318,0.001538822,0.00153875,0.001538703,0.374130652,0.370901259,Topic9,"Implicit discourse relation classification is of great challenge due to the
lack of connectives as strong linguistic cues, which motivates the use of
annotated implicit connectives to improve the recognition. We propose a feature
imitation framework in which an implicit relation network is driven to learn
from another neural network with access to connectives, and thus encouraged to
extract similarly salient features for accurate classification. We develop an
adversarial model to enable an adaptive imitation scheme through competition
between the implicit network and a rival feature discriminator. Our method
effectively transfers discriminability of connectives to the implicit features,
and achieves state-of-the-art performance on the PDTB benchmark.","Adversarial Connective-exploiting Networks for Implicit Discourse
  Relation Classification",2017
272,0.001428958,0.114101636,0.001428861,0.001428806,0.001428836,0.00142893,0.00142908,0.001428917,0.130554593,0.745341382,Topic10,"Tasks like code generation and semantic parsing require mapping unstructured
(or partially structured) inputs to well-formed, executable outputs. We
introduce abstract syntax networks, a modeling framework for these problems.
The outputs are represented as abstract syntax trees (ASTs) and constructed by
a decoder with a dynamically-determined modular structure paralleling the
structure of the output tree. On the benchmark Hearthstone dataset for code
generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy,
compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we
perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with
no task-specific engineering.",Abstract Syntax Networks for Code Generation and Semantic Parsing,2017
273,0.001923598,0.001923578,0.001923374,0.001923451,0.001923341,0.32639132,0.001923546,0.001923387,0.001923571,0.658220835,Topic10,"Word embeddings provide point representations of words containing useful
semantic information. We introduce multimodal word distributions formed from
Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty
information. To learn these distributions, we propose an energy-based
max-margin objective. We show that the resulting approach captures uniquely
expressive semantic information, and outperforms alternatives, such as word2vec
skip-grams, and Gaussian embeddings, on benchmark datasets such as word
similarity and entailment.",Multimodal Word Distributions,2017
274,0.001389353,0.001389168,0.001389118,0.001389223,0.001389257,0.545673789,0.00138921,0.001389171,0.001389423,0.443212287,Topic6,"In this work we present a technique to use natural language to help
reinforcement learning generalize to unseen environments. This technique uses
neural machine translation, specifically the use of encoder-decoder networks,
to learn associations between natural language behavior descriptions and
state-action information. We then use this learned model to guide agent
exploration using a modified version of policy shaping to make it more
effective at learning in unseen environments. We evaluate this technique using
the popular arcade game, Frogger, under ideal and non-ideal conditions. This
evaluation shows that our modified policy shaping algorithm improves over a
Q-learning agent as well as a baseline version of policy shaping.",Guiding Reinforcement Learning Exploration Using Natural Language,2017
275,0.000675909,0.0334523,0.24407212,0.000675809,0.000675834,0.000675866,0.252669679,0.000675781,0.308201533,0.158225169,Topic9,"We investigate task clustering for deep-learning based multi-task and
few-shot learning in a many-task setting. We propose a new method to measure
task similarities with cross-task transfer performance matrix for the deep
learning scenario. Although this matrix provides us critical information
regarding similarity between tasks, its asymmetric property and unreliable
performance scores can affect conventional clustering methods adversely.
Additionally, the uncertain task-pairs, i.e., the ones with extremely
asymmetric transfer scores, may collectively mislead clustering algorithms to
output an inaccurate task-partition. To overcome these limitations, we propose
a novel task-clustering algorithm by using the matrix completion technique. The
proposed algorithm constructs a partially-observed similarity matrix based on
the certainty of cluster membership of the task-pairs. We then use a matrix
completion algorithm to complete the similarity matrix. Our theoretical
analysis shows that under mild constraints, the proposed algorithm will
perfectly recover the underlying ""true"" similarity matrix with a high
probability. Our results show that the new task clustering method can discover
task clusters for training flexible and superior neural network models in a
multi-task learning setup for sentiment classification and dialog intent
classification tasks. Our task clustering approach also extends metric-based
few-shot learning methods to adapt multiple metrics, which demonstrates
empirical advantages when the tasks are diverse.",Robust Task Clustering for Deep Many-Task Learning,2017
276,0.154919488,0.001667145,0.001667091,0.001667238,0.001666963,0.001667225,0.001667183,0.001667081,0.001667153,0.831743434,Topic10,"We train multi-task autoencoders on linguistic tasks and analyze the learned
hidden sentence representations. The representations change significantly when
translation and part-of-speech decoders are added. The more decoders a model
employs, the better it clusters sentences according to their syntactic
similarity, as the representation space becomes less entangled. We explore the
structure of the representation space by interpolating between sentences, which
yields interesting pseudo-English sentences, many of which have recognizable
syntactic structure. Lastly, we point out an interesting property of our
models: The difference-vector between two sentences can be added to change a
third sentence with similar features in a meaningful way.","Natural Language Multitasking: Analyzing and Improving Syntactic
  Saliency of Hidden Representations",2018
277,0.263812694,0.00063702,0.000637069,0.000637111,0.000637086,0.000637055,0.03960613,0.08469248,0.085229657,0.523473698,Topic10,"With the increasing popularity of video sharing websites such as YouTube and
Facebook, multimodal sentiment analysis has received increasing attention from
the scientific community. Contrary to previous works in multimodal sentiment
analysis which focus on holistic information in speech segments such as bag of
words representations and average facial expression intensity, we develop a
novel deep architecture for multimodal sentiment analysis that performs
modality fusion at the word level. In this paper, we propose the Gated
Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is
composed of 2 modules. The Gated Multimodal Embedding alleviates the
difficulties of fusion when there are noisy modalities. The LSTM with Temporal
Attention performs word level fusion at a finer fusion resolution between input
modalities and attends to the most important time steps. As a result, the
GME-LSTM(A) is able to better model the multimodal structure of speech through
time and perform better sentiment comprehension. We demonstrate the
effectiveness of this approach on the publicly-available Multimodal Corpus of
Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving
state-of-the-art sentiment classification and regression results. Qualitative
analysis on our model emphasizes the importance of the Temporal Attention Layer
in sentiment prediction because the additional acoustic and visual modalities
are noisy. We also demonstrate the effectiveness of the Gated Multimodal
Embedding in selectively filtering these noisy modalities out. Our results and
analysis open new areas in the study of sentiment analysis in human
communication and provide new models for multimodal fusion.","Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement
  Learning",2018
278,0.00129912,0.001299026,0.001298933,0.001299009,0.268208122,0.001298941,0.001299053,0.001298947,0.072209939,0.650488911,Topic10,"Automatic summarisation is a popular approach to reduce a document to its
main arguments. Recent research in the area has focused on neural approaches to
summarisation, which can be very data-hungry. However, few large datasets exist
and none for the traditionally popular domain of scientific publications, which
opens up challenging research avenues centered on encoding large, complex
documents. In this paper, we introduce a new dataset for summarisation of
computer science publications by exploiting a large resource of author provided
summaries and show straightforward ways of extending it further. We develop
models on the dataset making use of both neural sentence encoding and
traditionally used summarisation features and show that models which encode
sentences as well as their local and global context perform best, significantly
outperforming well-established baseline methods.",A Supervised Approach to Extractive Summarisation of Scientific Papers,2017
279,0.21841772,0.001111293,0.001111281,0.001111319,0.001111401,0.001111357,0.001111337,0.001111323,0.357947079,0.415855889,Topic10,"Two recent approaches have achieved state-of-the-art results in image
captioning. The first uses a pipelined process where a set of candidate words
is generated by a convolutional neural network (CNN) trained on images, and
then a maximum entropy (ME) language model is used to arrange these words into
a coherent sentence. The second uses the penultimate activation layer of the
CNN as input to a recurrent neural network (RNN) that then generates the
caption sequence. In this paper, we compare the merits of these different
language modeling approaches for the first time by using the same
state-of-the-art CNN as input. We examine issues in the different approaches,
including linguistic irregularities, caption repetition, and data set overlap.
By combining key aspects of the ME and RNN methods, we achieve a new record
performance over previously published results on the benchmark COCO dataset.
However, the gains we see in BLEU do not translate to human judgments.",Language Models for Image Captioning: The Quirks and What Works,2015
280,0.421135503,0.001449536,0.001449523,0.001449726,0.001449601,0.001449529,0.086516222,0.001449505,0.001449697,0.482201157,Topic10,"This work aims to address the problem of image-based question-answering (QA)
with new models and datasets. In our work, we propose to use neural networks
and visual semantic embeddings, without intermediate stages such as object
detection and image segmentation, to predict answers to simple questions about
images. Our model performs 1.8 times better than the only published results on
an existing image QA dataset. We also present a question generation algorithm
that converts image descriptions, which are widely available, into QA form. We
used this algorithm to produce an order-of-magnitude larger dataset, with more
evenly distributed answers. A suite of baseline results on this new dataset are
also presented.",Exploring Models and Data for Image Question Answering,2015
281,0.423632443,0.050282719,0.000606213,0.000606164,0.090640749,0.000606221,0.061598135,0.000606176,0.000606205,0.370814975,Topic1,"Problems at the intersection of vision and language are of significant
importance both as challenging research questions and for the rich set of
applications they enable. However, inherent structure in our world and bias in
our language tend to be a simpler signal for learning than visual modalities,
resulting in models that ignore visual information, leading to an inflated
sense of their capability.
  We propose to counter these language priors for the task of Visual Question
Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance
the popular VQA dataset by collecting complementary images such that every
question in our balanced dataset is associated with not just a single image,
but rather a pair of similar images that result in two different answers to the
question. Our dataset is by construction more balanced than the original VQA
dataset and has approximately twice the number of image-question pairs. Our
complete balanced dataset is publicly available at www.visualqa.org as part of
the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA
v2.0).
  We further benchmark a number of state-of-art VQA models on our balanced
dataset. All models perform significantly worse on our balanced dataset,
suggesting that these models have indeed learned to exploit language priors.
This finding provides the first concrete empirical evidence for what seems to
be a qualitative sense among practitioners.
  Finally, our data collection protocol for identifying complementary images
enables us to develop a novel interpretable model, which in addition to
providing an answer to the given (image, question) pair, also provides a
counter-example based explanation. Specifically, it identifies an image that is
similar to the original image, but it believes has a different answer to the
same question. This can help in building trust for machines among their users.","Making the V in VQA Matter: Elevating the Role of Image Understanding in
  Visual Question Answering",2016
282,0.322970987,0.259097857,0.001666896,0.001667048,0.14156747,0.00166723,0.001667026,0.001667004,0.001667036,0.266361446,Topic1,"We propose a method for automatically answering questions about images by
bringing together recent advances from natural language processing and computer
vision. We combine discrete reasoning with uncertain predictions by a
multi-world approach that represents uncertainty about the perceived world in a
bayesian framework. Our approach can handle human questions of high complexity
about realistic scenes and replies with range of answer like counts, object
classes, instances and lists of them. The system is directly trained from
question-answer pairs. We establish a first benchmark for this task that can be
seen as a modern attempt at a visual turing test.","A Multi-World Approach to Question Answering about Real-World Scenes
  based on Uncertain Input",2014
283,0.00181892,0.36119772,0.001818425,0.001818686,0.62425327,0.001818453,0.001818474,0.00181855,0.001818609,0.001818893,Topic5,"Progress in language and image understanding by machines has sparkled the
interest of the research community in more open-ended, holistic tasks, and
refueled an old AI dream of building intelligent machines. We discuss a few
prominent challenges that characterize such holistic tasks and argue for
""question answering about images"" as a particular appealing instance of such a
holistic task. In particular, we point out that it is a version of a Turing
Test that is likely to be more robust to over-interpretations and contrast it
with tasks like grounding and generation of descriptions. Finally, we discuss
tools to measure progress in this field.",Hard to Cheat: A Turing Test based on Answering Questions about Images,2015
284,0.185312895,0.031558084,0.001266057,0.001266026,0.281999747,0.111244903,0.001266197,0.001266074,0.001266177,0.383553837,Topic10,"Recently, a number of deep-learning based models have been proposed for the
task of Visual Question Answering (VQA). The performance of most models is
clustered around 60-70%. In this paper we propose systematic methods to analyze
the behavior of these models as a first step towards recognizing their
strengths and weaknesses, and identifying the most fruitful directions for
progress. We analyze two models, one each from two major classes of VQA models
-- with-attention and without-attention and show the similarities and
differences in the behavior of these models. We also analyze the winning entry
of the VQA Challenge 2016.
  Our behavior analysis reveals that despite recent progress, today's VQA
models are ""myopic"" (tend to fail on sufficiently novel instances), often ""jump
to conclusions"" (converge on a predicted answer after 'listening' to just half
the question), and are ""stubborn"" (do not change their answers across images).",Analyzing the Behavior of Visual Question Answering Models,2016
285,0.232526935,0.001471143,0.001470795,0.001470921,0.124453819,0.001470858,0.001471017,0.001470906,0.095222747,0.538970859,Topic10,"Temporal common sense has applications in AI tasks such as QA, multi-document
summarization, and human-AI communication. We propose the task of sequencing --
given a jumbled set of aligned image-caption pairs that belong to a story, the
task is to sort them such that the output sequence forms a coherent story. We
present multiple approaches, via unary (position) and pairwise (order)
predictions, and their ensemble-based combinations, achieving strong results on
this task. We use both text-based and image-based features, which depict
complementary improvements. Using qualitative examples, we demonstrate that our
models have learnt interesting aspects of temporal common sense.",Sort Story: Sorting Jumbled Images and Captions into Stories,2016
286,0.619577294,0.001587795,0.001587591,0.001587736,0.00158754,0.001587743,0.001587753,0.001587474,0.333141322,0.036167753,Topic1,"We present Mean Box Pooling, a novel visual representation that pools over
CNN representations of a large number, highly overlapping object proposals. We
show that such representation together with nCCA, a successful multimodal
embedding technique, achieves state-of-the-art performance on the Visual
Madlibs task. Moreover, inspired by the nCCA's objective function, we extend
classical CNN+LSTM approach to train the network by directly maximizing the
similarity between the internal representation of the deep learning
architecture and candidate answers. Again, such approach achieves a significant
improvement over the prior work that also uses CNN+LSTM approach on Visual
Madlibs.","Mean Box Pooling: A Rich Image Representation and Output Embedding for
  the Visual Madlibs Task",2016
287,0.272660533,0.000909411,0.000909266,0.00090924,0.000909303,0.049499758,0.000909297,0.000909242,0.399759037,0.272624913,Topic9,"Recurrent neural networks have recently been used for learning to describe
images using natural language. However, it has been observed that these models
generalize poorly to scenes that were not observed during training, possibly
depending too strongly on the statistics of the text in the training data. Here
we propose to describe images using short structured representations, aiming to
capture the crux of a description. These structured representations allow us to
tease-out and evaluate separately two types of generalization: standard
generalization to new images with similar scenes, and generalization to new
combinations of known entities. We compare two learning approaches on the
MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,
Attend and Tell), and a simple structured prediction model on top of a deep
network. We find that the structured model generalizes to new compositions
substantially better than the LSTM, ~7 times the accuracy of predicting
structured representations. By providing a concrete method to quantify
generalization for unseen combinations, we argue that structured
representations and compositional splits are a useful benchmark for image
captioning, and advocate compositional models that capture linguistic and
visual structure.",Learning to generalize to new compositions in image understanding,2016
288,0.129043201,0.00147091,0.00147079,0.001470807,0.489396154,0.001470883,0.106714345,0.001470948,0.001470925,0.266021037,Topic5,"As machines have become more intelligent, there has been a renewed interest
in methods for measuring their intelligence. A common approach is to propose
tasks for which a human excels, but one which machines find difficult. However,
an ideal task should also be easy to evaluate and not be easily gameable. We
begin with a case study exploring the recently popular task of image captioning
and its limitations as a task for measuring machine intelligence. An
alternative and more promising task is Visual Question Answering that tests a
machine's ability to reason about language and vision. We describe a dataset
unprecedented in size created for the task that contains over 760,000 human
generated questions about images. Using around 10 million human generated
answers, machines may be easily evaluated.",Measuring Machine Intelligence Through Visual Question Answering,2016
289,0.468012861,0.001351688,0.001351532,0.001351658,0.169565189,0.001351594,0.001351626,0.00135157,0.001352037,0.352960244,Topic1,"Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.","Towards Transparent AI Systems: Interpreting Visual Question Answering
  Models",2016
290,0.323420885,0.000649544,0.00064949,0.000649433,0.215327316,0.000649534,0.000649466,0.000649474,0.000649491,0.456705366,Topic10,"We introduce the task of Visual Dialog, which requires an AI agent to hold a
meaningful dialog with humans in natural, conversational language about visual
content. Specifically, given an image, a dialog history, and a question about
the image, the agent has to ground the question in image, infer context from
history, and answer the question accurately. Visual Dialog is disentangled
enough from a specific downstream task so as to serve as a general test of
machine intelligence, while being grounded in vision enough to allow objective
evaluation of individual responses and benchmark progress. We develop a novel
two-person chat data-collection protocol to curate a large-scale Visual Dialog
dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10
question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog
question-answer pairs.
  We introduce a family of neural encoder-decoder models for Visual Dialog with
3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --
and 2 decoders (generative and discriminative), which outperform a number of
sophisticated baselines. We propose a retrieval-based evaluation protocol for
Visual Dialog where the AI agent is asked to sort a set of candidate answers
and evaluated on metrics such as mean-reciprocal-rank of human response. We
quantify gap between machine and human performance on the Visual Dialog task
via human studies. Putting it all together, we demonstrate the first 'visual
chatbot'! Our dataset, code, trained models and visual chatbot are available on
https://visualdialog.org",Visual Dialog,2016
291,0.00116319,0.00116292,0.001163012,0.207368401,0.001162997,0.001162975,0.001162952,0.001163037,0.403281907,0.381208609,Topic9,"Multi-task learning (MTL) involves the simultaneous training of two or more
related tasks over shared representations. In this work, we apply MTL to
audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn
a mapping between audio-visual fused features and frame labels obtained from
acoustic GMM/HMM model. This is combined with an auxiliary task which maps
visual features to frame labels obtained from a separate visual GMM/HMM model.
The MTL model is tested at various levels of babble noise and the results are
compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate
that MTL is especially useful at higher level of noise. Compared to base-line,
upto 7\% relative improvement in WER is reported at -3 SNR dB","Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic
  Speech Recognition",2017
292,0.209742572,0.000781498,0.000781322,0.000781398,0.21725584,0.287003825,0.00078138,0.000781426,0.00078151,0.281309228,Topic6,"We introduce the first goal-driven training for visual question answering and
dialog agents. Specifically, we pose a cooperative 'image guessing' game
between two agents -- Qbot and Abot -- who communicate in natural language
dialog so that Qbot can select an unseen image from a lineup of images. We use
deep reinforcement learning (RL) to learn the policies of these agents
end-to-end -- from pixels to multi-agent multi-round dialog to game reward.
  We demonstrate two experimental results.
  First, as a 'sanity check' demonstration of pure RL (from scratch), we show
results on a synthetic world, where the agents communicate in ungrounded
vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find
that two bots invent their own communication protocol and start using certain
symbols to ask/answer about certain visual attributes (shape/color/style).
Thus, we demonstrate the emergence of grounded language and communication among
'visual' dialog agents with no human supervision.
  Second, we conduct large-scale real-image experiments on the VisDial dataset,
where we pretrain with supervised dialog data and show that the RL 'fine-tuned'
agents significantly outperform SL agents. Interestingly, the RL Qbot learns to
ask questions that Abot is good at, ultimately resulting in more informative
dialog and a better team.","Learning Cooperative Visual Dialog Agents with Deep Reinforcement
  Learning",2017
293,0.193396617,0.000769453,0.071931487,0.000769367,0.378667004,0.000769476,0.000769484,0.000769392,0.000769458,0.351388263,Topic5,"Visual question answering (QA) has attracted a lot of attention lately, seen
essentially as a form of (visual) Turing test that artificial intelligence
should strive to achieve. In this paper, we study a crucial component of this
task: how can we design good datasets for the task? We focus on the design of
multiple-choice based datasets where the learner has to select the right answer
from a set of candidate ones including the target (i.e. the correct one) and
the decoys (i.e. the incorrect ones). Through careful analysis of the results
attained by state-of-the-art learning models and human annotators on existing
datasets, we show the design of the decoy answers has a significant impact on
how and what the learning models learn from the datasets. In particular, the
resulting learner can ignore the visual information, the question, or the both
while still doing well on the task. Inspired by this, we propose automatic
procedures to remedy such design deficiencies. We apply the procedures to
re-construct decoy answers for two popular visual QA datasets as well as to
create a new visual QA dataset from the Visual Genome project, resulting in the
largest dataset for this task. Extensive empirical studies show that the design
deficiencies have been alleviated in the remedied datasets and the performance
on them is likely a more faithful indicator of the difference among learning
models. The datasets are released and publicly available via
http://www.teds.usc.edu/website_vqa/.","Being Negative but Constructively: Lessons Learnt from Creating Better
  Visual Question Answering Datasets",2017
294,0.39315328,0.001099267,0.001099061,0.001099049,0.026781977,0.001099261,0.001099178,0.001099105,0.001099184,0.572370638,Topic10,"Visual Question Answering (VQA) has received a lot of attention over the past
couple of years. A number of deep learning models have been proposed for this
task. However, it has been shown that these models are heavily driven by
superficial correlations in the training data and lack compositionality -- the
ability to answer questions about unseen compositions of seen concepts. This
compositionality is desirable and central to intelligence. In this paper, we
propose a new setting for Visual Question Answering where the test
question-answer pairs are compositionally novel compared to training
question-answer pairs. To facilitate developing models under this setting, we
present a new compositional split of the VQA v1.0 dataset, which we call
Compositional VQA (C-VQA). We analyze the distribution of questions and answers
in the C-VQA splits. Finally, we evaluate several existing VQA models under
this new setting and show that the performances of these models degrade by a
significant amount compared to the original VQA setting.","C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0
  Dataset",2017
295,0.00250072,0.002500805,0.002500358,0.002500343,0.54609281,0.002500445,0.002500606,0.002500543,0.123594807,0.312808562,Topic5,"We discuss problems with the standard approaches to evaluation for tasks like
visual question answering, and argue that artificial data can be used to
address these as a complement to current practice. We demonstrate that with the
help of existing 'deep' linguistic processing technology we are able to create
challenging abstract datasets, which enable us to investigate the language
understanding abilities of multimodal deep learning models in detail.",Deep learning evaluation using deep linguistic processing,2017
296,0.001333773,0.001333609,0.171357791,0.001333688,0.00133355,0.00133368,0.512534121,0.00133351,0.306772674,0.001333604,Topic7,"We propose a simple yet effective technique for neural network learning. The
forward propagation is computed as usual. In back propagation, only a small
subset of the full gradient is computed to update the model parameters. The
gradient vectors are sparsified in such a way that only the top-$k$ elements
(in terms of magnitude) are kept. As a result, only $k$ rows or columns
(depending on the layout) of the weight matrix are modified, leading to a
linear reduction ($k$ divided by the vector dimension) in the computational
cost. Surprisingly, experimental results demonstrate that we can update only
1--4\% of the weights at each back propagation pass. This does not result in a
larger number of training iterations. More interestingly, the accuracy of the
resulting models is actually improved rather than degraded, and a detailed
analysis is given. The code is available at https://github.com/jklj077/meProp","meProp: Sparsified Back Propagation for Accelerated Deep Learning with
  Reduced Overfitting",2017
297,0.695236886,0.000980576,0.000980584,0.000980658,0.000980608,0.000980644,0.000980744,0.000980573,0.000980653,0.296918074,Topic1,"Adversarial samples are strategically modified samples, which are crafted
with the purpose of fooling a classifier at hand. An attacker introduces
specially crafted adversarial samples to a deployed classifier, which are being
mis-classified by the classifier. However, the samples are perceived to be
drawn from entirely different classes and thus it becomes hard to detect the
adversarial samples. Most of the prior works have been focused on synthesizing
adversarial samples in the image domain. In this paper, we propose a new method
of crafting adversarial text samples by modification of the original samples.
Modifications of the original text samples are done by deleting or replacing
the important or salient words in the text or by introducing new words in the
text sample. Our algorithm works best for the datasets which have
sub-categories within each of the classes of examples. While crafting
adversarial samples, one of the key constraint is to generate meaningful
sentences which can at pass off as legitimate from language (English)
viewpoint. Experimental results on IMDB movie review dataset for sentiment
analysis and Twitter dataset for gender detection show the efficiency of our
proposed method.",Towards Crafting Text Adversarial Samples,2017
298,0.560080427,0.045028887,0.001234733,0.001234772,0.001234773,0.1714504,0.001234835,0.001234743,0.001234833,0.216031597,Topic1,"Sequence-to-sequence models have shown promising improvements on the temporal
task of video captioning, but they optimize word-level cross-entropy loss
during training. First, using policy gradient and mixed-loss methods for
reinforcement learning, we directly optimize sentence-level task-based metrics
(as rewards), achieving significant improvements over the baseline, based on
both automatic metrics and human evaluation on multiple datasets. Next, we
propose a novel entailment-enhanced reward (CIDEnt) that corrects
phrase-matching based metrics (such as CIDEr) to only allow for
logically-implied partial matches and avoid contradictions, achieving further
significant improvements over the CIDEr-reward model. Overall, our
CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.",Reinforced Video Captioning with Entailment Rewards,2017
299,0.56347097,0.001852076,0.001852037,0.001852023,0.001852141,0.001852116,0.001852313,0.001852031,0.001852514,0.421711778,Topic1,"We address the problem of end-to-end visual storytelling. Given a photo
album, our model first selects the most representative (summary) photos, and
then composes a natural language story for the album. For this task, we make
use of the Visual Storytelling dataset and a model composed of three
hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album
photos, select representative (summary) photos, and compose the story.
Automatic and human evaluations show our model achieves better performance on
selection, generation, and retrieval than baselines.",Hierarchically-Attentive RNN for Album Summarization and Storytelling,2017
300,0.679250721,0.001031177,0.001031226,0.001031116,0.149448901,0.001031223,0.001031204,0.001031123,0.00103125,0.164082058,Topic1,"Due to their complex nature, it is hard to characterize the ways in which
machine learning models can misbehave or be exploited when deployed. Recent
work on adversarial examples, i.e. inputs with minor perturbations that result
in substantially different model predictions, is helpful in evaluating the
robustness of these models by exposing the adversarial scenarios where they
fail. However, these malicious perturbations are often unnatural, not
semantically meaningful, and not applicable to complicated domains such as
language. In this paper, we propose a framework to generate natural and legible
adversarial examples that lie on the data manifold, by searching in semantic
space of dense and continuous data representation, utilizing the recent
advances in generative adversarial networks. We present generated adversaries
to demonstrate the potential of the proposed approach for black-box classifiers
for a wide range of applications such as image classification, textual
entailment, and machine translation. We include experiments to show that the
generated adversaries are natural, legible to humans, and useful in evaluating
and analyzing black-box classifiers.",Generating Natural Adversarial Examples,2017
301,0.086516764,0.001064083,0.244907932,0.001064089,0.001064015,0.001064117,0.460912132,0.001063996,0.201278772,0.0010641,Topic7,"We propose a simple yet effective technique to simplify the training and the
resulting model of neural networks. In back propagation, only a small subset of
the full gradient is computed to update the model parameters. The gradient
vectors are sparsified in such a way that only the top-$k$ elements (in terms
of magnitude) are kept. As a result, only $k$ rows or columns (depending on the
layout) of the weight matrix are modified, leading to a linear reduction in the
computational cost. Based on the sparsified gradients, we further simplify the
model by eliminating the rows or columns that are seldom updated, which will
reduce the computational cost both in the training and decoding, and
potentially accelerate decoding in real-world applications. Surprisingly,
experimental results demonstrate that most of time we only need to update fewer
than 5% of the weights at each back propagation pass. More interestingly, the
accuracy of the resulting models is actually improved rather than degraded, and
a detailed analysis is given. The model simplification results show that we
could adaptively simplify the model which could often be reduced by around 9x,
without any loss on accuracy or even with improved accuracy.","Training Simplification and Model Simplification for Deep Learning: A
  Minimal Effort Back Propagation Method",2017
302,0.001563027,0.140238128,0.001562619,0.001562799,0.696944241,0.001562906,0.001562731,0.001562844,0.001562807,0.151877898,Topic5,"We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question (""What color is the car?""). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question (""orange"").
  This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.",Embodied Question Answering,2017
303,0.75866301,0.088819306,0.000591827,0.000591801,0.000591881,0.00059189,0.00059182,0.000591826,0.000591874,0.148374763,Topic1,"A number of studies have found that today's Visual Question Answering (VQA)
models are heavily driven by superficial correlations in the training data and
lack sufficient image grounding. To encourage development of models geared
towards the latter, we propose a new setting for VQA where for every question
type, train and test sets have different prior distributions of answers.
Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we
call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2
respectively). First, we evaluate several existing VQA models under this new
setting and show that their performance degrades significantly compared to the
original VQA setting. Second, we propose a novel Grounded Visual Question
Answering model (GVQA) that contains inductive biases and restrictions in the
architecture specifically designed to prevent the model from 'cheating' by
primarily relying on priors in the training data. Specifically, GVQA explicitly
disentangles the recognition of visual concepts present in the image from the
identification of plausible answer space for a given question, enabling the
model to more robustly generalize across different distributions of answers.
GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).
Our experiments demonstrate that GVQA significantly outperforms SAN on both
VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more
powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in
several cases. GVQA offers strengths complementary to SAN when trained and
evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more
transparent and interpretable than existing VQA models.","Don't Just Assume; Look and Answer: Overcoming Priors for Visual
  Question Answering",2017
304,0.348752883,0.000934846,0.000934688,0.000935134,0.289246479,0.173335613,0.000934702,0.000934791,0.000934727,0.183056136,Topic1,"In this work, we propose a goal-driven collaborative task that contains
vision, language, and action in a virtual environment as its core components.
Specifically, we develop a collaborative `Image Drawing' game between two
agents, called CoDraw. Our game is grounded in a virtual world that contains
movable clip art objects. Two players, Teller and Drawer, are involved. The
Teller sees an abstract scene containing multiple clip arts in a semantically
meaningful configuration, while the Drawer tries to reconstruct the scene on an
empty canvas using available clip arts. The two players communicate via two-way
communication using natural language. We collect the CoDraw dataset of ~10K
dialogs consisting of 138K messages exchanged between a Teller and a Drawer
from Amazon Mechanical Turk (AMT). We analyze our dataset and present three
models to model the players' behaviors, including an attention model to
describe and draw multiple clip arts at each round. The attention models are
quantitatively compared to the other models to show how the conventional
approaches work for this new task. We also present qualitative visualizations.",CoDraw: Visual Dialog for Collaborative Drawing,2017
305,0.000952577,0.000952755,0.000952554,0.000952577,0.595137721,0.081799912,0.00095268,0.000952609,0.088636112,0.228710502,Topic5,"Goal-oriented dialogue has been paid attention for its numerous applications
in artificial intelligence. To solve this task, deep learning and reinforcement
learning have recently been applied. However, these approaches struggle to find
a competent recurrent neural questioner, owing to the complexity of learning a
series of sentences. Motivated by theory of mind, we propose ""Answerer in
Questioner's Mind"" (AQM), a novel algorithm for goal-oriented dialogue. With
AQM, a questioner asks and infers based on an approximated probabilistic model
of the answerer. The questioner figures out the answerer's intent via selecting
a plausible question by explicitly calculating the information gain of the
candidate intentions and possible answers to each question. We test our
framework on two goal-oriented visual dialogue tasks: ""MNIST Counting Dialog""
and ""GuessWhat?!."" In our experiments, AQM outperforms comparative algorithms
and makes human-like dialogue. We further use AQM as a tool for analyzing the
mechanism of deep reinforcement learning approach and discuss the future
direction of practical goal-oriented neural dialogue systems.",Answerer in Questioner's Mind for Goal-Oriented Visual Dialogue,2018
306,0.001190954,0.001190724,0.001190734,0.001191073,0.001190784,0.001190772,0.509008688,0.0011908,0.304854545,0.177800928,Topic7,"We study the problem of structured prediction under test-time budget
constraints. We propose a novel approach applicable to a wide range of
structured prediction problems in computer vision and natural language
processing. Our approach seeks to adaptively generate computationally costly
features during test-time in order to reduce the computational cost of
prediction while maintaining prediction performance. We show that training the
adaptive feature generation system can be reduced to a series of structured
learning problems, resulting in efficient training using existing structured
learning algorithms. This framework provides theoretical justification for
several existing heuristic approaches found in literature. We evaluate our
proposed adaptive system on two structured prediction tasks, optical character
recognition (OCR) and dependency parsing and show strong performance in
reduction of the feature costs without degrading accuracy.",Resource Constrained Structured Prediction,2016
307,0.001031271,0.001031336,0.001031111,0.001031167,0.104173247,0.001031391,0.001031098,0.001031195,0.319038995,0.569569189,Topic10,"We propose a neural sequence-to-sequence model for direction following, a
task that is essential to realizing effective autonomous agents. Our
alignment-based encoder-decoder model with long short-term memory recurrent
neural networks (LSTM-RNN) translates natural language instructions to action
sequences based upon a representation of the observable world state. We
introduce a multi-level aligner that empowers our model to focus on sentence
""regions"" salient to the current world state by using multiple abstractions of
the input sentence. In contrast to existing methods, our model uses no
specialized linguistic resources (e.g., parsers) or task-specific annotations
(e.g., seed lexicons). It is therefore generalizable, yet still achieves the
best results reported to-date on a benchmark single-sentence dataset and
competitive results for the limited-training multi-sentence setting. We analyze
our model through a series of ablations that elucidate the contributions of the
primary components of our model.","Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to
  Action Sequences",2015
308,0.001041937,0.287690107,0.001041878,0.00104183,0.001041971,0.001041834,0.224174005,0.001041988,0.386818312,0.095066137,Topic9,"Building neural networks to query a knowledge base (a table) with natural
language is an emerging research topic in deep learning. An executor for table
querying typically requires multiple steps of execution because queries may
have complicated structures. In previous studies, researchers have developed
either fully distributed executors or symbolic executors for table querying. A
distributed executor can be trained in an end-to-end fashion, but is weak in
terms of execution efficiency and explicit interpretability. A symbolic
executor is efficient in execution, but is very difficult to train especially
at initial stages. In this paper, we propose to couple distributed and symbolic
execution for natural language queries, where the symbolic executor is
pretrained with the distributed executor's intermediate execution results in a
step-by-step fashion. Experiments show that our approach significantly
outperforms both distributed and symbolic executors, exhibiting high accuracy,
high learning efficiency, high execution efficiency, and high interpretability.",Coupling Distributed and Symbolic Execution for Natural Language Queries,2016
309,0.000943618,0.05202153,0.00094359,0.000943596,0.000943687,0.056778786,0.000943673,0.271510369,0.079588276,0.535382875,Topic10,"Due to the huge availability of documents in digital form, and the deception
possibility raise bound to the essence of digital documents and the way they
are spread, the authorship attribution problem has constantly increased its
relevance. Nowadays, authorship attribution,for both information retrieval and
analysis, has gained great importance in the context of security, trust and
copyright preservation. This work proposes an innovative multi-agent driven
machine learning technique that has been developed for authorship attribution.
By means of a preprocessing for word-grouping and time-period related analysis
of the common lexicon, we determine a bias reference level for the recurrence
frequency of the words within analysed texts, and then train a Radial Basis
Neural Networks (RBPNN)-based classifier to identify the correct author. The
main advantage of the proposed approach lies in the generality of the semantic
analysis, which can be applied to different contexts and lexical domains,
without requiring any modification. Moreover, the proposed system is able to
incorporate an external input, meant to tune the classifier, and then
self-adjust by means of continuous learning reinforcement.","An agent-driven semantical identifier using radial basis neural networks
  and reinforcement learning",2014
310,0.052825188,0.000813248,0.000813276,0.000813322,0.125511702,0.000813301,0.000813346,0.404468417,0.000813216,0.412314985,Topic10,"Humans and animals are constantly exposed to a continuous stream of sensory
information from different modalities. At the same time, they form more
compressed representations like concepts or symbols. In species that use
language, this process is further structured by this interaction, where a
mapping between the sensorimotor concepts and linguistic elements needs to be
established. There is evidence that children might be learning language by
simply disambiguating potential meanings based on multiple exposures to
utterances in different contexts (cross-situational learning). In existing
models, the mapping between modalities is usually found in a single step by
directly using frequencies of referent and meaning co-occurrences. In this
paper, we present an extension of this one-step mapping and introduce a newly
proposed sequential mapping algorithm together with a publicly available Matlab
implementation. For demonstration, we have chosen a less typical scenario:
instead of learning to associate objects with their names, we focus on body
representations. A humanoid robot is receiving tactile stimulations on its
body, while at the same time listening to utterances of the body part names
(e.g., hand, forearm and torso). With the goal at arriving at the correct ""body
categories"", we demonstrate how a sequential mapping algorithm outperforms
one-step mapping. In addition, the effect of data set size and noise in the
linguistic input are studied.","Where is my forearm? Clustering of body parts from simultaneous tactile
  and linguistic input using sequential mapping",2017
311,0.00086975,0.000869665,0.000869805,0.000869738,0.000869691,0.000869824,0.019390333,0.000869786,0.736636426,0.237884981,Topic9,"Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural
Networks (DNN), as they are able to better reduce spectral variation in the
input signal. This has also been confirmed experimentally, with CNNs showing
improvements in word error rate (WER) between 4-12% relative compared to DNNs
across a variety of LVCSR tasks. In this paper, we describe different methods
to further improve CNN performance. First, we conduct a deep analysis comparing
limited weight sharing and full weight sharing with state-of-the-art features.
Second, we apply various pooling strategies that have shown improvements in
computer vision to an LVCSR speech task. Third, we introduce a method to
effectively incorporate speaker adaptation, namely fMLLR, into log-mel
features. Fourth, we introduce an effective strategy to use dropout during
Hessian-free sequence training. We find that with these improvements,
particularly with fMLLR and dropout, we are able to achieve an additional 2-3%
relative improvement in WER on a 50-hour Broadcast News task over our previous
best CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%
relative improvement over our previous best CNN baseline.",Improvements to deep convolutional neural networks for LVCSR,2013
312,0.000869832,0.000869736,0.10369756,0.000869772,0.000869784,0.037854495,0.000869771,0.00086973,0.29109446,0.56213486,Topic10,"Collaborative filtering (CF) is a successful approach commonly used by many
recommender systems. Conventional CF-based methods use the ratings given to
items by users as the sole source of information for learning to make
recommendation. However, the ratings are often very sparse in many
applications, causing CF-based methods to degrade significantly in their
recommendation performance. To address this sparsity problem, auxiliary
information such as item content information may be utilized. Collaborative
topic regression (CTR) is an appealing recent method taking this approach which
tightly couples the two components that learn from two different sources of
information. Nevertheless, the latent representation learned by CTR may not be
very effective when the auxiliary information is very sparse. To address this
problem, we generalize recent advances in deep learning from i.i.d. input to
non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian
model called collaborative deep learning (CDL), which jointly performs deep
representation learning for the content information and collaborative filtering
for the ratings (feedback) matrix. Extensive experiments on three real-world
datasets from different domains show that CDL can significantly advance the
state of the art.",Collaborative Deep Learning for Recommender Systems,2014
313,0.001333743,0.001333829,0.143172194,0.001333681,0.001333743,0.001334092,0.001333688,0.001333596,0.556923544,0.29056789,Topic9,"Layer-wise relevance propagation (LRP) is a recently proposed technique for
explaining predictions of complex non-linear classifiers in terms of input
variables. In this paper, we apply LRP for the first time to natural language
processing (NLP). More precisely, we use it to explain the predictions of a
convolutional neural network (CNN) trained on a topic categorization task. Our
analysis highlights which words are relevant for a specific prediction of the
CNN. We compare our technique to standard sensitivity analysis, both
qualitatively and quantitatively, using a ""word deleting"" perturbation
experiment, a PCA analysis, and various visualizations. All experiments
validate the suitability of LRP for explaining the CNN predictions, which is
also in line with results reported in recent image classification studies.",Explaining Predictions of Non-Linear Classifiers in NLP,2016
314,0.001515544,0.259477002,0.246023294,0.00151577,0.083442534,0.286296598,0.001515558,0.001515439,0.057325435,0.061372826,Topic6,"We propose a new statistical model suitable for machine learning of systems
with long distance correlations such as natural languages. The model is based
on directed acyclic graph decorated by multi-linear tensor maps in the vertices
and vector spaces in the edges, called tensor network. Such tensor networks
have been previously employed for effective numerical computation of the
renormalization group flow on the space of effective quantum field theories and
lattice models of statistical mechanics. We provide explicit algebro-geometric
analysis of the parameter moduli space for tree graphs, discuss model
properties and applications such as statistical translation.",Tensor network language model,2017
315,0.00344894,0.297931058,0.143481232,0.003449686,0.00344877,0.143074629,0.003448792,0.003448879,0.003449391,0.394818622,Topic10,"We propose a statistical model for natural language that begins by
considering language as a monoid, then representing it in complex matrices with
a compatible translation invariant probability measure. We interpret the
probability measure as arising via the Born rule from a translation invariant
matrix product state.",Language as a matrix product state,2017
316,0.10476812,0.000934722,0.090665782,0.000934769,0.000934748,0.000934789,0.640463177,0.000934739,0.056212281,0.103216874,Topic7,"Hessian-free training has become a popular parallel second or- der
optimization technique for Deep Neural Network training. This study aims at
speeding up Hessian-free training, both by means of decreasing the amount of
data used for training, as well as through reduction of the number of Krylov
subspace solver iterations used for implicit estimation of the Hessian. In this
paper, we develop an L-BFGS based preconditioning scheme that avoids the need
to access the Hessian explicitly. Since L-BFGS cannot be regarded as a
fixed-point iteration, we further propose the employment of flexible Krylov
subspace solvers that retain the desired theoretical convergence guarantees of
their conventional counterparts. Second, we propose a new sampling algorithm,
which geometrically increases the amount of data utilized for gradient and
Krylov subspace iteration calculations. On a 50-hr English Broadcast News task,
we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a
300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no
loss in WER. These results suggest that even further speed-up is expected, as
problems scale and complexity grows.","Accelerating Hessian-free optimization for deep neural networks by
  implicit preconditioning and sampling",2013
317,0.336697152,0.000847641,0.000847589,0.000847613,0.255844484,0.000847586,0.000847628,0.000847632,0.000847617,0.401525059,Topic10,"While textual reviews have become prominent in many recommendation-based
systems, automated frameworks to provide relevant visual cues against text
reviews where pictures are not available is a new form of task confronted by
data mining and machine learning researchers. Suggestions of pictures that are
relevant to the content of a review could significantly benefit the users by
increasing the effectiveness of a review. We propose a deep learning-based
framework to automatically: (1) tag the images available in a review dataset,
(2) generate a caption for each image that does not have one, and (3) enhance
each review by recommending relevant images that might not be uploaded by the
corresponding reviewer. We evaluate the proposed framework using the Yelp
Challenge Dataset. While a subset of the images in this particular dataset are
correctly captioned, the majority of the pictures do not have any associated
text. Moreover, there is no mapping between reviews and images. Each image has
a corresponding business-tag where the picture was taken, though. The overall
data setting and unavailability of crucial pieces required for a mapping make
the problem of recommending images for reviews a major challenge. Qualitative
and quantitative evaluations indicate that our proposed framework provides high
quality enhancements through automatic captioning, tagging, and recommendation
for mapping reviews and images.",Is a Picture Worth Ten Thousand Words in a Review Dataset?,2016
318,0.001149669,0.0011497,0.514643356,0.00114971,0.001149747,0.345325842,0.001149796,0.001149731,0.13198273,0.00114972,Topic3,"Linear principal component analysis (PCA) can be extended to a nonlinear PCA
by using artificial neural networks. But the benefit of curved components
requires a careful control of the model complexity. Moreover, standard
techniques for model selection, including cross-validation and more generally
the use of an independent test set, fail when applied to nonlinear PCA because
of its inherent unsupervised characteristics. This paper presents a new
approach for validating the complexity of nonlinear PCA models by using the
error in missing data estimation as a criterion for model selection. It is
motivated by the idea that only the model of optimal complexity is able to
predict missing values with the highest accuracy. While standard test set
validation usually favours over-fitted nonlinear PCA models, the proposed model
validation approach correctly selects the optimal model complexity.",Validation of nonlinear PCA,2012
319,0.00196115,0.001961662,0.233116435,0.001961097,0.001961126,0.001961418,0.751193779,0.001961032,0.001961113,0.001961189,Topic7,"We consider the problem of learning from a similarity matrix (such as
spectral clustering and lowd imensional embedding), when computing pairwise
similarities are costly, and only a limited number of entries can be observed.
We provide a theoretical analysis using standard notions of graph
approximation, significantly generalizing previous results (which focused on
spectral clustering with two clusters). We also propose a new algorithmic
approach based on adaptive sampling, which experimentally matches or improves
on previous methods, while being considerably more general and computationally
cheaper.",Graph Approximation and Clustering on a Budget,2014
320,0.001333634,0.00133358,0.001333567,0.001333599,0.001333604,0.001333704,0.687433891,0.001333517,0.301897199,0.001333704,Topic7,"Multiclass prediction is the problem of classifying an object into a relevant
target class. We consider the problem of learning a multiclass predictor that
uses only few features, and in particular, the number of used features should
increase sub-linearly with the number of possible classes. This implies that
features should be shared by several classes. We describe and analyze the
ShareBoost algorithm for learning a multiclass predictor that uses few shared
features. We prove that ShareBoost efficiently finds a predictor that uses few
shared features (if such a predictor exists) and that it has a small
generalization error. We also describe how to use ShareBoost for learning a
non-linear predictor that has a fast evaluation time. In a series of
experiments with natural data sets we demonstrate the benefits of ShareBoost
and evaluate its success relatively to other state-of-the-art approaches.",ShareBoost: Efficient Multiclass Learning with Feature Sharing,2011
321,0.050165997,0.000602532,0.508274139,0.044969485,0.282765088,0.000602527,0.110812529,0.000602615,0.000602572,0.000602517,Topic3,"Due to advances in sensors, growing large and complex medical image data have
the ability to visualize the pathological change in the cellular or even the
molecular level or anatomical changes in tissues and organs. As a consequence,
the medical images have the potential to enhance diagnosis of disease,
prediction of clinical outcomes, characterization of disease progression,
management of health care and development of treatments, but also pose great
methodological and computational challenges for representation and selection of
features in image cluster analysis. To address these challenges, we first
extend one dimensional functional principal component analysis to the two
dimensional functional principle component analyses (2DFPCA) to fully capture
space variation of image signals. Image signals contain a large number of
redundant and irrelevant features which provide no additional or no useful
information for cluster analysis. Widely used methods for removing redundant
and irrelevant features are sparse clustering algorithms using a lasso-type
penalty to select the features. However, the accuracy of clustering using a
lasso-type penalty depends on how to select penalty parameters and a threshold
for selecting features. In practice, they are difficult to determine. Recently,
randomized algorithms have received a great deal of attention in big data
analysis. This paper presents a randomized algorithm for accurate feature
selection in image cluster analysis. The proposed method is applied to ovarian
and kidney cancer histology image data from the TCGA database. The results
demonstrate that the randomized feature selection method coupled with
functional principal component analysis substantially outperforms the current
sparse clustering algorithms in image cluster analysis.","Functional Principal Component Analysis and Randomized Sparse Clustering
  Algorithm for Medical Image Analysis",2014
322,0.654861287,0.001190702,0.001190885,0.001190819,0.001190889,0.001190934,0.001190869,0.001190681,0.162657829,0.174145105,Topic1,"Similarity between objects is multi-faceted and it can be easier for human
annotators to measure it when the focus is on a specific aspect. We consider
the problem of mapping objects into view-specific embeddings where the distance
between them is consistent with the similarity comparisons of the form ""from
the t-th view, object A is more similar to B than to C"". Our framework jointly
learns view-specific embeddings exploiting correlations between views.
Experiments on a number of datasets, including one of multi-view crowdsourced
comparison on bird images, show the proposed method achieves lower triplet
generalization error when compared to both learning embeddings independently
for each view and all views pooled into one view. Our method can also be used
to learn multiple measures of similarity over input features taking class
labels into account and compares favorably to existing approaches for
multi-task metric learning on the ISOLET dataset.","Jointly Learning Multiple Measures of Similarities from Triplet
  Comparisons",2015
323,0.109024443,0.000833507,0.03569951,0.000833514,0.000833532,0.849441397,0.0008335,0.000833528,0.000833567,0.000833501,Topic6,"The Gaussian process latent variable model (GP-LVM) provides a flexible
approach for non-linear dimensionality reduction that has been widely applied.
However, the current approach for training GP-LVMs is based on maximum
likelihood, where the latent projection variables are maximized over rather
than integrated out. In this paper we present a Bayesian method for training
GP-LVMs by introducing a non-standard variational inference framework that
allows to approximately integrate out the latent variables and subsequently
train a GP-LVM by maximizing an analytic lower bound on the exact marginal
likelihood. We apply this method for learning a GP-LVM from iid observations
and for learning non-linear dynamical systems where the observations are
temporally correlated. We show that a benefit of the variational Bayesian
procedure is its robustness to overfitting and its ability to automatically
select the dimensionality of the nonlinear latent space. The resulting
framework is generic, flexible and easy to extend for other purposes, such as
Gaussian process regression with uncertain inputs and semi-supervised Gaussian
processes. We demonstrate our method on synthetic data and standard machine
learning benchmarks, as well as challenging real world datasets, including high
resolution video data.","Variational Inference for Uncertainty on the Inputs of Gaussian Process
  Models",2014
324,0.814687488,0.129731747,0.001887168,0.00188702,0.001887192,0.001887372,0.001887149,0.001887094,0.00188739,0.042370379,Topic1,"Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.",Conditional Generative Adversarial Nets,2014
325,0.045378156,0.001163151,0.001162959,0.001163071,0.464881059,0.415024488,0.001163046,0.067737864,0.001163197,0.001163008,Topic5,"We provide a rigorous definition of the visual cause of a behavior that is
broadly applicable to the visually driven behavior in humans, animals, neurons,
robots and other perceiving systems. Our framework generalizes standard
accounts of causal learning to settings in which the causal variables need to
be constructed from micro-variables. We prove the Causal Coarsening Theorem,
which allows us to gain causal knowledge from observational data with minimal
experimental effort. The theorem provides a connection to standard inference
techniques in machine learning that identify features of an image that
correlate with, but may not cause, the target behavior. Finally, we propose an
active learning scheme to learn a manipulator function that performs optimal
manipulations on the image to automatically identify the visual cause of a
target behavior. We illustrate our inference and learning algorithms in
experiments based on both synthetic and real data.",Visual Causal Feature Learning,2014
326,0.003333869,0.302744468,0.101175733,0.003334155,0.003334417,0.003334383,0.00333406,0.003334287,0.572740368,0.00333426,Topic9,"We present experiments demonstrating that some other form of capacity
control, different from network size, plays a central role in learning
multilayer feed-forward networks. We argue, partially through analogy to matrix
factorization, that this is an inductive bias that can help shed light on deep
learning.","In Search of the Real Inductive Bias: On the Role of Implicit
  Regularization in Deep Learning",2014
327,0.428144497,0.000952615,0.000952631,0.000952635,0.000952597,0.000952603,0.157329054,0.000952598,0.40785819,0.00095258,Topic1,"The problem of domain generalization is to take knowledge acquired from a
number of related domains where training data is available, and to then
successfully apply it to previously unseen domains. We propose a new feature
learning algorithm, Multi-Task Autoencoder (MTAE), that provides good
generalization performance for cross-domain object recognition.
  Our algorithm extends the standard denoising autoencoder framework by
substituting artificially induced corruption with naturally occurring
inter-domain variability in the appearance of objects. Instead of
reconstructing images from noisy versions, MTAE learns to transform the
original image into analogs in multiple related domains. It thereby learns
features that are robust to variations across domains. The learnt features are
then used as inputs to a classifier.
  We evaluated the performance of the algorithm on benchmark image recognition
datasets, where the task is to learn features from multiple datasets and to
then predict the image label from unseen datasets. We found that (denoising)
MTAE outperforms alternative autoencoder-based models as well as the current
state-of-the-art algorithms for domain generalization.","Domain Generalization for Object Recognition with Multi-task
  Autoencoders",2015
328,0.429259112,0.000862254,0.00086236,0.000862323,0.000862357,0.563842416,0.000862237,0.000862354,0.000862337,0.00086225,Topic6,"Data-efficient reinforcement learning (RL) in continuous state-action spaces
using very high-dimensional observations remains a key challenge in developing
fully autonomous systems. We consider a particularly important instance of this
challenge, the pixels-to-torques problem, where an RL agent learns a
closed-loop control policy (""torques"") from pixel information only. We
introduce a data-efficient, model-based reinforcement learning algorithm that
learns such a closed-loop policy directly from pixel information. The key
ingredient is a deep dynamical model for learning a low-dimensional feature
embedding of images jointly with a predictive model in this low-dimensional
feature space. Joint learning is crucial for long-term predictions, which lie
at the core of the adaptive nonlinear model predictive control strategy that we
use for closed-loop control. Compared to state-of-the-art RL methods for
continuous states and actions, our approach learns quickly, scales to
high-dimensional state spaces, is lightweight and an important step toward
fully autonomous end-to-end learning from pixels to torques.","Data-Efficient Learning of Feedback Policies from Image Pixels using
  Deep Dynamical Models",2015
329,0.000806661,0.000806692,0.128817404,0.000806619,0.000806612,0.143939041,0.150500414,0.000806587,0.571903341,0.000806629,Topic9,"This paper addresses classification tasks on a particular target domain in
which labeled training data are only available from source domains different
from (but related to) the target. Two closely related frameworks, domain
adaptation and domain generalization, are concerned with such tasks, where the
only difference between those frameworks is the availability of the unlabeled
target data: domain adaptation can leverage unlabeled target information, while
domain generalization cannot. We propose Scatter Component Analyis (SCA), a
fast representation learning algorithm that can be applied to both domain
adaptation and domain generalization. SCA is based on a simple geometrical
measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA
finds a representation that trades between maximizing the separability of
classes, minimizing the mismatch between domains, and maximizing the
separability of data; each of which is quantified through scatter. The
optimization problem of SCA can be reduced to a generalized eigenvalue problem,
which results in a fast and exact solution. Comprehensive experiments on
benchmark cross-domain object recognition datasets verify that SCA performs
much faster than several state-of-the-art algorithms and also provides
state-of-the-art classification accuracy in both domain adaptation and domain
generalization. We also show that scatter can be used to establish a
theoretical generalization bound in the case of domain adaptation.","Scatter Component Analysis: A Unified Framework for Domain Adaptation
  and Domain Generalization",2015
330,0.00138919,0.001389165,0.653032043,0.060215989,0.001389088,0.00138928,0.277027691,0.001389351,0.001389145,0.001389058,Topic3,"Matrix rank minimization problem is in general NP-hard. The nuclear norm is
used to substitute the rank function in many recent studies. Nevertheless, the
nuclear norm approximation adds all singular values together and the
approximation error may depend heavily on the magnitudes of singular values.
This might restrict its capability in dealing with many practical problems. In
this paper, an arctangent function is used as a tighter approximation to the
rank function. We use it on the challenging subspace clustering problem. For
this nonconvex minimization problem, we develop an effective optimization
procedure based on a type of augmented Lagrange multipliers (ALM) method.
Extensive experiments on face clustering and motion segmentation show that the
proposed method is effective for rank approximation.",Robust Subspace Clustering via Tighter Rank Approximation,2015
331,0.000769476,0.0007694,0.000769411,0.055778245,0.207565388,0.000769403,0.000769485,0.321054079,0.274279261,0.137475853,Topic8,"The human face constantly conveys information, both consciously and
subconsciously. However, as basic as it is for humans to visually interpret
this information, it is quite a big challenge for machines. Conventional
semantic facial feature recognition and analysis techniques are already in use
and are based on physiological heuristics, but they suffer from lack of
robustness and high computation time. This thesis aims to explore ways for
machines to learn to interpret semantic information available in faces in an
automated manner without requiring manual design of feature detectors, using
the approach of Deep Learning. This thesis provides a study of the effects of
various factors and hyper-parameters of deep neural networks in the process of
determining an optimal network configuration for the task of semantic facial
feature recognition. This thesis explores the effectiveness of the system to
recognize the various semantic features (like emotions, age, gender, ethnicity
etc.) present in faces. Furthermore, the relation between the effect of
high-level concepts on low level features is explored through an analysis of
the similarities in low-level descriptors of different semantic features. This
thesis also demonstrates a novel idea of using a deep network to generate 3-D
Active Appearance Models of faces from real-world 2-D images.
  For a more detailed report on this work, please see [arXiv:1512.00743v1].",Recognizing Semantic Features in Faces using Deep Learning,2015
332,0.4185191,0.01103581,0.000847667,0.000847634,0.000847635,0.000847635,0.174042612,0.000847606,0.391316646,0.000847655,Topic1,"In this paper, we propose a novel unsupervised domain adaptation algorithm
based on deep learning for visual object recognition. Specifically, we design a
new model called Deep Reconstruction-Classification Network (DRCN), which
jointly learns a shared encoding representation for two tasks: i) supervised
classification of labeled source data, and ii) unsupervised reconstruction of
unlabeled target data.In this way, the learnt representation not only preserves
discriminability, but also encodes useful information from the target domain.
Our new DRCN model can be optimized by using backpropagation similarly as the
standard neural networks.
  We evaluate the performance of DRCN on a series of cross-domain object
recognition tasks, where DRCN provides a considerable improvement (up to ~8% in
accuracy) over the prior state-of-the-art algorithms. Interestingly, we also
observe that the reconstruction pipeline of DRCN transforms images from the
source domain into images whose appearance resembles the target dataset. This
suggests that DRCN's performance is due to constructing a single composite
representation that encodes information about both the structure of target
images and the classification of source images. Finally, we provide a formal
analysis to justify the algorithm's objective in domain adaptation context.","Deep Reconstruction-Classification Networks for Unsupervised Domain
  Adaptation",2016
333,0.000990364,0.000990406,0.50283578,0.00099027,0.118812758,0.000990352,0.000990315,0.000990431,0.371418957,0.000990367,Topic3,"Finding the most effective way to aggregate multi-subject fMRI data is a
long-standing and challenging problem. It is of increasing interest in
contemporary fMRI studies of human cognition due to the scarcity of data per
subject and the variability of brain anatomy and functional response across
subjects. Recent work on latent factor models shows promising results in this
task but this approach does not preserve spatial locality in the brain. We
examine two ways to combine the ideas of a factor model and a searchlight based
analysis to aggregate multi-subject fMRI data while preserving spatial
locality. We first do this directly by combining a recent factor method known
as a shared response model with searchlight analysis. Then we design a
multi-view convolutional autoencoder for the same task. Both approaches
preserve spatial locality and have competitive or better performance compared
with standard searchlight analysis and the shared response model applied across
the whole brain. We also report a system design to handle the computational
challenge of training the convolutional autoencoder.",A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation,2016
334,0.000990369,0.000990283,0.127111009,0.000990414,0.000990425,0.000990425,0.643076994,0.000990465,0.222879186,0.000990432,Topic7,"One way to solve lasso problems when the dictionary does not fit into
available memory is to first screen the dictionary to remove unneeded features.
Prior research has shown that sequential screening methods offer the greatest
promise in this endeavor. Most existing work on sequential screening targets
the context of tuning parameter selection, where one screens and solves a
sequence of $N$ lasso problems with a fixed grid of geometrically spaced
regularization parameters. In contrast, we focus on the scenario where a target
regularization parameter has already been chosen via cross-validated model
selection, and we then need to solve many lasso instances using this fixed
value. In this context, we propose and explore a feedback controlled sequential
screening scheme. Feedback is used at each iteration to select the next problem
to be solved. This allows the sequence of problems to be adapted to the
instance presented and the number of intermediate problems to be automatically
selected. We demonstrate our feedback scheme using several datasets including a
dictionary of approximate size 100,000 by 300,000.",Feedback-Controlled Sequential Lasso Screening,2016
335,0.001099155,0.001099132,0.378307071,0.068287985,0.001099115,0.001099263,0.545710856,0.001099114,0.001099191,0.001099119,Topic7,"Recently dictionary screening has been proposed as an effective way to
improve the computational efficiency of solving the lasso problem, which is one
of the most commonly used method for learning sparse representations. To
address today's ever increasing large dataset, effective screening relies on a
tight region bound on the solution to the dual lasso. Typical region bounds are
in the form of an intersection of a sphere and multiple half spaces. One way to
tighten the region bound is using more half spaces, which however, adds to the
overhead of solving the high dimensional optimization problem in lasso
screening. This paper reveals the interesting property that the optimization
problem only depends on the projection of features onto the subspace spanned by
the normals of the half spaces. This property converts an optimization problem
in high dimension to much lower dimension, and thus sheds light on reducing the
computation overhead of lasso screening based on tighter region bounds.",The Symmetry of a Simple Optimization Problem in Lasso Screening,2016
336,0.520205187,0.001562863,0.001562809,0.001562799,0.095113556,0.001563,0.373741181,0.001562771,0.001562947,0.001562888,Topic1,"Zero-Shot learning has been shown to be an efficient strategy for domain
adaptation. In this context, this paper builds on the recent work of Bucher et
al. [1], which proposed an approach to solve Zero-Shot classification problems
(ZSC) by introducing a novel metric learning based objective function. This
objective function allows to learn an optimal embedding of the attributes
jointly with a measure of similarity between images and attributes. This paper
extends their approach by proposing several schemes to control the generation
of the negative pairs, resulting in a significant improvement of the
performance and giving above state-of-the-art results on three challenging ZSC
datasets.",Hard Negative Mining for Metric Learning Based Zero-Shot Classification,2016
337,0.609549017,0.000847647,0.000847628,0.151639793,0.000847654,0.000847656,0.036596874,0.197128334,0.000847732,0.000847665,Topic1,"In this paper, we deal with two challenges for measuring the similarity of
the subject identities in practical video-based face recognition - the
variation of the head pose in uncontrolled environments and the computational
expense of processing videos. Since the frame-wise feature mean is unable to
characterize the pose diversity among frames, we define and preserve the
overall pose diversity and closeness in a video. Then, identity will be the
only source of variation across videos since the pose varies even within a
single video. Instead of simply using all the frames, we select those faces
whose pose point is closest to the centroid of the K-means cluster containing
that pose point. Then, we represent a video as a bag of frame-wise deep face
features while the number of features has been reduced from hundreds to K.
Since the video representation can well represent the identity, now we measure
the subject similarity between two videos as the max correlation among all
possible pairs in the two bags of features. On the official 5,000 video-pairs
of the YouTube Face dataset for face verification, our algorithm achieves a
comparable performance with VGG-face that averages over deep features of all
frames. Other vision tasks can also benefit from the generic idea of employing
geometric cues to improve the descriptiveness of deep features.",Pose-Selective Max Pooling for Measuring Similarity,2016
338,0.001149805,0.00114962,0.189345021,0.001149706,0.001149813,0.025679675,0.001149786,0.373064147,0.405012698,0.001149728,Topic9,"A fall is an abnormal activity that occurs rarely, so it is hard to collect
real data for falls. It is, therefore, difficult to use supervised learning
methods to automatically detect falls. Another challenge in using machine
learning methods to automatically detect falls is the choice of engineered
features. In this paper, we propose to use an ensemble of autoencoders to
extract features from different channels of wearable sensor data trained only
on normal activities. We show that the traditional approach of choosing a
threshold as the maximum of the reconstruction error on the training normal
data is not the right way to identify unseen falls. We propose two methods for
automatic tightening of reconstruction error from only the normal activities
for better identification of unseen falls. We present our results on two
activity recognition datasets and show the efficacy of our proposed method
against traditional autoencoder models and two standard one-class
classification methods.","Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble
  of Autoencoders",2016
339,0.001205047,0.125772613,0.001205171,0.001205099,0.001204996,0.392097872,0.001205156,0.001205003,0.473694007,0.001205037,Topic9,"This paper studies the generalization error of invariant classifiers. In
particular, we consider the common scenario where the classification task is
invariant to certain transformations of the input, and that the classifier is
constructed (or learned) to be invariant to these transformations. Our approach
relies on factoring the input space into a product of a base space and a set of
transformations. We show that whereas the generalization error of a
non-invariant classifier is proportional to the complexity of the input space,
the generalization error of an invariant classifier is proportional to the
complexity of the base space. We also derive a set of sufficient conditions on
the geometry of the base space and the set of transformations that ensure that
the complexity of the base space is much smaller than the complexity of the
input space. Our analysis applies to general classifiers such as convolutional
neural networks. We demonstrate the implications of the developed theory for
such classifiers with experiments on the MNIST and CIFAR-10 datasets.",Generalization Error of Invariant Classifiers,2016
340,0.517278497,0.262663525,0.001299325,0.001299083,0.001299155,0.001299173,0.001299146,0.001299194,0.210963959,0.001298944,Topic1,"Given a state-of-the-art deep neural network classifier, we show the
existence of a universal (image-agnostic) and very small perturbation vector
that causes natural images to be misclassified with high probability. We
propose a systematic algorithm for computing universal perturbations, and show
that state-of-the-art deep neural networks are highly vulnerable to such
perturbations, albeit being quasi-imperceptible to the human eye. We further
empirically analyze these universal perturbations and show, in particular, that
they generalize very well across neural networks. The surprising existence of
universal perturbations reveals important geometric correlations among the
high-dimensional decision boundary of classifiers. It further outlines
potential security breaches with the existence of single directions in the
input space that adversaries can possibly exploit to break a classifier on most
natural images.",Universal adversarial perturbations,2016
341,0.262270395,0.032099761,0.12126755,0.000704408,0.126015377,0.000704421,0.000704434,0.098204768,0.357324497,0.00070439,Topic9,"Limited annotated data available for the recognition of facial expression and
action units embarrasses the training of deep networks, which can learn
disentangled invariant features. However, a linear model with just several
parameters normally is not demanding in terms of training data. In this paper,
we propose an elegant linear model to untangle confounding factors in
challenging realistic multichannel signals such as 2D face videos. The simple
yet powerful model does not rely on huge training data and is natural for
recognizing facial actions without explicitly disentangling the identity. Base
on well-understood intuitive linear models such as Sparse Representation based
Classification (SRC), previous attempts require a prepossessing of explicit
decoupling which is practically inexact. Instead, we exploit the low-rank
property across frames to subtract the underlying neutral faces which are
modeled jointly with sparse representation on the action components with group
sparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot
automatic method on raw face videos performs as competitive as SRC applied on
manually prepared action components and performs even better than SRC in terms
of true positive rate. We apply the model to the even more challenging task of
facial action unit recognition, verified on the MPI Face Video Database
(MPI-VDB) achieving a decent performance. All the programs and data have been
made publicly available.",Linear Disentangled Representation Learning for Facial Actions,2017
342,0.735983948,0.001176776,0.001176886,0.00117677,0.001176812,0.001176764,0.001176781,0.044879272,0.210899302,0.001176688,Topic1,"Machine learning and deep learning in particular has advanced tremendously on
perceptual tasks in recent years. However, it remains vulnerable against
adversarial perturbations of the input that have been crafted specifically to
fool the system while being quasi-imperceptible to a human. In this work, we
propose to augment deep neural networks with a small ""detector"" subnetwork
which is trained on the binary classification task of distinguishing genuine
data from data containing adversarial perturbations. Our method is orthogonal
to prior work on addressing adversarial perturbations, which has mostly focused
on making the classification network itself more robust. We show empirically
that adversarial perturbations can be detected surprisingly well even though
they are quasi-imperceptible to humans. Moreover, while the detectors have been
trained to detect only a specific adversary, they generalize to similar and
weaker adversaries. In addition, we propose an adversarial attack that fools
both the classifier and the detector and a novel training procedure for the
detector that counteracts this attack.",On Detecting Adversarial Perturbations,2017
343,0.542415959,0.00097113,0.000971187,0.000971268,0.000971135,0.146165933,0.086849024,0.000971052,0.218742196,0.000971117,Topic1,"Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN's training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide more accurate estimation on the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.",Activation Maximization Generative Adversarial Nets,2017
344,0.184010059,0.001316283,0.001315993,0.001316062,0.523738905,0.047076787,0.178008167,0.001316049,0.060585664,0.001316032,Topic5,"As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks ""look"" in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.",Interpretable Explanations of Black Boxes by Meaningful Perturbation,2017
345,0.000893023,0.077099708,0.000893032,0.000893203,0.000893124,0.574605601,0.00089312,0.000893155,0.342042987,0.000893048,Topic6,"Though the deep learning is pushing the machine learning to a new stage,
basic theories of machine learning are still limited. The principle of
learning, the role of the a prior knowledge, the role of neuron bias, and the
basis for choosing neural transfer function and cost function, etc., are still
far from clear. In this paper, we present a general theoretical framework for
machine learning. We classify the prior knowledge into common and
problem-dependent parts, and consider that the aim of learning is to maximally
incorporate them. The principle we suggested for maximizing the former is the
design risk minimization principle, while the neural transfer function, the
cost function, as well as pretreatment of samples, are endowed with the role
for maximizing the latter. The role of the neuron bias is explained from a
different angle. We develop a Monte Carlo algorithm to establish the
input-output responses, and we control the input-output sensitivity of a
learning machine by controlling that of individual neurons. Applications of
function approaching and smoothing, pattern recognition and classification, are
provided to illustrate how to train general learning machines based on our
theory and algorithm. Our method may in addition induce new applications, such
as the transductive inference.",A General Theory for Training Learning Machine,2017
346,0.001449679,0.001449635,0.185386738,0.00144969,0.20705274,0.001449767,0.001449767,0.001449579,0.597412857,0.001449548,Topic9,"This paper introduces a generalization of Convolutional Neural Networks
(CNNs) from low-dimensional grid data, such as images, to graph-structured
data. We propose a novel spatial convolution utilizing a random walk to uncover
the relations within the input, analogous to the way the standard convolution
uses the spatial neighborhood of a pixel on the grid. The convolution has an
intuitive interpretation, is efficient and scalable and can also be used on
data with varying graph structure. Furthermore, this generalization can be
applied to many standard regression or classification problems, by learning the
the underlying graph. We empirically demonstrate the performance of the
proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular
activity data set.","A Generalization of Convolutional Neural Networks to Graph-Structured
  Data",2017
347,0.188235812,0.160360491,0.001351903,0.001351624,0.001351717,0.234345086,0.18831619,0.001351765,0.221983675,0.001351737,Topic6,"Recent work has shown that state-of-the-art classifiers are quite brittle, in
the sense that a small adversarial change of an originally with high confidence
correctly classified input leads to a wrong classification again with high
confidence. This raises concerns that such classifiers are vulnerable to
attacks and calls into question their usage in safety-critical systems. We show
in this paper for the first time formal guarantees on the robustness of a
classifier by giving instance-specific lower bounds on the norm of the input
manipulation required to change the classifier decision. Based on this analysis
we propose the Cross-Lipschitz regularization functional. We show that using
this form of regularization in kernel methods resp. neural networks improves
the robustness of the classifier without any loss in prediction performance.","Formal Guarantees on the Robustness of a Classifier against Adversarial
  Manipulation",2017
348,0.218627302,0.140935835,0.001031154,0.227532166,0.001031252,0.073972928,0.001031214,0.001031104,0.333775955,0.00103109,Topic9,"The goal of this paper is to analyze the geometric properties of deep neural
network classifiers in the input space. We specifically study the topology of
classification regions created by deep networks, as well as their associated
decision boundary. Through a systematic empirical investigation, we show that
state-of-the-art deep nets learn connected classification regions, and that the
decision boundary in the vicinity of datapoints is flat along most directions.
We further draw an essential connection between two seemingly unrelated
properties of deep networks: their sensitivity to additive perturbations in the
inputs, and the curvature of their decision boundary. The directions where the
decision boundary is curved in fact remarkably characterize the directions to
which the classifier is the most vulnerable. We finally leverage a fundamental
asymmetry in the curvature of the decision boundary of deep nets, and propose a
method to discriminate between original images, and images perturbed with small
adversarial examples. We show the effectiveness of this purely geometric
approach for detecting small adversarial perturbations in images, and for
recovering the labels of perturbed images.",Classification regions of deep neural networks,2017
349,0.255631418,0.272893865,0.001190734,0.148161757,0.001190761,0.153405205,0.001190767,0.001190635,0.16395417,0.001190688,Topic2,"Deep networks have recently been shown to be vulnerable to universal
perturbations: there exist very small image-agnostic perturbations that cause
most natural images to be misclassified by such classifiers. In this paper, we
propose the first quantitative analysis of the robustness of classifiers to
universal perturbations, and draw a formal link between the robustness to
universal perturbations, and the geometry of the decision boundary.
Specifically, we establish theoretical bounds on the robustness of classifiers
under two decision boundary models (flat and curved models). We show in
particular that the robustness of deep networks to universal perturbations is
driven by a key property of their curvature: there exists shared directions
along which the decision boundary of deep networks is systematically positively
curved. Under such conditions, we prove the existence of small universal
perturbations. Our analysis further provides a novel geometric method for
computing universal perturbations, in addition to explaining their properties.",Analysis of universal adversarial perturbations,2017
350,0.410017465,0.001176642,0.001176626,0.001176728,0.001176762,0.29759346,0.001176827,0.00117658,0.284152238,0.001176672,Topic1,"Generative adversarial networks (GANs) can implicitly learn rich
distributions over images, audio, and data which are hard to model with an
explicit likelihood. We present a practical Bayesian formulation for
unsupervised and semi-supervised learning with GANs. Within this framework, we
use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of
the generator and discriminator networks. The resulting approach is
straightforward and obtains good performance without any standard interventions
such as feature matching, or mini-batch discrimination. By exploring an
expressive posterior over the parameters of the generator, the Bayesian GAN
avoids mode-collapse, produces interpretable and diverse candidate samples, and
provides state-of-the-art quantitative results for semi-supervised learning on
benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,
Wasserstein GANs, and DCGAN ensembles.",Bayesian GAN,2017
351,0.928101257,0.001818463,0.001818854,0.001818699,0.001818725,0.057349579,0.001818543,0.001818631,0.001818639,0.00181861,Topic1,"We present a new model DrNET that learns disentangled image representations
from video. Our approach leverages the temporal coherence of video and a novel
adversarial loss to learn a representation that factorizes each frame into a
stationary part and a temporally varying component. The disentangled
representation can be used for a range of tasks. For example, applying a
standard LSTM to the time-vary components enables prediction of future frames.
We evaluate our approach on a range of synthetic and real videos, demonstrating
the ability to coherently generate hundreds of steps into the future.",Unsupervised Learning of Disentangled Representations from Video,2017
352,0.465578161,0.001408774,0.001408771,0.001408735,0.001408676,0.523151893,0.00140891,0.001408647,0.001408752,0.001408681,Topic6,"Generative adversarial nets (GANs) are a promising technique for modeling a
distribution from samples. It is however well known that GAN training suffers
from instability due to the nature of its maximin formulation. In this paper,
we explore ways to tackle the instability problem by dualizing the
discriminator. We start from linear discriminators in which case conjugate
duality provides a mechanism to reformulate the saddle point objective into a
maximization problem, such that both the generator and the discriminator of
this 'dualing GAN' act in concert. We then demonstrate how to extend this
intuition to non-linear formulations. For GANs with linear discriminators our
approach is able to remove the instability in training, while for GANs with
nonlinear discriminators our approach provides an alternative to the commonly
used GAN training algorithm.",Dualing GANs,2017
353,0.000990418,0.000990235,0.532354331,0.000990419,0.00099025,0.000990309,0.000990345,0.000990368,0.459723093,0.00099023,Topic3,"Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT
are computationally expensive. To address this problem, we recently proposed
the world-first deep convolutional neural network (CNN) for low-dose X-ray CT
and won the second place in 2016 AAPM Low-Dose CT Grand Challenge. However,
some of the texture were not fully recovered. To cope with this problem, here
we propose a deep residual learning approach in directional wavelet domain. The
proposed method is motivated by an observation that a deep convolutional neural
network can be interpreted as a multilayer convolutional framelets expansion
using non-local basis convolved with data-driven local basis. We further extend
the idea to derive a deep convolutional framelet expansion by combining global
redundant transforms and signal boosting from multiple signal representations.
Extensive experimental results confirm that the proposed network has
significantly improved performance and preserves the detail texture of the
original images","Wavelet Residual Network for Low-Dose CT via Deep Convolutional
  Framelets",2017
354,0.524523584,0.00091779,0.000917688,0.21666943,0.178543213,0.000917687,0.000917682,0.074757526,0.000917738,0.000917662,Topic1,"The success of various applications including robotics, digital content
creation, and visualization demand a structured and abstract representation of
the 3D world from limited sensor data. Inspired by the nature of human
perception of 3D shapes as a collection of simple parts, we explore such an
abstract shape representation based on primitives. Given a single depth image
of an object, we present 3D-PRNN, a generative recurrent neural network that
synthesizes multiple plausible shapes composed of a set of primitives. Our
generative model encodes symmetry characteristics of common man-made objects,
preserves long-range structural coherence, and describes objects of varying
complexity with a compact representation. We also propose a method based on
Gaussian Fields to generate a large scale dataset of primitive-based shape
representations to train our network. We evaluate our approach on a wide range
of examples and show that it outperforms nearest-neighbor based shape retrieval
methods and is on-par with voxel-based generative models while using a
significantly reduced parameter space.",3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks,2017
355,0.096136056,0.002273523,0.002273201,0.002273171,0.002273257,0.508841792,0.002273232,0.002273542,0.210258411,0.171123814,Topic6,"In this article, we mathematically study several GAN related topics,
including Inception score, label smoothing, gradient vanishing and the
-log(D(x)) alternative.
  --- An advanced version is included in arXiv:1703.02000 ""Activation
Maximization Generative Adversarial Nets"". Please refer Section 6 in 1703.02000
for detailed analysis on Inception Score, and refer its appendix for the
discussions on Label Smoothing, Gradient Vanishing and -log(D(x)) Alternative.
---","Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))
  Alternative",2017
356,0.091733444,0.001000216,0.001000101,0.0010004,0.405982423,0.30501997,0.001000323,0.001000306,0.191262656,0.001000161,Topic5,"Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.",A Brief Survey of Deep Reinforcement Learning,2017
357,0.22256489,0.000649511,0.00064978,0.000649499,0.000649433,0.000649561,0.192678387,0.210381896,0.290142754,0.08098429,Topic9,"Large-scale deep neural networks (DNNs) are both compute and memory
intensive. As the size of DNNs continues to grow, it is critical to improve the
energy efficiency and performance while maintaining accuracy. For DNNs, the
model size is an important factor affecting performance, scalability and energy
efficiency. Weight pruning achieves good compression ratios but suffers from
three drawbacks: 1) the irregular network structure after pruning; 2) the
increased training complexity; and 3) the lack of rigorous guarantee of
compression ratio and inference accuracy. To overcome these limitations, this
paper proposes CirCNN, a principled approach to represent weights and process
neural networks using block-circulant matrices. CirCNN utilizes the Fast
Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the
computational complexity (both in inference and training) from O(n2) to
O(nlogn) and the storage complexity from O(n2) to O(n), with negligible
accuracy loss. Compared to other approaches, CirCNN is distinct due to its
mathematical rigor: it can converge to the same effectiveness as DNNs without
compression. The CirCNN architecture, a universal DNN inference engine that can
be implemented on various hardware/software platforms with configurable network
architecture. To demonstrate the performance and energy efficiency, we test
CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN
architecture achieves very high energy efficiency and performance with a small
hardware footprint. Based on the FPGA implementation and ASIC synthesis
results, CirCNN achieves 6-102X energy efficiency improvements compared with
the best state-of-the-art results.","CirCNN: Accelerating and Compressing Deep Neural Networks Using
  Block-CirculantWeight Matrices",2017
358,0.166856073,0.00138913,0.149650922,0.001389133,0.001389296,0.055972011,0.00138912,0.001389259,0.619185827,0.00138923,Topic9,"We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.","XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual
  Classification",2017
359,0.001136744,0.001136612,0.055312662,0.001136607,0.719112272,0.00113669,0.001136614,0.001136554,0.001136721,0.217618524,Topic5,"Low dimensional embeddings that capture the main variations of interest in
collections of data are important for many applications. One way to construct
these embeddings is to acquire estimates of similarity from the crowd. However,
similarity is a multi-dimensional concept that varies from individual to
individual. Existing models for learning embeddings from the crowd typically
make simplifying assumptions such as all individuals estimate similarity using
the same criteria, the list of criteria is known in advance, or that the crowd
workers are not influenced by the data that they see. To overcome these
limitations we introduce Context Embedding Networks (CENs). In addition to
learning interpretable embeddings from images, CENs also model worker biases
for different attributes along with the visual context i.e. the visual
attributes highlighted by a set of images. Experiments on two noisy crowd
annotated datasets show that modeling both worker bias and visual context
results in more interpretable embeddings compared to existing approaches.",Context Embedding Networks,2017
360,0.28500808,0.000826691,0.000826557,0.00082659,0.353728253,0.000826618,0.000826552,0.000826704,0.355477336,0.00082662,Topic9,"The meteoric rise of deep learning models in computer vision research, having
achieved human-level accuracy in image recognition tasks is firm evidence of
the impact of representation learning of deep neural networks. In the chemistry
domain, recent advances have also led to the development of similar CNN models,
such as Chemception, that is trained to predict chemical properties using
images of molecular drawings. In this work, we investigate the effects of
systematically removing and adding localized domain-specific information to the
image channels of the training data. By augmenting images with only 3
additional basic information, and without introducing any architectural
changes, we demonstrate that an augmented Chemception (AugChemception)
outperforms the original model in the prediction of toxicity, activity, and
solvation free energy. Then, by altering the information content in the images,
and examining the resulting model's performance, we also identify two distinct
learning patterns in predicting toxicity/activity as compared to solvation free
energy. These patterns suggest that Chemception is learning about its tasks in
the manner that is consistent with established knowledge. Thus, our work
demonstrates that advanced chemical knowledge is not a pre-requisite for deep
learning models to accurately predict complex chemical properties.","How Much Chemistry Does a Deep Neural Network Need to Know to Make
  Accurate Predictions?",2017
361,0.490786859,0.001492756,0.001492969,0.001492785,0.001492998,0.497270004,0.001492881,0.001492802,0.001492992,0.001492953,Topic6,"Disentangled representations, where the higher level data generative factors
are reflected in disjoint latent dimensions, offer several benefits such as
ease of deriving invariant representations, transferability to other tasks,
interpretability, etc. We consider the problem of unsupervised learning of
disentangled representations from large pool of unlabeled observations, and
propose a variational inference based approach to infer disentangled latent
factors. We introduce a regularizer on the expectation of the approximate
posterior over observed data that encourages the disentanglement. We evaluate
the proposed approach using several quantitative metrics and empirically
observe significant gains over existing methods in terms of both
disentanglement and data likelihood (reconstruction quality).","Variational Inference of Disentangled Latent Concepts from Unlabeled
  Observations",2017
362,0.000609879,0.000609837,0.000609839,0.000609914,0.000609862,0.37990615,0.557803338,0.000609858,0.058021498,0.000609826,Topic7,"We study the properties of the endpoint of stochastic gradient descent (SGD).
By approximating SGD as a stochastic differential equation (SDE) we consider
the Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption
of isotropic variance in loss gradients. Through this analysis, we find that
three factors - learning rate, batch size and the variance of the loss
gradients - control the trade-off between the depth and width of the minima
found by SGD, with wider minima favoured by a higher ratio of learning rate to
batch size. We have direct control over the learning rate and batch size, while
the variance is determined by the choice of model architecture, model
parameterization and dataset. In the equilibrium distribution only the ratio of
learning rate to batch size appears, implying that the equilibrium distribution
is invariant under a simultaneous rescaling of learning rate and batch size by
the same amount. We then explore experimentally how learning rate and batch
size affect SGD from two perspectives: the endpoint of SGD and the dynamics
that lead up to it. For the endpoint, the experiments suggest the endpoint of
SGD is invariant under simultaneous rescaling of batch size and learning rate,
and also that a higher ratio leads to flatter minima, both findings are
consistent with our theoretical analysis. We note experimentally that the
dynamics also seem to be invariant under the same rescaling of learning rate
and batch size, which we explore showing that one can exchange batch size and
learning rate for cyclical learning rate schedule. Next, we illustrate how
noise affects memorization, showing that high noise levels lead to better
generalization. Finally, we find experimentally that the invariance under
simultaneous rescaling of learning rate and batch size breaks down if the
learning rate gets too large or the batch size gets too small.",Three Factors Influencing Minima in SGD,2017
363,0.000980621,0.038217014,0.000980545,0.000980683,0.298445725,0.063699055,0.00098066,0.000980662,0.593754441,0.000980594,Topic9,"Achieving superhuman playing level by AlphaGo corroborated the capabilities
of convolutional neural architectures (CNNs) for capturing complex spatial
patterns. This result was to a great extent due to several analogies between Go
board states and 2D images CNNs have been designed for, in particular
translational invariance and a relatively large board. In this paper, we verify
whether CNN-based move predictors prove effective for Othello, a game with
significantly different characteristics, including a much smaller board size
and complete lack of translational invariance. We compare several CNN
architectures and board encodings, augment them with state-of-the-art
extensions, train on an extensive database of experts' moves, and examine them
with respect to move prediction accuracy and playing strength. The empirical
evaluation confirms high capabilities of neural move predictors and suggests a
strong correlation between prediction accuracy and playing strength. The best
CNNs not only surpass all other 1-ply Othello players proposed to date but
defeat (2-ply) Edax, the best open-source Othello player.",Learning to Play Othello with Deep Neural Networks,2017
364,0.001205126,0.00120509,0.148825996,0.15422095,0.248545645,0.083973384,0.00120504,0.114160661,0.245453059,0.001205048,Topic5,"Can artificial intelligence (AI) learn complicated non-linear physics? Here
we propose a novel deep learning approach that learns non-linear photon
scattering physics and obtains accurate 3D distribution of optical anomalies.
In contrast to the traditional black-box deep learning approaches to inverse
problems, our deep network learns to invert the Lippmann-Schwinger integral
equation which describes the essential physics of photon migration of diffuse
near-infrared (NIR) photons in turbid media. As an example for clinical
relevance, we applied the method to our prototype diffuse optical tomography
(DOT). We show that our deep neural network, trained with only simulation data,
can accurately recover the location of anomalies within biomimetic phantoms and
live animals without the use of an exogenous contrast agent.","Deep Learning Can Reverse Photon Migration for Diffuse Optical
  Tomography",2017
365,0.248821693,0.000917777,0.000917564,0.000917565,0.193618778,0.000917592,0.000917591,0.000917632,0.551136096,0.000917712,Topic9,"With access to large datasets, deep neural networks (DNN) have achieved
human-level accuracy in image and speech recognition tasks. However, in
chemistry, data is inherently small and fragmented. In this work, we develop an
approach of using rule-based knowledge for training ChemNet, a transferable and
generalizable deep neural network for chemical property prediction that learns
in a weak-supervised manner from large unlabeled chemical databases. When
coupled with transfer learning approaches to predict other smaller datasets for
chemical properties that it was not originally trained on, we show that
ChemNet's accuracy outperforms contemporary DNN models that were trained using
conventional supervised learning. Furthermore, we demonstrate that the ChemNet
pre-training approach is equally effective on both CNN (Chemception) and RNN
(SMILES2vec) models, indicating that this approach is network architecture
agnostic and is effective across multiple data modalities. Our results indicate
a pre-trained ChemNet that incorporates chemistry domain knowledge, enables the
development of generalizable neural networks for more accurate prediction of
novel chemical properties.","Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for
  Transferable Chemical Property Prediction",2017
366,0.190899532,0.000854877,0.47864316,0.000854951,0.000854901,0.000854903,0.000854978,0.155490273,0.169837589,0.000854835,Topic3,"In portable, three dimensional, and ultra-fast ultrasound (US) imaging
systems, there is an increasing need to reconstruct high quality images from a
limited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling.
However, due to the severe side lobe artifacts from RF sub-sampling, the
standard beam-former often produces blurry images with less contrast that are
not suitable for diagnostic purpose. To address this problem, some researchers
have studied compressed sensing (CS) to exploit the sparsity of the image or RF
data in some domains. However, the existing CS approaches require either
hardware changes or computationally expensive algorithms. To overcome these
limitations, here we propose a novel deep learning approach that directly
interpolates the missing RF data by utilizing redundancy in the Rx-SC plane. In
particular, the network design principle derives from a novel interpretation of
the deep neural network as a cascaded convolution framelets that learns the
data-driven bases for Hankel matrix decomposition. Our extensive experimental
results from sub-sampled RF data from a real US system confirmed that the
proposed method can effectively reduce the data rate without sacrificing the
image quality.",Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging,2017
367,0.222305081,0.001266036,0.694428983,0.001266257,0.023633112,0.001266108,0.001266137,0.001266059,0.052036261,0.001265965,Topic3,"Interior tomography for the region-of-interest (ROI) imaging has advantages
of using a small detector and reducing X-ray radiation dose. However, standard
analytic reconstruction suffers from severe cupping artifacts due to existence
of null space in the truncated Radon transform. Existing penalized
reconstruction methods may address this problem but they require extensive
computations due to the iterative reconstruction. Inspired by the recent deep
learning approaches to low-dose and sparse view CT, here we propose a deep
learning architecture that removes null space signals from the FBP
reconstruction. Experimental results have shown that the proposed method
provides near-perfect reconstruction with about 7-10 dB improvement in PSNR
over existing methods in spite of significantly reduced run-time complexity.",Deep Learning Interior Tomography for Region-of-Interest Reconstruction,2017
368,0.189482522,0.001052867,0.299349177,0.277883632,0.001052986,0.001052885,0.001052872,0.146929738,0.0810905,0.00105282,Topic3,"For homeland and transportation security applications, 2D X-ray explosive
detection system (EDS) have been widely used, but they have limitations in
recognizing 3D shape of the hidden objects. Among various types of 3D computed
tomography (CT) systems to address this issue, this paper is interested in a
stationary CT using fixed X-ray sources and detectors. However, due to the
limited number of projection views, analytic reconstruction algorithms produce
severe streaking artifacts. Inspired by recent success of deep learning
approach for sparse view CT reconstruction, here we propose a novel image and
sinogram domain deep learning architecture for 3D reconstruction from very
sparse view measurement. The algorithm has been tested with the real data from
a prototype 9-view dual energy stationary CT EDS carry-on baggage scanner
developed by GEMSS Medical Systems, Korea, which confirms the superior
reconstruction performance over the existing approaches.",Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner,2018
369,0.000763554,0.000763541,0.000763514,0.000763492,0.000763521,0.00076355,0.197657209,0.00076348,0.796234614,0.000763524,Topic9,"Deep learning has shown promising results on many machine learning tasks but
DL models are often complex networks with large number of neurons and layers,
and recently, complex layer structures known as building blocks. Finding the
best deep model requires a combination of finding both the right architecture
and the correct set of parameters appropriate for that architecture. In
addition, this complexity (in terms of layer types, number of neurons, and
number of layers) also present problems with generalization since larger
networks are easier to overfit to the data. In this paper, we propose a search
framework for finding effective architectural building blocks for convolutional
neural networks (CNN). Our approach is much faster at finding models that are
close to state-of-the-art in performance. In addition, the models discovered by
our approach are also smaller than models discovered by similar techniques. We
achieve these twin advantages by designing our search space in such a way that
it searches over a reduced set of state-of-the-art building blocks for CNNs
including residual block, inception block, inception-residual block, ResNeXt
block and many others. We apply this technique to generate models for multiple
image datasets and show that these models achieve performance comparable to
state-of-the-art (and even surpassing the state-of-the-art in one case). We
also show that learned models are transferable between datasets.","Effective Building Block Design for Deep Convolutional Neural Networks
  using Search",2018
370,0.434668737,0.000877402,0.000877424,0.000877334,0.000877404,0.265622272,0.000877393,0.00087742,0.293567152,0.000877463,Topic1,"Deep metric learning has been demonstrated to be highly effective in learning
semantic representation and encoding information that can be used to measure
data similarity, by relying on the embedding learned from metric learning. At
the same time, variational autoencoder (VAE) has widely been used to
approximate inference and proved to have a good performance for directed
probabilistic models. However, for traditional VAE, the data label or feature
information are intractable. Similarly, traditional representation learning
approaches fail to represent many salient aspects of the data. In this project,
we propose a novel integrated framework to learn latent embedding in VAE by
incorporating deep metric learning. The features are learned by optimizing a
triplet loss on the mean vectors of VAE in conjunction with standard evidence
lower bound (ELBO) of VAE. This approach, which we call Triplet based
Variational Autoencoder (TVAE), allows us to capture more fine-grained
information in the latent embedding. Our model is tested on MNIST data set and
achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma &
Welling, 2013) achieves triplet accuracy of 75.08%.",TVAE: Triplet-Based Variational Autoencoder using Metric Learning,2018
371,0.000833592,0.000833566,0.000833474,0.061060503,0.721593416,0.126386156,0.000833449,0.000833542,0.085958793,0.000833509,Topic5,"Infants are experts at playing, with an amazing ability to generate novel
structured behaviors in unstructured environments that lack clear extrinsic
reward signals. We seek to mathematically formalize these abilities using a
neural network that implements curiosity-driven intrinsic motivation. Using a
simple but ecologically naturalistic simulated environment in which an agent
can move and interact with objects it sees, we propose a ""world-model"" network
that learns to predict the dynamic consequences of the agent's actions.
Simultaneously, we train a separate explicit ""self-model"" that allows the agent
to track the error map of its own world-model, and then uses the self-model to
adversarially challenge the developing world-model. We demonstrate that this
policy causes the agent to explore novel and informative interactions with its
environment, leading to the generation of a spectrum of complex behaviors,
including ego-motion prediction, object attention, and object gathering.
Moreover, the world-model that the agent learns supports improved performance
on object dynamics prediction, detection, localization and recognition tasks.
Taken together, our results are initial steps toward creating flexible
autonomous agents that self-supervise in complex novel physical environments.",Learning to Play with Intrinsically-Motivated Self-Aware Agents,2018
372,0.000961862,0.000961848,0.000961704,0.000961928,0.811267207,0.131713813,0.000961664,0.000961761,0.05028651,0.000961704,Topic5,"Infants are experts at playing, with an amazing ability to generate novel
structured behaviors in unstructured environments that lack clear extrinsic
reward signals. We seek to replicate some of these abilities with a neural
network that implements curiosity-driven intrinsic motivation. Using a simple
but ecologically naturalistic simulated environment in which the agent can move
and interact with objects it sees, the agent learns a world model predicting
the dynamic consequences of its actions. Simultaneously, the agent learns to
take actions that adversarially challenge the developing world model, pushing
the agent to explore novel and informative interactions with its environment.
We demonstrate that this policy leads to the self-supervised emergence of a
spectrum of complex behaviors, including ego motion prediction, object
attention, and object gathering. Moreover, the world model that the agent
learns supports improved performance on object dynamics prediction and
localization tasks. Our results are a proof-of-principle that computational
models of intrinsic motivation might account for key features of developmental
visuomotor learning in infants.","Emergence of Structured Behaviors from Curiosity-Based Intrinsic
  Motivation",2018
373,0.751842084,0.001667063,0.001666851,0.001666961,0.001667211,0.234821892,0.001667003,0.001667045,0.001666939,0.001666949,Topic1,"Generating video frames that accurately predict future world states is
challenging. Existing approaches either fail to capture the full distribution
of outcomes, or yield blurry generations, or both. In this paper we introduce
an unsupervised video generation model that learns a prior model of uncertainty
in a given environment. Video frames are generated by drawing samples from this
prior and combining them with a deterministic estimate of the future frame. The
approach is simple and easily trained end-to-end on a variety of datasets.
Sample generations are both varied and sharp, even many frames into the future,
and compare favorably to those from existing approaches.",Stochastic Video Generation with a Learned Prior,2018
374,0.502103055,0.000680337,0.00068039,0.166880218,0.000680414,0.000680383,0.000680507,0.000680357,0.32625396,0.00068038,Topic1,"Supervised object detection and semantic segmentation require object or even
pixel level annotations. When there exist image level labels only, it is
challenging for weakly supervised algorithms to achieve accurate predictions.
The accuracy achieved by top weakly supervised algorithms is still
significantly lower than their fully supervised counterparts. In this paper, we
propose a novel weakly supervised curriculum learning pipeline for multi-label
object recognition, detection and semantic segmentation. In this pipeline, we
first obtain intermediate object localization and pixel labeling results for
the training images, and then use such results to train task-specific deep
networks in a fully supervised manner. The entire process consists of four
stages, including object localization in the training images, filtering and
fusing object instances, pixel labeling for the training images, and
task-specific network training. To obtain clean object instances in the
training images, we propose a novel algorithm for filtering, fusing and
classifying object instances collected from multiple solution mechanisms. In
this algorithm, we incorporate both metric learning and density-based
clustering to filter detected object instances. Experiments show that our
weakly supervised pipeline achieves state-of-the-art results in multi-label
image classification as well as weakly supervised object detection and very
competitive results in weakly supervised semantic segmentation on MS-COCO,
PASCAL VOC 2007 and PASCAL VOC 2012.","Multi-Evidence Filtering and Fusion for Multi-Label Classification,
  Object Detection and Semantic Segmentation Based on Weakly Supervised
  Learning",2018
375,0.04255365,0.418306604,0.001449703,0.001449593,0.001449565,0.001449689,0.001449562,0.001449456,0.528992602,0.001449577,Topic9,"In the recent literature the important role of depth in deep learning has
been emphasized. In this paper we argue that sufficient width of a feedforward
network is equally important by answering the simple question under which
conditions the decision regions of a neural network are connected. It turns out
that for a class of activation functions including leaky ReLU, neural networks
having a pyramidal structure, that is no layer has more hidden units than the
input dimension, produce necessarily connected decision regions. This implies
that a sufficiently wide layer is necessary to produce disconnected decision
regions. We discuss the implications of this result for the construction of
neural networks, in particular the relation to the problem of adversarial
manipulation of classifiers.","Neural Networks Should Be Wide Enough to Learn Disconnected Decision
  Regions",2018
376,0.001351928,0.001351626,0.112490365,0.00135176,0.462565429,0.001351516,0.001351626,0.001351671,0.4154825,0.001351579,Topic5,"We develop three efficient approaches for generating visual explanations from
3D convolutional neural networks (3D-CNNs) for Alzheimer's disease
classification. One approach conducts sensitivity analysis on hierarchical 3D
image segmentation, and the other two visualize network activations on a
spatial map. Visual checks and a quantitative localization benchmark indicate
that all approaches identify important brain parts for Alzheimer's disease
diagnosis. Comparative analysis show that the sensitivity analysis based
approach has difficulty handling loosely distributed cerebral cortex, and
approaches based on visualization of activations are constrained by the
resolution of the convolutional layer. The complementarity of these methods
improves the understanding of 3D-CNNs in Alzheimer's disease classification
from different perspectives.","Visual Explanations From Deep 3D Convolutional Neural Networks for
  Alzheimer's Disease Classification",2018
377,0.180566818,0.001204943,0.001204987,0.001205226,0.001204937,0.00120511,0.3425738,0.001204928,0.468424293,0.001204958,Topic9,"Deep neural networks are typically trained by optimizing a loss function with
an SGD variant, in conjunction with a decaying learning rate, until
convergence. We show that simple averaging of multiple points along the
trajectory of SGD, with a cyclical or constant learning rate, leads to better
generalization than conventional training. We also show that this Stochastic
Weight Averaging (SWA) procedure finds much broader optima than SGD, and
approximates the recent Fast Geometric Ensembling (FGE) approach with a single
model. Using SWA we achieve notable improvement in test accuracy over
conventional SGD training on a range of state-of-the-art residual networks,
PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and
ImageNet. In short, SWA is extremely easy to implement, improves
generalization, and has almost no computational overhead.",Averaging Weights Leads to Wider Optima and Better Generalization,2018
378,0.00060621,0.000606264,0.208533884,0.043492466,0.00060617,0.179463835,0.164181097,0.000606248,0.345985294,0.055918533,Topic9,"By drawing on ideas from optimisation theory, artificial neural networks
(ANN), graph embeddings and sparse representations, I develop a novel
technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at
addressing the feature extraction problem. The proposed method uses (preferably
deep) ANNs for projecting input attribute vectors to an output space wherein
pairwise distances are maximized for vectors belonging to different classes,
but minimized for those belonging to the same class, while simultaneously
enforcing sparsity on the ANN outputs. The vectors that result from the
projection can then be used as features in any classifier of choice.
Mathematically, I formulate the proposed method as the minimisation of an
objective function which can be interpreted, in the ANN output space, as a
negative factor of the sum of the squares of the pair-wise distances between
output vectors belonging to different classes, added to a positive factor of
the sum of squares of the pair-wise distances between output vectors belonging
to the same classes, plus sparsity and weight decay terms. To derive an
algorithm for minimizing the objective function via gradient descent, I use the
multi-variate version of the chain rule to obtain the partial derivatives of
the function with respect to ANN weights and biases, and find that each of the
required partial derivatives can be expressed as a sum of six terms. As it
turns out, four of those six terms can be computed using the standard back
propagation algorithm; the fifth can be computed via a slight modification of
the standard backpropagation algorithm; while the sixth one can be computed via
simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora
of digits and letters, the CMU PIE database of faces, the MNIST digits
database, and other standard machine learning databases.",SENNS: Sparse Extraction Neural NetworkS for Feature Extraction,2014
379,0.436107271,0.001219808,0.001219734,0.001219692,0.049543753,0.505810346,0.00121979,0.001219871,0.001219891,0.001219843,Topic6,"We propose a method to optimize the representation and distinguishability of
samples from two probability distributions, by maximizing the estimated power
of a statistical test based on the maximum mean discrepancy (MMD). This
optimized MMD is applied to the setting of unsupervised learning by generative
adversarial networks (GAN), in which a model attempts to generate realistic
samples, and a discriminator attempts to tell these apart from data samples. In
this context, the MMD may be used in two roles: first, as a discriminator,
either directly on the samples, or on features of the samples. Second, the MMD
can be used to evaluate the performance of a generative model, by testing the
model's samples against a reference data set. In the latter role, the optimized
MMD is particularly helpful, as it gives an interpretable indication of how the
model and data distributions differ, even in cases where individual model
samples are not easily distinguished either by eye or by classifier.","Generative Models and Model Criticism via Optimized Maximum Mean
  Discrepancy",2016
380,0.001449657,0.001449562,0.06123036,0.001449624,0.001449627,0.468788144,0.175302172,0.001449802,0.285981558,0.001449494,Topic6,"Many real world stochastic control problems suffer from the ""curse of
dimensionality"". To overcome this difficulty, we develop a deep learning
approach that directly solves high-dimensional stochastic control problems
based on Monte-Carlo sampling. We approximate the time-dependent controls as
feedforward neural networks and stack these networks together through model
dynamics. The objective function for the control problem plays the role of the
loss function for the deep neural network. We test this approach using examples
from the areas of optimal trading and energy storage. Our results suggest that
the algorithm presented here achieves satisfactory accuracy and at the same
time, can handle rather high dimensional problems.",Deep Learning Approximation for Stochastic Control Problems,2016
381,0.22251895,0.211657425,0.001041838,0.001041832,0.42764626,0.001041966,0.001041942,0.001041882,0.131925862,0.001042043,Topic5,"In de novo drug design, computational strategies are used to generate novel
molecules with good affinity to the desired biological target. In this work, we
show that recurrent neural networks can be trained as generative models for
molecular structures, similar to statistical language models in natural
language processing. We demonstrate that the properties of the generated
molecules correlate very well with the properties of the molecules used to
train the model. In order to enrich libraries with molecules active towards a
given biological target, we propose to fine-tune the model with small sets of
molecules, which are known to be active against that target.
  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test
molecules that medicinal chemists designed, whereas against Plasmodium
falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled
with a scoring function, our model can perform the complete de novo drug design
cycle to generate large sets of novel molecules for drug discovery.","Generating Focussed Molecule Libraries for Drug Discovery with Recurrent
  Neural Networks",2017
382,0.001234935,0.001234804,0.001234835,0.001234978,0.001235115,0.726936526,0.149245802,0.001234933,0.115173243,0.001234829,Topic6,"Deep reinforcement learning (RL) methods generally engage in exploratory
behavior through noise injection in the action space. An alternative is to add
noise directly to the agent's parameters, which can lead to more consistent
exploration and a richer set of behaviors. Methods such as evolutionary
strategies use parameter perturbations, but discard all temporal structure in
the process and require significantly more samples. Combining parameter noise
with traditional RL methods allows to combine the best of both worlds. We
demonstrate that both off- and on-policy methods benefit from this approach
through experimental comparison of DQN, DDPG, and TRPO on high-dimensional
discrete action environments as well as continuous control tasks. Our results
show that RL with parameter noise learns more efficiently than traditional RL
with action space noise and evolutionary strategies individually.",Parameter Space Noise for Exploration,2017
383,0.000793789,0.065212851,0.000793765,0.038615778,0.000793862,0.259968506,0.000793917,0.206098289,0.381920891,0.045008352,Topic9,"With the development of neural networks based machine learning and their
usage in mission critical applications, voices are rising against the
\textit{black box} aspect of neural networks as it becomes crucial to
understand their limits and capabilities. With the rise of neuromorphic
hardware, it is even more critical to understand how a neural network, as a
distributed system, tolerates the failures of its computing nodes, neurons, and
its communication channels, synapses. Experimentally assessing the robustness
of neural networks involves the quixotic venture of testing all the possible
failures, on all the possible inputs, which ultimately hits a combinatorial
explosion for the first, and the impossibility to gather all the possible
inputs for the second.
  In this paper, we prove an upper bound on the expected error of the output
when a subset of neurons crashes. This bound involves dependencies on the
network parameters that can be seen as being too pessimistic in the average
case. It involves a polynomial dependency on the Lipschitz coefficient of the
neurons activation function, and an exponential dependency on the depth of the
layer where a failure occurs. We back up our theoretical results with
experiments illustrating the extent to which our prediction matches the
dependencies between the network parameters and robustness. Our results show
that the robustness of neural networks to the average crash can be estimated
without the need to neither test the network on all failure configurations, nor
access the training set used to train the network, both of which are
practically impossible requirements.",On The Robustness of a Neural Network,2017
384,0.001333707,0.16922839,0.00133346,0.00133347,0.00133377,0.248373546,0.001333661,0.001333608,0.573062721,0.001333667,Topic9,"In this paper we introduce ZhuSuan, a python probabilistic programming
library for Bayesian deep learning, which conjoins the complimentary advantages
of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike
existing deep learning libraries, which are mainly designed for deterministic
neural networks and supervised tasks, ZhuSuan is featured for its deep root
into Bayesian inference, thus supporting various kinds of probabilistic models,
including both the traditional hierarchical Bayesian models and recent deep
generative models. We use running examples to illustrate the probabilistic
programming on ZhuSuan, including Bayesian logistic regression, variational
auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural
networks.",ZhuSuan: A Library for Bayesian Deep Learning,2017
385,0.000787677,0.000787535,0.000787603,0.000787608,0.000787596,0.505009876,0.352182176,0.137294723,0.000787639,0.000787567,Topic6,"The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. Among the few proposed
approaches, the recently introduced Black-DROPS algorithm exploits a black-box
optimization algorithm to achieve both high data-efficiency and good
computation times when several cores are used; nevertheless, like all
model-based policy search approaches, Black-DROPS does not scale to high
dimensional state/action spaces. In this paper, we introduce a new model
learning procedure in Black-DROPS that leverages parameterized black-box priors
to (1) scale up to high-dimensional systems, and (2) be robust to large
inaccuracies of the prior information. We demonstrate the effectiveness of our
approach with the ""pendubot"" swing-up task in simulation and with a physical
hexapod robot (48D state space, 18D action space) that has to walk forward as
fast as possible. The results show that our new algorithm is more
data-efficient than previous model-based policy search algorithms (with and
without priors) and that it can allow a physical 6-legged robot to learn new
gaits in only 16 to 30 seconds of interaction time.","Using Parameterized Black-Box Priors to Scale Up Model-Based Policy
  Search for Robotics",2017
386,0.001250504,0.001250375,0.001250194,0.362435376,0.001250413,0.627561323,0.001250482,0.001250408,0.001250551,0.001250374,Topic6,"One of the most interesting features of Bayesian optimization for direct
policy search is that it can leverage priors (e.g., from simulation or from
previous tasks) to accelerate learning on a robot. In this paper, we are
interested in situations for which several priors exist but we do not know in
advance which one fits best the current situation. We tackle this problem by
introducing a novel acquisition function, called Most Likely Expected
Improvement (MLEI), that combines the likelihood of the priors and the expected
improvement. We evaluate this new acquisition function on a transfer learning
task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has
to learn to walk on flat ground and on stairs, with priors corresponding to
different stairs and different kinds of damages. Our results show that MLEI
effectively identifies and exploits the priors, even when there is no obvious
match between the current situations and the priors.","Bayesian Optimization with Automatic Prior Selection for Data-Efficient
  Direct Policy Search",2017
387,0.001031254,0.001031237,0.001031135,0.001031205,0.001031151,0.281668739,0.18836988,0.001031051,0.522743277,0.00103107,Topic9,"In this paper, we study the representational power of deep neural networks
(DNN) that belong to the family of piecewise-linear (PWL) functions, based on
PWL activation units such as rectifier or maxout. We investigate the complexity
of such networks by studying the number of linear regions of the PWL function.
Typically, a PWL function from a DNN can be seen as a large family of linear
functions acting on millions of such regions. We directly build upon the work
of Montufar et al. (2014), Montufar (2017) and Raghu et al. (2017) by refining
the upper and lower bounds on the number of linear regions for rectified and
maxout networks. In addition to achieving tighter bounds, we also develop a
novel method to perform exact enumeration or counting of the number of linear
regions with a mixed-integer linear formulation that maps the input space to
output. We use this new capability to visualize how the number of linear
regions change while training DNNs.",Bounding and Counting Linear Regions of Deep Neural Networks,2017
388,0.051836106,0.001234864,0.001234961,0.001234845,0.001234848,0.112845808,0.081554487,0.068169874,0.679419493,0.001234715,Topic9,"Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.",Deep Rewiring: Training very sparse deep networks,2017
389,0.110144102,0.001250284,0.001250416,0.001250257,0.001250339,0.00125034,0.073519014,0.074925328,0.207063272,0.528096649,Topic10,"To compare entities of differing types and structural components, the
artificial neural network paradigm was used to cross-compare structural
components between heterogeneous documents. Trainable weighted structural
components were input into machine-learned activation functions of the neurons.
The model was used for matching news articles and videos, where the inputs and
activation functions respectively consisted of term vectors and cosine
similarity measures between the weighted structural components. The model was
tested with different weights, achieving as high as 59.2% accuracy for matching
videos to news articles. A mobile application user interface for recommending
related videos for news articles was developed to demonstrate consumer value,
including its potential usefulness for cross-selling products from unrelated
categories.","Comparing heterogeneous entities using artificial neural networks of
  trainable weighted structural components and machine-learned activation
  functions",2018
390,0.000574849,0.000574821,0.099400077,0.049716175,0.124545076,0.371697579,0.00057486,0.226331189,0.126010553,0.000574821,Topic6,"We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive
Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal
exploration mechanism which allows active learning of inverse models in
high-dimensional redundant robots. This allows a robot to efficiently and
actively learn distributions of parameterized motor skills/policies that solve
a corresponding distribution of parameterized tasks/goals. The architecture
makes the robot sample actively novel parameterized tasks in the task space,
based on a measure of competence progress, each of which triggers low-level
goal-directed learning of the motor policy pa- rameters that allow to solve it.
For both learning and generalization, the system leverages regression
techniques which allow to infer the motor policy parameters corresponding to a
given novel parameterized task, and based on the previously learnt
correspondences between policy and task parameters. We present experiments with
high-dimensional continuous sensorimotor spaces in three different robotic
setups: 1) learning the inverse kinematics in a highly-redundant robotic arm,
2) learning omnidirectional locomotion with motor primitives in a quadruped
robot, 3) an arm learning to control a fishing rod with a flexible wire. We
show that 1) exploration in the task space can be a lot faster than exploration
in the actuator space for learning inverse models in redundant robots; 2)
selecting goals maximizing competence progress creates developmental
trajectories driving the robot to progressively focus on tasks of increasing
complexity and is statistically significantly more efficient than selecting
tasks randomly, as well as more efficient than different standard active motor
babbling methods; 3) this architecture allows the robot to actively discover
which parts of its task space it can learn to reach and which part it cannot.","Active Learning of Inverse Models with Intrinsically Motivated Goal
  Exploration in Robots",2013
391,0.143695051,0.000746492,0.000746416,0.278964322,0.137853729,0.026443907,0.000746432,0.000746438,0.409310779,0.000746435,Topic9,"In this work we present a novel end-to-end framework for tracking and
classifying a robot's surroundings in complex, dynamic and only partially
observable real-world environments. The approach deploys a recurrent neural
network to filter an input stream of raw laser measurements in order to
directly infer object locations, along with their identity in both visible and
occluded areas. To achieve this we first train the network using unsupervised
Deep Tracking, a recently proposed theoretical framework for end-to-end space
occupancy prediction. We show that by learning to track on a large amount of
unsupervised data, the network creates a rich internal representation of its
environment which we in turn exploit through the principle of inductive
transfer of knowledge to perform the task of it's semantic classification. As a
result, we show that only a small amount of labelled data suffices to steer the
network towards mastering this additional task. Furthermore we propose a novel
recurrent neural network architecture specifically tailored to tracking and
semantic classification in real-world robotics applications. We demonstrate the
tracking and classification performance of the method on real-world data
collected at a busy road junction. Our evaluation shows that the proposed
end-to-end framework compares favourably to a state-of-the-art, model-free
tracking solution and that it outperforms a conventional one-shot training
scheme for semantic classification.","End-to-End Tracking and Semantic Segmentation Using Recurrent Neural
  Networks",2016
392,0.202409535,0.000901075,0.000901094,0.404370874,0.000901181,0.00090111,0.000901049,0.102773818,0.285039142,0.000901123,Topic4,"This paper presents to the best of our knowledge the first end-to-end object
tracking approach which directly maps from raw sensor input to object tracks in
sensor space without requiring any feature engineering or system identification
in the form of plant or sensor models. Specifically, our system accepts a
stream of raw sensor data at one end and, in real-time, produces an estimate of
the entire environment state at the output including even occluded objects. We
achieve this by framing the problem as a deep learning task and exploit
sequence models in the form of recurrent neural networks to learn a mapping
from sensor measurements to object tracks. In particular, we propose a learning
method based on a form of input dropout which allows learning in an
unsupervised manner, only based on raw, occluded sensor data without access to
ground-truth annotations. We demonstrate our approach using a synthetic dataset
designed to mimic the task of tracking objects in 2D laser data -- as commonly
encountered in robotics applications -- and show that it learns to track many
dynamic objects despite occlusions and the presence of sensor noise.",Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks,2016
393,0.27759593,0.000752176,0.000752002,0.091972755,0.256708977,0.00075208,0.000752032,0.000752118,0.369209898,0.000752032,Topic9,"While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning - leveraging
unlabeled examples to learn about the structure of a domain - remains a
difficult unsolved challenge. Here, we explore prediction of future frames in a
video sequence as an unsupervised learning rule for learning about the
structure of the visual world. We describe a predictive neural network
(""PredNet"") architecture that is inspired by the concept of ""predictive coding""
from the neuroscience literature. These networks learn to predict future frames
in a video sequence, with each layer in the network making local predictions
and only forwarding deviations from those predictions to subsequent network
layers. We show that these networks are able to robustly learn to predict the
movement of synthetic (rendered) objects, and that in doing so, the networks
learn internal representations that are useful for decoding latent object
parameters (e.g. pose) that support object recognition with fewer training
views. We also show that these networks can scale to complex natural image
streams (car-mounted camera videos), capturing key aspects of both egocentric
movement and the movement of objects in the visual scene, and the
representation learned in this setting is useful for estimating the steering
angle. Altogether, these results suggest that prediction represents a powerful
framework for unsupervised learning, allowing for implicit learning of object
and scene structure.","Deep Predictive Coding Networks for Video Prediction and Unsupervised
  Learning",2016
394,0.001075558,0.001075452,0.109897015,0.260856474,0.001075542,0.001075474,0.001075515,0.001075438,0.621718066,0.001075466,Topic9,"This paper proposes a computationally efficient approach to detecting objects
natively in 3D point clouds using convolutional neural networks (CNNs). In
particular, this is achieved by leveraging a feature-centric voting scheme to
implement novel convolutional layers which explicitly exploit the sparsity
encountered in the input. To this end, we examine the trade-off between
accuracy and speed for different architectures and additionally propose to use
an L1 penalty on the filter activations to further encourage sparsity in the
intermediate representations. To the best of our knowledge, this is the first
work to propose sparse convolutional layers and L1 regularisation for efficient
large-scale processing of 3D data. We demonstrate the efficacy of our approach
on the KITTI object detection benchmark and show that Vote3Deep models with as
few as three layers outperform the previous state of the art in both laser and
laser-vision based approaches by margins of up to 40% while remaining highly
competitive in terms of processing time.","Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient
  Convolutional Neural Networks",2016
395,0.261774313,0.001250263,0.001250298,0.001250311,0.001250296,0.594414666,0.001250373,0.001250183,0.135059143,0.001250153,Topic6,"We propose studying GAN training dynamics as regret minimization, which is in
contrast to the popular view that there is consistent minimization of a
divergence between real and generated distributions. We analyze the convergence
of GAN training from this new point of view to understand why mode collapse
happens. We hypothesize the existence of undesirable local equilibria in this
non-convex game to be responsible for mode collapse. We observe that these
local equilibria often exhibit sharp gradients of the discriminator function
around some real data points. We demonstrate that these degenerate local
equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show
that DRAGAN enables faster training, achieves improved stability with fewer
mode collapses, and leads to generator networks with better modeling
performance across a variety of architectures and objective functions.",On Convergence and Stability of GANs,2017
396,0.151543766,0.000781455,0.000781355,0.000781415,0.537979593,0.3050065,0.00078138,0.000781443,0.000781431,0.000781662,Topic5,"Imitation learning is an effective approach for autonomous systems to acquire
control policies when an explicit reward function is unavailable, using
supervision provided as demonstrations from an expert, typically a human
operator. However, standard imitation learning methods assume that the agent
receives examples of observation-action tuples that could be provided, for
instance, to a supervised learning algorithm. This stands in contrast to how
humans and animals imitate: we observe another person performing some behavior
and then figure out which actions will realize that behavior, compensating for
changes in viewpoint, surroundings, and embodiment. We term this kind of
imitation learning as imitation-from-observation and propose an imitation
learning method based on video prediction with context translation and deep
reinforcement learning. This lifts the assumption in imitation learning that
the demonstration should consist of observations and actions in the same
environment, and enables a variety of interesting applications, including
learning robotic skills that involve tool use simply by observing videos of
human tool use. Our experimental results show that our approach can perform
imitation-from-observation for a variety of real-world robotic tasks modeled on
common household chores, acquiring skills such as sweeping from videos of a
human demonstrator. Videos can be found at
https://sites.google.com/site/imitationfromobservation","Imitation from Observation: Learning to Imitate Behaviors from Raw Video
  via Context Translation",2017
397,0.000971176,0.000971122,0.000971169,0.000971138,0.036251526,0.532437708,0.000971196,0.000971119,0.424512706,0.00097114,Topic6,"Unsupervised pretraining and dropout have been well studied, especially with
respect to regularization and output consistency. However, our understanding
about the explicit convergence rates of the parameter estimates, and their
dependence on the learning (like denoising and dropout rate) and structural
(like depth and layer lengths) aspects of the network is less mature. An
interesting question in this context is to ask if the network structure could
""guide"" the choices of such learning parameters. In this work, we explore these
gaps between network structure, the learning mechanisms and their interaction
with parameter convergence rates. We present a way to address these issues
based on the backpropagation convergence rates for general nonconvex objectives
using first-order information. We then incorporate two learning mechanisms into
this general framework -- denoising autoencoder and dropout, and subsequently
derive the convergence rates of deep networks. Building upon these bounds, we
provide insights into the choices of learning parameters and network sizes that
achieve certain levels of convergence accuracy. The results derived here
support existing empirical observations, and we also conduct a set of
experiments to evaluate them.","Convergence rates for pretraining and dropout: Guiding learning
  parameters using network structure",2015
398,0.000901158,0.000901224,0.000901068,0.000901078,0.000901022,0.000901157,0.042258117,0.000901002,0.95053314,0.000901034,Topic9,"Deep Convolutional Neural Networks (CNN) enforces supervised information only
at the output layer, and hidden layers are trained by back propagating the
prediction error from the output layer without explicit supervision. We propose
a supervised feature learning approach, Label Consistent Neural Network, which
enforces direct supervision in late hidden layers. We associate each neuron in
a hidden layer with a particular class label and encourage it to be activated
for input signals from the same class. More specifically, we introduce a label
consistency regularization called ""discriminative representation error"" loss
for late hidden layers and combine it with classification error loss to build
our overall objective function. This label consistency constraint alleviates
the common problem of gradient vanishing and tends to faster convergence; it
also makes the features derived from late hidden layers discriminative enough
for classification even using a simple $k$-NN classifier, since input signals
from the same class will have very similar representations. Experimental
results demonstrate that our approach achieves state-of-the-art performances on
several public benchmarks for action and object category recognition.",Learning Discriminative Features via Label Consistent Neural Network,2016
399,0.280978454,0.001075619,0.522318743,0.001075585,0.01861966,0.001075605,0.107098183,0.065606953,0.001075533,0.001075664,Topic3,"This paper proposes an out-of-sample extension framework for a global
manifold learning algorithm (Isomap) that uses temporal information in
out-of-sample points in order to make the embedding more robust to noise and
artifacts. Given a set of noise-free training data and its embedding, the
proposed framework extends the embedding for a noisy time series. This is
achieved by adding a spatio-temporal compactness term to the optimization
objective of the embedding. To the best of our knowledge, this is the first
method for out-of-sample extension of manifold embeddings that leverages timing
information available for the extension set. Experimental results demonstrate
that our out-of-sample extension algorithm renders a more robust and accurate
embedding of sequentially ordered image data in the presence of various noise
and artifacts when compared to other timing-aware embeddings. Additionally, we
show that an out-of-sample extension framework based on the proposed algorithm
outperforms the state of the art in eye-gaze estimation.","Out-of-Sample Extension for Dimensionality Reduction of Noisy Time
  Series",2016
400,0.594220437,0.077707248,0.001923367,0.001923421,0.00192366,0.001923716,0.001923685,0.001923477,0.314607356,0.001923633,Topic1,"Machine learning methods in general and Deep Neural Networks in particular
have shown to be vulnerable to adversarial perturbations. So far this
phenomenon has mainly been studied in the context of whole-image
classification. In this contribution, we analyse how adversarial perturbations
can affect the task of semantic segmentation. We show how existing adversarial
attackers can be transferred to this task and that it is possible to create
imperceptible adversarial perturbations that lead a deep network to misclassify
almost all pixels of a chosen class while leaving network prediction nearly
unchanged outside this class.",Adversarial Examples for Semantic Image Segmentation,2017
401,0.588584628,0.166376323,0.000565054,0.000565105,0.128735966,0.000565153,0.112912435,0.000565105,0.000565119,0.000565112,Topic1,"Many machine learning algorithms are vulnerable to almost imperceptible
perturbations of their inputs. So far it was unclear how much risk adversarial
perturbations carry for the safety of real-world machine learning applications
because most methods used to generate such perturbations rely either on
detailed model information (gradient-based attacks) or on confidence scores
such as class probabilities (score-based attacks), neither of which are
available in most real-world scenarios. In many such cases one currently needs
to retreat to transfer-based attacks which rely on cumbersome substitute
models, need access to the training data and can be defended against. Here we
emphasise the importance of attacks which solely rely on the final model
decision. Such decision-based attacks are (1) applicable to real-world
black-box models such as autonomous cars, (2) need less knowledge and are
easier to apply than transfer-based attacks and (3) are more robust to simple
defences than gradient- or score-based attacks. Previous attacks in this
category were limited to simple models or simple datasets. Here we introduce
the Boundary Attack, a decision-based attack that starts from a large
adversarial perturbation and then seeks to reduce the perturbation while
staying adversarial. The attack is conceptually simple, requires close to no
hyperparameter tuning, does not rely on substitute models and is competitive
with the best gradient-based attacks in standard computer vision tasks like
ImageNet. We apply the attack on two black-box algorithms from Clarifai.com.
The Boundary Attack in particular and the class of decision-based attacks in
general open new avenues to study the robustness of machine learning models and
raise new questions regarding the safety of deployed machine learning systems.
An implementation of the attack is available as part of Foolbox at
https://github.com/bethgelab/foolbox .","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box
  Machine Learning Models",2017
402,0.106952851,0.149586467,0.000893123,0.000892999,0.000893131,0.075097729,0.000893104,0.393311396,0.270586155,0.000893045,Topic8,"Effective and efficient mitigation of malware is a long-time endeavor in the
information security community. The development of an anti-malware system that
can counteract an unknown malware is a prolific activity that may benefit
several sectors. We envision an intelligent anti-malware system that utilizes
the power of deep learning (DL) models. Using such models would enable the
detection of newly-released malware through mathematical generalization. That
is, finding the relationship between a given malware $x$ and its corresponding
malware family $y$, $f: x \mapsto y$. To accomplish this feat, we used the
Malimg dataset (Nataraj et al., 2011) which consists of malware images that
were processed from malware binaries, and then we trained the following DL
models 1 to classify each malware family: CNN-SVM (Tang, 2013), GRU-SVM
(Agarap, 2017), and MLP-SVM. Empirical evidence has shown that the GRU-SVM
stands out among the DL models with a predictive accuracy of ~84.92%. This
stands to reason for the mentioned model had the relatively most sophisticated
architecture design among the presented models. The exploration of an even more
optimal DL-SVM model is the next stage towards the engineering of an
intelligent anti-malware system.","Towards Building an Intelligent Anti-Malware System: A Deep Learning
  Approach using Support Vector Machine (SVM) for Malware Classification",2017
403,0.000735498,0.000735455,0.154620894,0.000735431,0.000735482,0.000735528,0.026185791,0.000735507,0.071837684,0.742942732,Topic10,"Feature extraction has gained increasing attention in the field of machine
learning, as in order to detect patterns, extract information, or predict
future observations from big data, the urge of informative features is crucial.
The process of extracting features is highly linked to dimensionality reduction
as it implies the transformation of the data from a sparse high-dimensional
space, to higher level meaningful abstractions. This dissertation employs
Neural Networks for distributed paragraph representations, and Latent Dirichlet
Allocation to capture higher level features of paragraph vectors. Although
Neural Networks for distributed paragraph representations are considered the
state of the art for extracting paragraph vectors, we show that a quick topic
analysis model such as Latent Dirichlet Allocation can provide meaningful
features too. We evaluate the two methods on the CMU Movie Summary Corpus, a
collection of 25,203 movie plot summaries extracted from Wikipedia. Finally,
for both approaches, we use K-Nearest Neighbors to discover similar movies, and
plot the projected representations using T-Distributed Stochastic Neighbor
Embedding to depict the context similarities. These similarities, expressed as
movie distances, can be used for movies recommendation. The recommended movies
of this approach are compared with the recommended movies from IMDB, which use
a collaborative filtering recommendation approach, to show that our two models
could constitute either an alternative or a supplementary recommendation
approach.","Feature extraction using Latent Dirichlet Allocation and Neural
  Networks: A case study on movie synopses",2016
404,0.001298968,0.00129901,0.001298887,0.001298887,0.75054444,0.001298963,0.001299057,0.001299017,0.001298997,0.239063775,Topic5,"During the past decade, several areas of speech and language understanding
have witnessed substantial breakthroughs from the use of data-driven models. In
the area of dialogue systems, the trend is less obvious, and most practical
systems are still built through significant engineering and expert knowledge.
Nevertheless, several recent results suggest that data-driven approaches are
feasible and quite promising. To facilitate research in this area, we have
carried out a wide survey of publicly available datasets suitable for
data-driven learning of dialogue systems. We discuss important characteristics
of these datasets, how they can be used to learn diverse dialogue strategies,
and their other potential uses. We also examine methods for transfer learning
between datasets and the use of external knowledge. Finally, we discuss
appropriate choice of evaluation metrics for the learning objective.",A Survey of Available Corpora for Building Data-Driven Dialogue Systems,2015
405,0.001064185,0.001064009,0.208729765,0.001064134,0.001064031,0.091170233,0.00106404,0.001064019,0.001064109,0.692651476,Topic10,"Word embedding maps words into a low-dimensional continuous embedding space
by exploiting the local word collocation patterns in a small context window. On
the other hand, topic modeling maps documents onto a low-dimensional topic
space, by utilizing the global word collocation patterns in the same document.
These two types of patterns are complementary. In this paper, we propose a
generative topic embedding model to combine the two types of patterns. In our
model, topics are represented by embedding vectors, and are shared across
documents. The probability of each word is influenced by both its local context
and its topic. A variational inference method yields the topic embeddings as
well as the topic mixing proportions for each document. Jointly they represent
the document in a low-dimensional continuous space. In two document
classification tasks, our method performs better than eight existing methods,
with fewer features. In addition, we illustrate with an example that our method
can generate coherent topics even based on only one document.","Generative Topic Embedding: a Continuous Representation of Documents
  (Extended Version with Proofs)",2016
406,0.001852499,0.001852487,0.001852505,0.001852132,0.001852321,0.074881064,0.001852316,0.001852225,0.001852401,0.91030005,Topic10,"As entity type systems become richer and more fine-grained, we expect the
number of types assigned to a given entity to increase. However, most
fine-grained typing work has focused on datasets that exhibit a low degree of
type multiplicity. In this paper, we consider the high-multiplicity regime
inherent in data sources such as Wikipedia that have semi-open type systems. We
introduce a set-prediction approach to this problem and show that our model
outperforms unstructured baselines on a new Wikipedia-based fine-grained typing
corpus.",Fine-Grained Entity Typing with High-Multiplicity Assignments,2017
407,0.000826711,0.180936164,0.000826561,0.000826621,0.749211392,0.000826621,0.000826639,0.015157722,0.000826741,0.049734828,Topic5,"As language and visual understanding by machines progresses rapidly, we are
observing an increasing interest in holistic architectures that tightly
interlink both modalities in a joint learning and inference process. This trend
has allowed the community to progress towards more challenging and open tasks
and refueled the hope at achieving the old AI dream of building machines that
could pass a turing test in open domains. In order to steadily make progress
towards this goal, we realize that quantifying performance becomes increasingly
difficult. Therefore we ask how we can precisely define such challenges and how
we can evaluate different algorithms on this open tasks? In this paper, we
summarize and discuss such challenges as well as try to give answers where
appropriate options are available in the literature. We exemplify some of the
solutions on a recently presented dataset of question-answering task based on
real-world indoor images that establishes a visual turing challenge. Finally,
we argue despite the success of unique ground-truth annotation, we likely have
to step away from carefully curated dataset and rather rely on 'social
consensus' as the main driving force to create suitable benchmarks. Providing
coverage in this inherently ambiguous output space is an emerging challenge
that we face in order to make quantifiable progress in this area.",Towards a Visual Turing Challenge,2014
408,0.00097109,0.06921614,0.000970999,0.000971059,0.446491803,0.000971163,0.000971082,0.41911366,0.000971105,0.059351899,Topic5,"A growing field in robotics and Artificial Intelligence (AI) research is
human-robot collaboration, whose target is to enable effective teamwork between
humans and robots. However, in many situations human teams are still superior
to human-robot teams, primarily because human teams can easily agree on a
common goal with language, and the individual members observe each other
effectively, leveraging their shared motor repertoire and sensorimotor
resources. This paper shows that for cognitive robots it is possible, and
indeed fruitful, to combine knowledge acquired from interacting with elements
of the environment (affordance exploration) with the probabilistic observation
of another agent's actions.
  We propose a model that unites (i) learning robot affordances and word
descriptions with (ii) statistical recognition of human gestures with vision
sensors. We discuss theoretical motivations, possible implementations, and we
show initial results which highlight that, after having acquired knowledge of
its surrounding environment, a humanoid robot can generalize this knowledge to
the case when it observes another agent (human partner) performing the same
motor actions previously executed during training.","Interactive Robot Learning of Gestures, Language and Affordances",2017
409,0.298311132,0.000917602,0.000917587,0.000917728,0.000917706,0.000917653,0.000917634,0.142151523,0.000917637,0.553113798,Topic10,"Automatic transcriptions of consumer-generated multi-media content such as
""Youtube"" videos still exhibit high word error rates. Such data typically
occupies a very broad domain, has been recorded in challenging conditions, with
cheap hardware and a focus on the visual modality, and may have been
post-processed or edited. In this paper, we extend our earlier work on adapting
the acoustic model of a DNN-based speech recognition system to an RNN language
model and show how both can be adapted to the objects and scenes that can be
automatically detected in the video. We are working on a corpus of ""how-to""
videos from the web, and the idea is that an object that can be seen (""car""),
or a scene that is being detected (""kitchen"") can be used to condition both
models on the ""context"" of the recording, thereby reducing perplexity and
improving transcription. We achieve good improvements in both cases and compare
and analyze the respective reductions in word error rate. We expect that our
results can be used for any type of speech processing in which ""context""
information is available, for example in robotics, man-machine interaction, or
when indexing large audio-visual archives, and should ultimately help to bring
together the ""video-to-text"" and ""speech-to-text"" communities.",Visual Features for Context-Aware Speech Recognition,2017
410,0.167070681,0.00166707,0.001667205,0.001667134,0.44818712,0.001667123,0.040150546,0.001667118,0.001667179,0.334588823,Topic5,"In this work we propose a blackbox intervention method for visual dialog
models, with the aim of assessing the contribution of individual linguistic or
visual components. Concretely, we conduct structured or randomized
interventions that aim to impair an individual component of the model, and
observe changes in task performance. We reproduce a state-of-the-art visual
dialog model and demonstrate that our methodology yields surprising insights,
namely that both dialog and image information have minimal contributions to
task performance. The intervention method presented here can be applied as a
sanity check for the strength and robustness of each component in visual dialog
systems.",Examining Cooperation in Visual Dialog Models,2017
411,0.261651549,0.001613074,0.001613211,0.001613538,0.238722107,0.02340808,0.057882634,0.001613407,0.001613469,0.410268932,Topic10,"Sports channel video portals offer an exciting domain for research on
multimodal, multilingual analysis. We present methods addressing the problem of
automatic video highlight prediction based on joint visual features and textual
analysis of the real-world audience discourse with complex slang, in both
English and traditional Chinese. We present a novel dataset based on League of
Legends championships recorded from North American and Taiwanese Twitch.tv
channels (will be released for further research), and demonstrate strong
results on these using multimodal, character-level CNN-RNN model architectures.",Video Highlight Prediction Using Audience Chat Reactions,2017
412,0.195121959,0.000819913,0.0008199,0.067084669,0.000819913,0.000819934,0.000819887,0.00081996,0.426390639,0.306483226,Topic9,"Modern automatic speech recognition (ASR) systems need to be robust under
acoustic variability arising from environmental, speaker, channel, and
recording conditions. Ensuring such robustness to variability is a challenge in
modern day neural network-based ASR systems, especially when all types of
variability are not seen during training. We attempt to address this problem by
encouraging the neural network acoustic model to learn invariant feature
representations. We use ideas from recent research on image generation using
Generative Adversarial Networks and domain adaptation ideas extending
adversarial gradient-based training. A recent work from Ganin et al. proposes
to use adversarial training for image domain adaptation by using an
intermediate representation from the main target classification network to
deteriorate the domain classifier performance through a separate neural
network. Our work focuses on investigating neural architectures which produce
representations invariant to noise conditions for ASR. We evaluate the proposed
architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We
show that our method generalizes better than the standard multi-condition
training especially when only a few noise categories are seen during training.",Invariant Representations for Noisy Speech Recognition,2016
413,0.001205187,0.00120506,0.001205058,0.001205141,0.421759968,0.001205134,0.001205104,0.390963021,0.001205114,0.178841214,Topic5,"This paper presents a self-supervised method for detecting the active speaker
in a multi-person spoken interaction scenario. We argue that this capability is
a fundamental prerequisite for any artificial cognitive system attempting to
acquire language in social settings. Our methods are able to detect an
arbitrary number of possibly overlapping active speakers based exclusively on
visual information about their face. Our methods do not rely on external
annotations, thus complying with cognitive development. Instead, they use
information from the auditory modality to support learning in the visual
domain. The methods have been extensively evaluated on a large multi-person
face-to-face interaction dataset. The results reach an accuracy of 80% on a
multi-speaker setting. We believe this system represents an essential component
of any artificial cognitive system or robotic platform engaging in social
interaction.","Self-Supervised Vision-Based Detection of the Active Speaker as a
  Prerequisite for Socially-Aware Language Acquisition",2017
414,0.001538789,0.150865395,0.001538793,0.001538729,0.774807965,0.0015388,0.001538944,0.001538834,0.001538841,0.063554908,Topic5,"In this paper, we describe a solution to tackle a common set of challenges in
e-commerce, which arise from the fact that new products are continually being
added to the catalogue. The challenges involve properly personalising the
customer experience, forecasting demand and planning the product range. We
argue that the foundational piece to solve all of these problems is having
consistent and detailed information about each product, information that is
rarely available or consistent given the multitude of suppliers and types of
products. We describe in detail the architecture and methodology implemented at
ASOS, one of the world's largest fashion e-commerce retailers, to tackle this
problem. We then show how this quantitative understanding of the products can
be leveraged to improve recommendations in a hybrid recommender system
approach.","Product Characterisation towards Personalisation: Learning Attributes
  from Unstructured Data to Recommend Fashion Products",2018
415,0.000826594,0.183455501,0.000826591,0.000826683,0.237830433,0.000826675,0.000826597,0.134969957,0.00082661,0.438784359,Topic10,"The speech code is a vehicle of language: it defines a set of forms used by a
community to carry information. Such a code is necessary to support the
linguistic interactions that allow humans to communicate. How then may a speech
code be formed prior to the existence of linguistic interactions? Moreover, the
human speech code is discrete and compositional, shared by all the individuals
of a community but different across communities, and phoneme inventories are
characterized by statistical regularities. How can a speech code with these
properties form? We try to approach these questions in the paper, using the
""methodology of the artificial"". We build a society of artificial agents, and
detail a mechanism that shows the formation of a discrete speech code without
pre-supposing the existence of linguistic capacities or of coordinated
interactions. The mechanism is based on a low-level model of sensory-motor
interactions. We show that the integration of certain very simple and non
language-specific neural devices leads to the formation of a speech code that
has properties similar to the human speech code. This result relies on the
self-organizing properties of a generic coupling between perception and
production within agents, and on the interactions between agents. The
artificial system helps us to develop better intuitions on how speech might
have appeared, by showing how self-organization might have helped natural
selection to find speech.",The Self-Organization of Speech Sounds,2005
416,0.003847216,0.00384718,0.003846708,0.003846928,0.003847182,0.137921354,0.343275812,0.003846938,0.003846856,0.491873827,Topic10,"The F-measure or F-score is one of the most commonly used single number
measures in Information Retrieval, Natural Language Processing and Machine
Learning, but it is based on a mistake, and the flawed assumptions render it
unsuitable for use in most contexts! Fortunately, there are better
alternatives.","What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes",2015
417,0.075706423,0.001123951,0.001123759,0.001123857,0.316806203,0.001123944,0.068451803,0.001123855,0.414205736,0.119210468,Topic9,"PAQ8 is an open source lossless data compression algorithm that currently
achieves the best compression rates on many benchmarks. This report presents a
detailed description of PAQ8 from a statistical machine learning perspective.
It shows that it is possible to understand some of the modules of PAQ8 and use
this understanding to improve the method. However, intuitive statistical
explanations of the behavior of other modules remain elusive. We hope the
description in this report will be a starting point for discussions that will
increase our understanding, lead to improvements to PAQ8, and facilitate a
transfer of knowledge from PAQ8 to other machine learning methods, such a
recurrent neural networks and stochastic memoizers. Finally, the report
presents a broad range of new applications of PAQ to machine learning tasks
including language modeling and adaptive text prediction, adaptive game
playing, classification, and compression using features from the field of deep
learning.",A Machine Learning Perspective on Predictive Coding with PAQ,2011
418,0.000645307,0.000645345,0.000645375,0.000645289,0.000645278,0.192549455,0.802288067,0.000645255,0.000645337,0.000645292,Topic7,"Recently, there has been a renewed interest in the machine learning community
for variants of a sparse greedy approximation procedure for concave
optimization known as {the Frank-Wolfe (FW) method}. In particular, this
procedure has been successfully applied to train large-scale instances of
non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has
allowed to obtain efficient algorithms but also important theoretical results,
including convergence analysis of training algorithms and new characterizations
of model sparsity.
  In this paper, we present and analyze a novel variant of the FW method based
on a new way to perform away steps, a classic strategy used to accelerate the
convergence of the basic FW procedure. Our formulation and analysis is focused
on a general concave maximization problem on the simplex. However, the
specialization of our algorithm to quadratic forms is strongly related to some
classic methods in computational geometry, namely the Gilbert and MDM
algorithms.
  On the theoretical side, we demonstrate that the method matches the
guarantees in terms of convergence rate and number of iterations obtained by
using classic away steps. In particular, the method enjoys a linear rate of
convergence, a result that has been recently proved for MDM on quadratic forms.
  On the practical side, we provide experiments on several classification
datasets, and evaluate the results using statistical tests. Experiments show
that our method is faster than the FW method with classic away steps, and works
well even in the cases in which classic away steps slow down the algorithm.
Furthermore, these improvements are obtained without sacrificing the predictive
accuracy of the obtained SVM model.","A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale
  SVM Training",2013
419,0.557560675,0.039041681,0.032498547,0.000893027,0.000893166,0.000893128,0.000893139,0.000893035,0.167365333,0.199068269,Topic1,"Despite significant progress in object categorization, in recent years, a
number of important challenges remain, mainly, ability to learn from limited
labeled data and ability to recognize object classes within large, potentially
open, set of labels. Zero-shot learning is one way of addressing these
challenges, but it has only been shown to work with limited sized class
vocabularies and typically requires separation between supervised and
unsupervised classes, allowing former to inform the latter but not vice versa.
We propose the notion of semi-supervised vocabulary-informed learning to
alleviate the above mentioned challenges and address problems of supervised,
zero-shot and open set recognition using a unified framework. Specifically, we
propose a maximum margin framework for semantic manifold-based recognition that
incorporates distance constraints from (both supervised and unsupervised)
vocabulary atoms, ensuring that labeled samples are projected closest to their
correct prototypes, in the embedding space, than to others. We show that
resulting model shows improvements in supervised, zero-shot, and large open set
recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.",Semi-supervised Vocabulary-informed Learning,2016
420,0.156271633,0.001041935,0.001041883,0.001041909,0.001042009,0.266917424,0.434196456,0.001041855,0.001041989,0.136362907,Topic7,"To cope with the high level of ambiguity faced in domains such as Computer
Vision or Natural Language processing, robust prediction methods often search
for a diverse set of high-quality candidate solutions or proposals. In
structured prediction problems, this becomes a daunting task, as the solution
space (image labelings, sentence parses, etc.) is exponentially large. We study
greedy algorithms for finding a diverse subset of solutions in
structured-output spaces by drawing new connections between submodular
functions over combinatorial item sets and High-Order Potentials (HOPs) studied
for graphical models. Specifically, we show via examples that when marginal
gains of submodular diversity functions allow structured representations, this
enables efficient (sub-linear time) approximate maximization by reducing the
greedy augmentation step to inference in a factor graph with appropriately
constructed HOPs. We discuss benefits, tradeoffs, and show that our
constructions lead to significantly better proposals.","Submodular meets Structured: Finding Diverse Subsets in
  Exponentially-Large Structured Item Sets",2014
421,0.7101631,0.000613613,0.110900975,0.000613638,0.174640387,0.000613712,0.000613629,0.000613663,0.000613677,0.000613606,Topic1,"Many problems in image processing and computer vision (e.g. colorization,
style transfer) can be posed as 'manipulating' an input image into a
corresponding output image given a user-specified guiding signal. A holy-grail
solution towards generic image manipulation should be able to efficiently alter
an input image with any personalized signals (even signals unseen during
training), such as diverse paintings and arbitrary descriptive attributes.
However, existing methods are either inefficient to simultaneously process
multiple signals (let alone generalize to unseen signals), or unable to handle
signals from other modalities. In this paper, we make the first attempt to
address the zero-shot image manipulation task. We cast this problem as
manipulating an input image according to a parametric model whose key
parameters can be conditionally generated from any guiding signal (even unseen
ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a
fully-differentiable architecture that jointly optimizes an
image-transformation network (TNet) and a parameter network (PNet). The PNet
learns to generate key transformation parameters for the TNet given any guiding
signal while the TNet performs fast zero-shot image manipulation according to
both signal-dependent parameters from the PNet and signal-invariant parameters
from the TNet itself. Extensive experiments show that our ZM-Net can perform
high-quality image manipulation conditioned on different forms of guiding
signals (e.g. style images and attributes) in real-time (tens of milliseconds
per image) even for unseen signals. Moreover, a large-scale style dataset with
over 20,000 style images is also constructed to promote further research.",ZM-Net: Real-time Zero-shot Image Manipulation Network,2017
422,0.737127331,0.000621298,0.000621222,0.000621219,0.000621264,0.198431964,0.04326892,0.000621266,0.017444213,0.000621303,Topic1,"We propose an intuitive generalization to the Generative Adversarial Networks
(GANs) and its conditional variants to address the well known mode collapse
problem. Firstly, we propose a multi-agent GAN architecture incorporating
multiple generators and one discriminator. Secondly, to enforce different
generators to capture diverse high probability modes, we modify discriminator's
objective function where along with finding the real and fake samples, the
discriminator has to identify the generator that generated the fake sample.
Intuitively, to succeed in this task, the discriminator must learn to push
different generators towards different identifiable modes. Our framework
(MAD-GAN) is generalizable in the sense that it can be easily combined with
other existing variants of GANs to produce diverse samples. We perform
extensive experiments on synthetic and real datasets and compare MAD-GAN with
different variants of GAN. We show high quality diverse sample generations for
the challenging tasks such as image-to-image translation (known to learn delta
distribution) and face generation. In addition, we show that MAD-GAN is able to
disentangle different modalities even when trained using highly challenging
multi-view dataset (mixture of forests, icebergs, bedrooms etc). In the end, we
also show its efficacy for the unsupervised feature representation task. In the
appendix we introduce a similarity based competing objective which encourages
the different generators to generate varied samples judged by a user defined
similarity metric. We show extensive evaluations on a 1-D setting of mixture of
gaussians for non parametric density estimation. The theoretical proofs back
the efficacy of the framework and explains why various generators are pushed
towards distinct clusters of modes.",Multi-Agent Diverse Generative Adversarial Networks,2017
423,0.409806078,0.001163083,0.096600514,0.001163178,0.001163042,0.097201065,0.389414047,0.001162952,0.001163077,0.001162964,Topic1,"Generative Adversarial Nets (GANs) represent an important milestone for
effective generative models, which has inspired numerous variants seemingly
different from each other. One of the main contributions of this paper is to
reveal a unified geometric structure in GAN and its variants. Specifically, we
show that the adversarial generative model training can be decomposed into
three geometric steps: separating hyperplane search, discriminator parameter
update away from the separating hyperplane, and the generator update along the
normal vector direction of the separating hyperplane. This geometric intuition
reveals the limitations of the existing approaches and leads us to propose a
new formulation called geometric GAN using SVM separating hyperplane that
maximizes the margin. Our theoretical analysis shows that the geometric GAN
converges to a Nash equilibrium between the discriminator and generator. In
addition, extensive numerical results show that the superior performance of
geometric GAN.",Geometric GAN,2017
424,0.024285051,0.000763519,0.000763498,0.000763483,0.000763547,0.000763527,0.467429951,0.036233927,0.467470002,0.000763495,Topic9,"Training deep networks is expensive and time-consuming with the training
period increasing with data size and growth in model parameters. In this paper,
we provide a framework for distributed training of deep networks over a cluster
of CPUs in Apache Spark. The framework implements both Data Parallelism and
Model Parallelism making it suitable to use for deep networks which require
huge training data and model parameters which are too big to fit into the
memory of a single machine. It can be scaled easily over a cluster of cheap
commodity hardware to attain significant speedup and obtain better results
making it quite economical as compared to farm of GPUs and supercomputers. We
have proposed a new algorithm for training of deep networks for the case when
the network is partitioned across the machines (Model Parallelism) along with
detailed cost analysis and proof of convergence of the same. We have developed
implementations for Fully-Connected Feedforward Networks, Convolutional Neural
Networks, Recurrent Neural Networks and Long Short-Term Memory architectures.
We present the results of extensive simulations demonstrating the speedup and
accuracy obtained by our framework for different sizes of the data and model
parameters with variation in the number of worker cores/partitions; thereby
showing that our proposed framework can achieve significant speedup (upto 11X
for CNN) and is also quite scalable.","A Data and Model-Parallel, Distributed and Scalable Framework for
  Training of Deep Networks in Apache Spark",2017
425,0.228229854,0.001010376,0.001010322,0.001010405,0.001010433,0.001010432,0.001010516,0.001010429,0.76368685,0.001010384,Topic9,"Recently, deep neural networks have demonstrated excellent performances in
recognizing the age and gender on human face images. However, these models were
applied in a black-box manner with no information provided about which facial
features are actually used for prediction and how these features depend on
image preprocessing, model initialization and architecture choice. We present a
study investigating these different effects.
  In detail, our work compares four popular neural network architectures,
studies the effect of pretraining, evaluates the robustness of the considered
alignment preprocessings via cross-method test set swapping and intuitively
visualizes the model's prediction strategies in given preprocessing conditions
using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our
evaluations on the challenging Adience benchmark show that suitable parameter
initialization leads to a holistic perception of the input, compensating
artefactual data representations. With a combination of simple preprocessing
steps, we reach state of the art performance in gender recognition.","Understanding and Comparing Deep Neural Networks for Age and Gender
  Classification",2017
426,0.001234785,0.001235014,0.001234853,0.001235005,0.001234737,0.633395465,0.001234959,0.001234701,0.356725741,0.00123474,Topic6,"We analyze the convergence of (stochastic) gradient descent algorithm for
learning a convolutional filter with Rectified Linear Unit (ReLU) activation
function. Our analysis does not rely on any specific form of the input
distribution and our proofs only use the definition of ReLU, in contrast with
previous works that are restricted to standard Gaussian input. We show that
(stochastic) gradient descent with random initialization can learn the
convolutional filter in polynomial time and the convergence rate depends on the
smoothness of the input distribution and the closeness of patches. To the best
of our knowledge, this is the first recovery guarantee of gradient-based
algorithms for convolutional filter on non-Gaussian input distributions. Our
theory also justifies the two-stage learning rate strategy in deep neural
networks. While our focus is theoretical, we also present experiments that
illustrate our theoretical findings.",When is a Convolutional Filter Easy To Learn?,2017
427,0.001149773,0.001149738,0.274641265,0.214529878,0.001149707,0.001149759,0.357672352,0.001149575,0.146258288,0.001149665,Topic7,"Sparsity inducing regularization is an important part for learning
over-complete visual representations. Despite the popularity of $\ell_1$
regularization, in this paper, we investigate the usage of non-convex
regularizations in this problem. Our contribution consists of three parts.
First, we propose the leaky capped norm regularization (LCNR), which allows
model weights below a certain threshold to be regularized more strongly as
opposed to those above, therefore imposes strong sparsity and only introduces
controllable estimation bias. We propose a majorization-minimization algorithm
to optimize the joint objective function. Second, our study over monocular 3D
shape recovery and neural networks with LCNR outperforms $\ell_1$ and other
non-convex regularizations, achieving state-of-the-art performance and faster
convergence. Third, we prove a theoretical global convergence speed on the 3D
recovery problem. To the best of our knowledge, this is the first convergence
analysis of the 3D recovery problem.","Learning Sparse Visual Representations with Leaky Capped Norm
  Regularizers",2017
428,0.186094277,0.00125035,0.001250155,0.001250241,0.488676041,0.001250223,0.001250241,0.001250252,0.316477951,0.001250269,Topic5,"ConvNets and Imagenet have driven the recent success of deep learning for
image classification. However, the marked slowdown in performance improvement,
the recent studies on the lack of robustness of neural networks to adversarial
examples and their tendency to exhibit undesirable biases (e.g racial biases)
questioned the reliability and the sustained development of these methods. This
work investigates these questions from the perspective of the end-user by using
human subject studies and explanations. We experimentally demonstrate that the
accuracy and robustness of ConvNets measured on Imagenet are underestimated. We
show that explanations can mitigate the impact of misclassified adversarial
examples from the perspective of the end-user and we introduce a novel tool for
uncovering the undesirable biases learned by a model. These contributions also
show that explanations are a promising tool for improving our understanding of
ConvNets' predictions and for designing more reliable models","ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection,
  Adversarial Examples and Model Criticism",2017
429,0.001052866,0.001052776,0.001052842,0.104188008,0.00105277,0.47367064,0.18725886,0.001052726,0.22856578,0.001052731,Topic6,"We consider the problem of learning a one-hidden-layer neural network with
non-overlapping convolutional layer and ReLU activation function, i.e.,
$f(\mathbf{Z}; \mathbf{w}, \mathbf{a}) = \sum_j
a_j\sigma(\mathbf{w}^\top\mathbf{Z}_j)$, in which both the convolutional
weights $\mathbf{w}$ and the output weights $\mathbf{a}$ are parameters to be
learned. We prove that with Gaussian input $\mathbf{Z}$, there is a spurious
local minimum that is not a global mininum. Surprisingly, in the presence of
local minimum, starting from randomly initialized weights, gradient descent
with weight normalization can still be proven to recover the true parameters
with constant probability (which can be boosted to arbitrarily high accuracy
with multiple restarts). We also show that with constant probability, the same
procedure could also converge to the spurious local minimum, showing that the
local minimum plays a non-trivial role in the dynamics of gradient descent.
Furthermore, a quantitative analysis shows that the gradient descent dynamics
has two phases: it starts off slow, but converges much faster after several
iterations.","Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of
  Spurious Local Minima",2017
430,0.116694554,0.000826635,0.000826796,0.000826641,0.510665599,0.270294124,0.000826591,0.000826606,0.097385859,0.000826595,Topic5,"In many real-world scenarios, rewards extrinsic to the agent are extremely
sparse, or absent altogether. In such cases, curiosity can serve as an
intrinsic reward signal to enable the agent to explore its environment and
learn skills that might be useful later in its life. We formulate curiosity as
the error in an agent's ability to predict the consequence of its own actions
in a visual feature space learned by a self-supervised inverse dynamics model.
Our formulation scales to high-dimensional continuous state spaces like images,
bypasses the difficulties of directly predicting pixels, and, critically,
ignores the aspects of the environment that cannot affect the agent. The
proposed approach is evaluated in two environments: VizDoom and Super Mario
Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where
curiosity allows for far fewer interactions with the environment to reach the
goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent
to explore more efficiently; and 3) generalization to unseen scenarios (e.g.
new levels of the same game) where the knowledge gained from earlier experience
helps the agent explore new places much faster than starting from scratch. Demo
video and code available at https://pathak22.github.io/noreward-rl/",Curiosity-driven Exploration by Self-supervised Prediction,2017
431,0.687833623,0.001408746,0.001408745,0.001408829,0.001408752,0.001408884,0.260319092,0.001408748,0.001408875,0.041985706,Topic1,"Generating adversarial examples is a critical step for evaluating and
improving the robustness of learning machines. So far, most existing methods
only work for classification and are not designed to alter the true performance
measure of the problem at hand. We introduce a novel flexible approach named
Houdini for generating adversarial examples specifically tailored for the final
performance measure of the task considered, be it combinatorial and
non-decomposable. We successfully apply Houdini to a range of applications such
as speech recognition, pose estimation and semantic segmentation. In all cases,
the attacks based on Houdini achieve higher success rate than those based on
the traditional surrogates used to train the models while using a less
perceptible adversarial perturbation.",Houdini: Fooling Deep Structured Prediction Models,2017
432,0.321006643,0.000952699,0.000952543,0.000952538,0.283508282,0.000952658,0.000952648,0.000952624,0.388816746,0.000952619,Topic9,"With the recent renaissance of deep convolution neural networks, encouraging
breakthroughs have been achieved on the supervised recognition tasks, where
each class has sufficient training data and fully annotated training data.
However, to scale the recognition to a large number of classes with few or now
training samples for each class remains an unsolved problem. One approach to
scaling up the recognition is to develop models capable of recognizing unseen
categories without any training instances, or zero-shot recognition/ learning.
This article provides a comprehensive review of existing zero-shot recognition
techniques covering various aspects ranging from representations of models, and
from datasets and evaluation settings. We also overview related recognition
tasks including one-shot and open set recognition which can be used as natural
extensions of zero-shot recognition when limited number of class samples become
available or when zero-shot recognition is implemented in a real-world setting.
Importantly, we highlight the limitations of existing approaches and point out
future research directions in this existing new research area.",Recent Advances in Zero-shot Recognition,2017
433,0.001075505,0.001075483,0.001075441,0.034224505,0.00107545,0.226959563,0.099537411,0.001075421,0.632825794,0.001075428,Topic9,"We analyze the expressiveness and loss surface of practical deep
convolutional neural networks (CNNs) with shared weights and max pooling
layers. We show that such CNNs produce linearly independent features at a
""wide"" layer which has more neurons than the number of training samples. This
condition holds e.g. for the VGG network. Furthermore, we provide for such wide
CNNs necessary and sufficient conditions for global minima with zero training
error. For the case where the wide layer is followed by a fully connected
layer, we show that almost every critical point of the empirical loss is a
global minimum with zero training error. Our analysis suggests that both depth
and width are very important in deep learning. While depth brings more
representational power and allows the network to learn high level features,
width smoothes the optimization landscape of the loss function in the sense
that a sufficiently wide network has a well-behaved loss surface with
potentially no bad local minima.",The loss surface and expressivity of deep convolutional neural networks,2017
434,0.001010384,0.0010104,0.001010296,0.001010351,0.376278376,0.175133079,0.001010361,0.001010368,0.441516085,0.001010301,Topic9,"This paper introduces a novel framework for combining scientific knowledge of
physics-based models with neural networks to advance scientific discovery. This
framework, termed as physics-guided neural network (PGNN), leverages the output
of physics-based model simulations along with observational features to
generate predictions using a neural network architecture. Further, this paper
presents a novel framework for using physics-based loss functions in the
learning objective of neural networks, to ensure that the model predictions not
only show lower errors on the training set but are also scientifically
consistent with the known physics on the unlabeled set. We illustrate the
effectiveness of PGNN for the problem of lake temperature modeling, where
physical relationships between the temperature, density, and depth of water are
used to design a physics-based loss function. By using scientific knowledge to
guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as
scientific consistency of results.","Physics-guided Neural Networks (PGNN): An Application in Lake
  Temperature Modeling",2017
435,0.054730691,0.000763514,0.23996124,0.000763549,0.000763572,0.128002545,0.572724098,0.000763538,0.000763609,0.000763644,Topic7,"Spectral clustering has found extensive use in many areas. Most traditional
spectral clustering algorithms work in three separate steps: similarity graph
construction; continuous labels learning; discretizing the learned labels by
k-means clustering. Such common practice has two potential flaws, which may
lead to severe information loss and performance degradation. First, predefined
similarity graph might not be optimal for subsequent clustering. It is
well-accepted that similarity graph highly affects the clustering results. To
this end, we propose to automatically learn similarity information from data
and simultaneously consider the constraint that the similarity matrix has exact
c connected components if there are c clusters. Second, the discrete solution
may deviate from the spectral solution since k-means method is well-known as
sensitive to the initialization of cluster centers. In this work, we transform
the candidate solution into a new one that better approximates the discrete
one. Finally, those three subtasks are integrated into a unified framework,
with each subtask iteratively boosted by using the results of the others
towards an overall optimal solution. It is known that the performance of a
kernel method is largely determined by the choice of kernels. To tackle this
practical problem of how to select the most suitable kernel for a particular
data set, we further extend our model to incorporate multiple kernel learning
ability. Extensive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.",Unified Spectral Clustering with Optimal Graph,2017
436,0.000877352,0.060128672,0.000877432,0.000877333,0.00087739,0.614395454,0.000877481,0.000877336,0.302559066,0.017652484,Topic6,"Dropout is a simple but effective technique for learning in neural networks
and other settings. A sound theoretical understanding of dropout is needed to
determine when dropout should be applied and how to use it most effectively. In
this paper we continue the exploration of dropout as a regularizer pioneered by
Wager, et.al. We focus on linear classification where a convex proxy to the
misclassification loss (i.e. the logistic loss used in logistic regression) is
minimized. We show: (a) when the dropout-regularized criterion has a unique
minimizer, (b) when the dropout-regularization penalty goes to infinity with
the weights, and when it remains bounded, (c) that the dropout regularization
can be non-monotonic as individual weights increase from 0, and (d) that the
dropout regularization penalty may not be convex. This last point is
particularly surprising because the combination of dropout regularization with
any convex loss proxy is always a convex function.
  In order to contrast dropout regularization with $L_2$ regularization, we
formalize the notion of when different sources are more compatible with
different regularizers. We then exhibit distributions that are provably more
compatible with dropout regularization than $L_2$ regularization, and vice
versa. These sources provide additional insight into how the inductive biases
of dropout and $L_2$ regularization differ. We provide some similar results for
$L_1$ regularization.",On the Inductive Bias of Dropout,2014
437,0.001234797,0.001234848,0.084216302,0.001234755,0.00123483,0.249915934,0.001234949,0.001234701,0.657224023,0.001234861,Topic9,"We analyze dropout in deep networks with rectified linear units and the
quadratic loss. Our results expose surprising differences between the behavior
of dropout and more traditional regularizers like weight decay. For example, on
some simple data sets dropout training produces negative weights even though
the output is the sum of the inputs. This provides a counterpoint to the
suggestion that dropout discourages co-adaptation of weights. We also show that
the dropout penalty can grow exponentially in the depth of the network while
the weight-decay penalty remains essentially linear, and that dropout is
insensitive to various re-scalings of the input features, outputs, and network
weights. This last insensitivity implies that there are no isolated local
minima of the dropout training criterion. Our work uncovers new properties of
dropout, extends our understanding of why dropout succeeds, and lays the
foundation for further progress.",Surprising properties of dropout in deep networks,2016
438,0.001149642,0.060382535,0.00114977,0.001149667,0.001149647,0.14091112,0.001149818,0.275445153,0.516362951,0.001149697,Topic9,"Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at
harnessing the energy efficiency of spike-domain processing by building on
computing elements that operate on, and exchange, spikes. In this paper, the
problem of training a two-layer SNN is studied for the purpose of
classification, under a Generalized Linear Model (GLM) probabilistic neural
model that was previously considered within the computational neuroscience
literature. Conventional classification rules for SNNs operate offline based on
the number of output spikes at each output neuron. In contrast, a novel
training method is proposed here for a first-to-spike decoding rule, whereby
the SNN can perform an early classification decision once spike firing is
detected at an output neuron. Numerical results bring insights into the optimal
parameter selection for the GLM neuron and on the accuracy-complexity trade-off
performance of conventional and first-to-spike decoding.","Training Probabilistic Spiking Neural Networks with First-to-spike
  Decoding",2017
439,0.001176707,0.185006512,0.073156994,0.001176714,0.001176824,0.286800979,0.447975156,0.001176713,0.001176746,0.001176654,Topic7,"Enormous successes have been made by quantum algorithms during the last
decade. In this paper, we combine the quantum game with the problem of data
clustering, and then develop a quantum-game-based clustering algorithm, in
which data points in a dataset are considered as players who can make decisions
and implement quantum strategies in quantum games. After each round of a
quantum game, each player's expected payoff is calculated. Later, he uses a
link-removing-and-rewiring (LRR) function to change his neighbors and adjust
the strength of links connecting to them in order to maximize his payoff.
Further, algorithms are discussed and analyzed in two cases of strategies, two
payoff matrixes and two LRR functions. Consequently, the simulation results
have demonstrated that data points in datasets are clustered reasonably and
efficiently, and the clustering algorithms have fast rates of convergence.
Moreover, the comparison with other algorithms also provides an indication of
the effectiveness of the proposed approach.",A Novel Clustering Algorithm Based on Quantum Games,2008
440,0.000584915,0.000584971,0.000584936,0.000584935,0.000584916,0.469875631,0.053931079,0.000584906,0.472098826,0.000584886,Topic9,"Despite the widespread practical success of deep learning methods, our
theoretical understanding of the dynamics of learning in deep neural networks
remains quite sparse. We attempt to bridge the gap between the theory and
practice of deep learning by systematically analyzing learning dynamics for the
restricted case of deep linear neural networks. Despite the linearity of their
input-output map, such networks have nonlinear gradient descent dynamics on
weights that change with the addition of each new hidden layer. We show that
deep linear networks exhibit nonlinear learning phenomena similar to those seen
in simulations of nonlinear networks, including long plateaus followed by rapid
transitions to lower error solutions, and faster convergence from greedy
unsupervised pretraining initial conditions than from random initial
conditions. We provide an analytical description of these phenomena by finding
new exact solutions to the nonlinear dynamics of deep learning. Our theoretical
analysis also reveals the surprising finding that as the depth of a network
approaches infinity, learning speed can nevertheless remain finite: for a
special class of initial conditions on the weights, very deep networks incur
only a finite, depth independent, delay in learning speed relative to shallow
networks. We show that, under certain conditions on the training data,
unsupervised pretraining can find this special class of initial conditions,
while scaled random Gaussian initializations cannot. We further exhibit a new
class of random orthogonal initial conditions on weights that, like
unsupervised pre-training, enjoys depth independent learning times. We further
show that these initial conditions also lead to faithful propagation of
gradients even in deep nonlinear networks, as long as they operate in a special
regime known as the edge of chaos.","Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks",2013
441,0.000943711,0.127072731,0.505965239,0.00094357,0.000943645,0.33518467,0.000943815,0.00094361,0.026115294,0.000943714,Topic3,"In signal analysis and synthesis, linear approximation theory considers a
linear decomposition of any given signal in a set of atoms, collected into a
so-called dictionary. Relevant sparse representations are obtained by relaxing
the orthogonality condition of the atoms, yielding overcomplete dictionaries
with an extended number of atoms. More generally than the linear decomposition,
overcomplete kernel dictionaries provide an elegant nonlinear extension by
defining the atoms through a mapping kernel function (e.g., the gaussian
kernel). Models based on such kernel dictionaries are used in neural networks,
gaussian processes and online learning with kernels.
  The quality of an overcomplete dictionary is evaluated with a diversity
measure the distance, the approximation, the coherence and the Babel measures.
In this paper, we develop a framework to examine overcomplete kernel
dictionaries with the entropy from information theory. Indeed, a higher value
of the entropy is associated to a further uniform spread of the atoms over the
space. For each of the aforementioned diversity measures, we derive lower
bounds on the entropy. Several definitions of the entropy are examined, with an
extensive analysis in both the input space and the mapped feature space.",Entropy of Overcomplete Kernel Dictionaries,2014
442,0.00063712,0.000637064,0.00063709,0.000637167,0.850974665,0.000637056,0.000637102,0.000637121,0.109972712,0.034592903,Topic5,"Measuring the morphological parameters of galaxies is a key requirement for
studying their formation and evolution. Surveys such as the Sloan Digital Sky
Survey (SDSS) have resulted in the availability of very large collections of
images, which have permitted population-wide analyses of galaxy morphology.
Morphological analysis has traditionally been carried out mostly via visual
inspection by trained experts, which is time-consuming and does not scale to
large ($\gtrsim10^4$) numbers of images.
  Although attempts have been made to build automated classification systems,
these have not been able to achieve the desired level of accuracy. The Galaxy
Zoo project successfully applied a crowdsourcing strategy, inviting online
users to classify images by answering a series of questions. Unfortunately,
even this approach does not scale well enough to keep up with the increasing
availability of galaxy images.
  We present a deep neural network model for galaxy morphology classification
which exploits translational and rotational symmetry. It was developed in the
context of the Galaxy Challenge, an international competition to build the best
model for morphology classification based on annotated images from the Galaxy
Zoo project.
  For images with high agreement among the Galaxy Zoo participants, our model
is able to reproduce their consensus with near-perfect accuracy ($> 99\%$) for
most questions. Confident model predictions are highly accurate, which makes
the model suitable for filtering large collections of images and forwarding
challenging images to experts for manual annotation. This approach greatly
reduces the experts' workload without affecting accuracy. The application of
these algorithms to larger sets of training data will be critical for analysing
results from future surveys such as the LSST.","Rotation-invariant convolutional neural networks for galaxy morphology
  prediction",2015
443,0.128385838,0.088092724,0.611858907,0.000893117,0.000893099,0.000893143,0.11628541,0.050911648,0.000893087,0.000893027,Topic3,"The nonnegative matrix factorization (NMF) is widely used in signal and image
processing, including bio-informatics, blind source separation and
hyperspectral image analysis in remote sensing. A great challenge arises when
dealing with a nonlinear formulation of the NMF. Within the framework of kernel
machines, the models suggested in the literature do not allow the
representation of the factorization matrices, which is a fallout of the curse
of the pre-image. In this paper, we propose a novel kernel-based model for the
NMF that does not suffer from the pre-image problem, by investigating the
estimation of the factorization matrices directly in the input space. For
different kernel functions, we describe two schemes for iterative algorithms:
an additive update rule based on a gradient descent scheme and a multiplicative
update rule in the same spirit as in the Lee and Seung algorithm. Within the
proposed framework, we develop several extensions to incorporate constraints,
including sparseness, smoothness, and spatial regularization with a
total-variation-like penalty. The effectiveness of the proposed method is
demonstrated with the problem of unmixing hyperspectral images, using
well-known real images and results with state-of-the-art techniques.","Kernel Nonnegative Matrix Factorization Without the Curse of the
  Pre-image - Application to Unmixing Hyperspectral Images",2014
444,0.001052816,0.001053039,0.16676369,0.001052803,0.001052844,0.392808936,0.401911952,0.001052912,0.032198159,0.001052849,Topic7,"Many machine learning frameworks, such as resource-allocating networks,
kernel-based methods, Gaussian processes, and radial-basis-function networks,
require a sparsification scheme in order to address the online learning
paradigm. For this purpose, several online sparsification criteria have been
proposed to restrict the model definition on a subset of samples. The most
known criterion is the (linear) approximation criterion, which discards any
sample that can be well represented by the already contributing samples, an
operation with excessive computational complexity. Several computationally
efficient sparsification criteria have been introduced in the literature, such
as the distance, the coherence and the Babel criteria. In this paper, we
provide a framework that connects these sparsification criteria to the issue of
approximating samples, by deriving theoretical bounds on the approximation
errors. Moreover, we investigate the error of approximating any feature, by
proposing upper-bounds on the approximation error for each of the
aforementioned sparsification criteria. Two classes of features are described
in detail, the empirical mean and the principal axes in the kernel principal
component analysis.",Approximation errors of online sparsification criteria,2014
445,0.001315996,0.195632764,0.001316152,0.084767026,0.001316014,0.139112425,0.0013161,0.001316063,0.572591164,0.001316295,Topic9,"First steps towards a mathematical theory of deep convolutional neural
networks for feature extraction were made---for the continuous-time case---in
Mallat, 2012, and Wiatowski and B\""olcskei, 2015. This paper considers the
discrete case, introduces new convolutional neural network architectures, and
proposes a mathematical framework for their analysis. Specifically, we
establish deformation and translation sensitivity results of local and global
nature, and we investigate how certain structural properties of the input
signal are reflected in the corresponding feature vectors. Our theory applies
to general filters and general Lipschitz-continuous non-linearities and pooling
operators. Experiments on handwritten digit classification and facial landmark
detection---including feature importance evaluation---complement the
theoretical findings.",Discrete Deep Feature Extraction: A Theory and New Architectures,2016
446,0.001429131,0.001428904,0.001428768,0.001428735,0.00142882,0.001428934,0.001428804,0.00142884,0.160436694,0.828132371,Topic10,"We propose Neural Responding Machine (NRM), a neural network-based response
generator for Short-Text Conversation. NRM takes the general encoder-decoder
framework: it formalizes the generation of response as a decoding process based
on the latent representation of the input text, while both encoding and
decoding are realized with recurrent neural networks (RNN). The NRM is trained
with a large amount of one-round conversation data collected from a
microblogging service. Empirical study shows that NRM can generate
grammatically correct and content-wise appropriate responses to over 75% of the
input text, outperforming state-of-the-arts in the same setting, including
retrieval-based and SMT-based models.",Neural Responding Machine for Short-Text Conversation,2015
447,0.135756371,0.001449639,0.001449455,0.001449588,0.001449757,0.133009297,0.135367642,0.001449716,0.001449747,0.587168788,Topic10,"We propose an online, end-to-end, neural generative conversational model for
open-domain dialogue. It is trained using a unique combination of offline
two-phase supervised learning and online human-in-the-loop active learning.
While most existing research proposes offline supervision or hand-crafted
reward functions for online reinforcement, we devise a novel interactive
learning mechanism based on hamming-diverse beam search for response generation
and one-character user-feedback at each step. Experiments show that our model
inherently promotes the generation of semantically relevant and interesting
responses, and can be used to train agents with customized personas, moods and
conversational styles.",Deep Active Learning for Dialogue Generation,2016
448,0.00161339,0.058487536,0.001613331,0.001613168,0.188482036,0.001613334,0.001613278,0.001613237,0.177038656,0.566312035,Topic10,"Teaching machines to read natural language documents remains an elusive
challenge. Machine reading systems can be tested on their ability to answer
questions posed on the contents of documents that they have seen, but until now
large scale training and test datasets have been missing for this type of
evaluation. In this work we define a new methodology that resolves this
bottleneck and provides large scale supervised reading comprehension data. This
allows us to develop a class of attention based deep neural networks that learn
to read real documents and answer complex questions with minimal prior
knowledge of language structure.",Teaching Machines to Read and Comprehend,2015
449,0.001316207,0.001316176,0.026164124,0.001316055,0.001316223,0.001316059,0.001316091,0.001316099,0.084396034,0.880226933,Topic10,"Deep compositional models of meaning acting on distributional representations
of words in order to produce vectors of larger text constituents are evolving
to a popular area of NLP research. We detail a compositional distributional
framework based on a rich form of word embeddings that aims at facilitating the
interactions between words in the context of a sentence. Embeddings and
composition layers are jointly learned against a generic objective that
enhances the vectors with syntactic information from the surrounding context.
Furthermore, each word is associated with a number of senses, the most
plausible of which is selected dynamically during the composition process. We
evaluate the produced vectors qualitatively and quantitatively with positive
results. At the sentence level, the effectiveness of the framework is
demonstrated on the MSRPar task, for which we report results within the
state-of-the-art range.","Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models
  of Meaning",2015
450,0.163009135,0.00089299,0.000893037,0.000893107,0.000893037,0.000893005,0.000893039,0.000893008,0.148444264,0.682295378,Topic10,"Matching natural language sentences is central for many applications such as
information retrieval and question answering. Existing deep models rely on a
single sentence representation or multiple granularity representations for
matching. However, such methods cannot well capture the contextualized local
information in the matching process. To tackle this problem, we present a new
deep architecture to match two sentences with multiple positional sentence
representations. Specifically, each positional sentence representation is a
sentence representation at this position, generated by a bidirectional long
short term memory (Bi-LSTM). The matching score is finally produced by
aggregating interactions between these different positional sentence
representations, through $k$-Max pooling and a multi-layer perceptron. Our
model has several advantages: (1) By using Bi-LSTM, rich context of the whole
sentence is leveraged to capture the contextualized local information in each
positional sentence representation; (2) By matching with multiple positional
sentence representations, it is flexible to aggregate different important
contextualized local information in a sentence to support the matching; (3)
Experiments on different tasks such as question answering and sentence
completion demonstrate the superiority of our model.","A Deep Architecture for Semantic Matching with Multiple Positional
  Sentence Representations",2015
451,0.001449537,0.001449498,0.025235879,0.001449526,0.001449624,0.001449488,0.001449505,0.001449536,0.23953836,0.725079046,Topic10,"Artificial neural networks are powerful models, which have been widely
applied into many aspects of machine translation, such as language modeling and
translation modeling. Though notable improvements have been made in these
areas, the reordering problem still remains a challenge in statistical machine
translations. In this paper, we present a novel neural reordering model that
directly models word pairs and alignment. By utilizing LSTM recurrent neural
networks, much longer context could be learned for reordering prediction.
Experimental results on NIST OpenMT12 Arabic-English and Chinese-English
1000-best rescoring task show that our LSTM neural reordering feature is robust
and achieves significant improvements over various baseline systems.",LSTM Neural Reordering Feature for Statistical Machine Translation,2015
452,0.000971102,0.000971164,0.000971017,0.000971078,0.000971062,0.000971211,0.000971133,0.000971044,0.153264762,0.838966427,Topic10,"Natural language inference (NLI) is a fundamentally important task in natural
language processing that has many applications. The recently released Stanford
Natural Language Inference (SNLI) corpus has made it possible to develop and
evaluate learning-centered methods such as deep neural networks for natural
language inference (NLI). In this paper, we propose a special long short-term
memory (LSTM) architecture for NLI. Our model builds on top of a recently
proposed neural attention model for NLI but is based on a significantly
different idea. Instead of deriving sentence embeddings for the premise and the
hypothesis to be used for classification, our solution uses a match-LSTM to
perform word-by-word matching of the hypothesis with the premise. This LSTM is
able to place more emphasis on important word-level matching results. In
particular, we observe that this LSTM remembers important mismatches that are
critical for predicting the contradiction or the neutral relationship label. On
the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the
state of the art.",Learning Natural Language Inference with LSTM,2015
453,0.001020744,0.04954624,0.099747982,0.001020698,0.001020744,0.001020655,0.080836172,0.001020595,0.496200262,0.268565908,Topic9,"Recursive neural networks (RNN) and their recently proposed extension
recursive long short term memory networks (RLSTM) are models that compute
representations for sentences, by recursively combining word embeddings
according to an externally provided parse tree. Both models thus, unlike
recurrent networks, explicitly make use of the hierarchical structure of a
sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the
vanishing gradient and long distance dependency problem, and that RLSTMs
greatly improve over RNN's on these problems. We present an artificial learning
task that allows us to quantify the severity of these problems for both models.
We further show that a ratio of gradients (at the root node and a focal leaf
node) is highly indicative of the success of backpropagation at optimizing the
relevant weights low in the tree. This paper thus provides an explanation for
existing, superior results of RLSTMs on tasks such as sentiment analysis, and
suggests that the benefits of including hierarchical structure and of including
LSTM-style gating are complementary.","Quantifying the vanishing gradient and long distance dependency problem
  in recursive neural networks and recursive LSTMs",2016
454,0.001219951,0.001220054,0.00121974,0.001219667,0.001219765,0.001219697,0.001219716,0.001219728,0.350217427,0.640024256,Topic10,"Without discourse connectives, classifying implicit discourse relations is a
challenging task and a bottleneck for building a practical discourse parser.
Previous research usually makes use of one kind of discourse framework such as
PDTB or RST to improve the classification performance on discourse relations.
Actually, under different discourse annotation frameworks, there exist multiple
corpora which have internal connections. To exploit the combination of
different discourse corpora, we design related discourse classification tasks
specific to a corpus, and propose a novel Convolutional Neural Network embedded
multi-task learning system to synthesize these tasks by learning both unique
and shared representations for each task. The experimental results on the PDTB
implicit discourse relation classification task demonstrate that our model
achieves significant gains over baseline systems.","Implicit Discourse Relation Classification via Multi-Task Neural
  Networks",2016
455,0.091816055,0.001205613,0.001205228,0.001205105,0.001205004,0.001205306,0.001205336,0.001205008,0.387568091,0.512179255,Topic10,"Neural network based approaches for sentence relation modeling automatically
generate hidden matching features from raw sentence pairs. However, the quality
of matching feature representation may not be satisfied due to complex semantic
relations such as entailment or contradiction. To address this challenge, we
propose a new deep neural network architecture that jointly leverage
pre-trained word embedding and auxiliary character embedding to learn sentence
meanings. The two kinds of word sequence representations as inputs into
multi-layer bidirectional LSTM to learn enhanced sentence representation. After
that, we construct matching features followed by another temporal CNN to learn
high-level hidden matching feature representations. Experimental results
demonstrate that our approach consistently outperforms the existing methods on
standard evaluation datasets.","Enhancing Sentence Relation Modeling with Auxiliary Character-level
  Embedding",2016
456,0.000952627,0.000952728,0.00095258,0.000952693,0.000952735,0.031004529,0.000952648,0.000952574,0.272164993,0.690161893,Topic10,"Previous studies in Open Information Extraction (Open IE) are mainly based on
extraction patterns. They manually define patterns or automatically learn them
from a large corpus. However, these approaches are limited when grasping the
context of a sentence, and they fail to capture implicit relations. In this
paper, we address this problem with the following methods. First, we exploit
long short-term memory (LSTM) networks to extract higher-level features along
the shortest dependency paths, connecting headwords of relations and arguments.
The path-level features from LSTM networks provide useful clues regarding
contextual information and the validity of arguments. Second, we constructed
samples to train LSTM networks without the need for manual labeling. In
particular, feedback negative sampling picks highly negative samples among
non-positive samples through a model trained with positive samples. The
experimental results show that our approach produces more precise and abundant
extractions than state-of-the-art open IE systems. To the best of our
knowledge, this is the first work to apply deep learning to Open IE.","Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks
  with Feedback Negative Sampling",2016
457,0.000961752,0.000961841,0.00096175,0.000961734,0.000961753,0.000961701,0.050273618,0.000961755,0.084763882,0.858230214,Topic10,"With the rapid growth of knowledge bases (KBs) on the web, how to take full
advantage of them becomes increasingly important. Knowledge base-based question
answering (KB-QA) is one of the most promising approaches to access the
substantial knowledge. Meantime, as the neural network-based (NN-based) methods
develop, NN-based KB-QA has already achieved impressive results. However,
previous work did not put emphasis on question representation, and the question
is converted into a fixed vector regardless of its candidate answers. This
simple representation strategy is unable to express the proper information of
the question. Hence, we present a neural attention-based model to represent the
questions dynamically according to the different focuses of various candidate
answer aspects. In addition, we leverage the global knowledge inside the
underlying KB, aiming at integrating the rich KB information into the
representation of the answers. And it also alleviates the out of vocabulary
(OOV) problem, which helps the attention model to represent the question more
precisely. The experimental results on WEBQUESTIONS demonstrate the
effectiveness of the proposed approach.","Question Answering over Knowledge Base with Neural Attention Combining
  Global Knowledge Information",2016
458,0.047515512,0.000877545,0.000877285,0.000877572,0.000877372,0.000877397,0.000877335,0.000877311,0.000877368,0.945465302,Topic10,"The ability to reason with natural language is a fundamental prerequisite for
many NLP tasks such as information extraction, machine translation and question
answering. To quantify this ability, systems are commonly tested whether they
can recognize textual entailment, i.e., whether one sentence can be inferred
from another one. However, in most NLP applications only single source
sentences instead of sentence pairs are available. Hence, we propose a new task
that measures how well a model can generate an entailed sentence from a source
sentence. We take entailment-pairs of the Stanford Natural Language Inference
corpus and train an LSTM with attention. On a manually annotated test set we
found that 82% of generated sentences are correct, an improvement of 10.3% over
an LSTM baseline. A qualitative analysis shows that this model is not only
capable of shortening input sentences, but also inferring new statements via
paraphrasing and phrase entailment. We then apply this model recursively to
input-output pairs, thereby generating natural language inference chains that
can be used to automatically construct an entailment graph from source
sentences. Finally, by swapping source and target sentences we can also train a
model that given an input sentence invents additional information to generate a
new sentence.",Generating Natural Language Inference Chains,2016
459,0.001316075,0.204404444,0.001315974,0.001315972,0.067272809,0.001316131,0.001316065,0.001316071,0.427530992,0.292895467,Topic9,"Recurrent neural networks such as the GRU and LSTM found wide adoption in
natural language processing and achieve state-of-the-art results for many
tasks. These models are characterized by a memory state that can be written to
and read from by applying gated composition operations to the current input and
the previous state. However, they only cover a small subset of potentially
useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that
allow for arbitrary differentiable functions as composition operations.
Furthermore, MuFuRUs allow for an input- and state-dependent choice of these
composition operations that is learned. Our experiments demonstrate that the
additional functionality helps in different sequence modeling tasks, including
the evaluation of propositional logic formulae, language modeling and sentiment
analysis.",MuFuRU: The Multi-Function Recurrent Unit,2016
460,0.05173968,0.081055092,0.000943608,0.000943645,0.075319847,0.000943675,0.000943586,0.079230138,0.439386946,0.269493783,Topic9,"Recurrent neural networks, and in particular long short-term memory (LSTM)
networks, are a remarkably effective tool for sequence modeling that learn a
dense black-box hidden representation of their sequential input. Researchers
interested in better understanding these models have studied the changes in
hidden state representations over time and noticed some interpretable patterns
but also significant noise. In this work, we present LSTMVIS, a visual analysis
tool for recurrent neural networks with a focus on understanding these hidden
state dynamics. The tool allows users to select a hypothesis input range to
focus on local state changes, to match these states changes to similar patterns
in a large data set, and to align these results with structural annotations
from their domain. We show several use cases of the tool for analyzing specific
hidden state properties on dataset containing nesting, phrase structure, and
chord progressions, and demonstrate how the tool can be used to isolate
patterns for further statistical analysis. We characterize the domain, the
different stakeholders, and their goals and tasks.","LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in
  Recurrent Neural Networks",2016
461,0.281678995,0.00119073,0.001190783,0.001190797,0.001190645,0.067474203,0.017842749,0.001190694,0.33519415,0.291856257,Topic9,"Neural Machine Translation (NMT), like many other deep learning domains,
typically suffers from over-parameterization, resulting in large storage sizes.
This paper examines three simple magnitude-based pruning schemes to compress
NMT models, namely class-blind, class-uniform, and class-distribution, which
differ in terms of how pruning thresholds are computed for the different
classes of weights in the NMT architecture. We demonstrate the efficacy of
weight pruning as a compression technique for a state-of-the-art NMT system. We
show that an NMT model with over 200 million parameters can be pruned by 40%
with very little performance loss as measured on the WMT'14 English-German
translation task. This sheds light on the distribution of redundancy in the NMT
architecture. Our main result is that with retraining, we can recover and even
surpass the original performance with an 80%-pruned model.",Compression of Neural Machine Translation Models via Pruning,2016
462,0.245105151,0.001064368,0.001064023,0.001063972,0.001064033,0.001064198,0.001064077,0.001063978,0.162808068,0.584638133,Topic10,"Natural Language Inference is an important task for Natural Language
Understanding. It is concerned with classifying the logical relation between
two sentences. In this paper, we propose several text generative neural
networks for generating text hypothesis, which allows construction of new
Natural Language Inference datasets. To evaluate the models, we propose a new
metric -- the accuracy of the classifier trained on the generated dataset. The
accuracy obtained by our best generative model is only 2.7% lower than the
accuracy of the classifier trained on the original, human crafted dataset.
Furthermore, the best generated dataset combined with the original dataset
achieves the highest accuracy. The best model learns a mapping embedding for
each training example. By comparing various metrics we show that datasets that
obtain higher ROUGE or METEOR scores do not necessarily yield higher
classification accuracies. We also provide analysis of what are the
characteristics of a good dataset including the distinguishability of the
generated datasets from the original one.","Constructing a Natural Language Inference Dataset using Generative
  Neural Networks",2016
463,0.000971187,0.000971024,0.109932899,0.00097107,0.000971138,0.000971023,0.000971118,0.0009711,0.211780174,0.671489267,Topic10,"While question answering (QA) with neural network, i.e. neural QA, has
achieved promising results in recent years, lacking of large scale real-word QA
dataset is still a challenge for developing and evaluating neural QA system. To
alleviate this problem, we propose a large scale human annotated real-world QA
dataset WebQA with more than 42k questions and 556k evidences. As existing
neural QA methods resolve QA either as sequence generation or
classification/ranking problem, they face challenges of expensive softmax
computation, unseen answers handling or separate candidate answer generation
component. In this work, we cast neural QA as a sequence labeling problem and
propose an end-to-end sequence labeling model, which overcomes all the above
challenges. Experimental results on WebQA show that our model outperforms the
baselines significantly with an F1 score of 74.69% with word-based input, and
the performance drops only 3.72 F1 points with more challenging character-based
input.","Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain
  Factoid Question Answering",2016
464,0.001266176,0.001266017,0.001266093,0.001266041,0.001265998,0.026636864,0.001266078,0.001266047,0.091097367,0.873403319,Topic10,"We present Tweet2Vec, a novel method for generating general-purpose vector
representation of tweets. The model learns tweet embeddings using
character-level CNN-LSTM encoder-decoder. We trained our model on 3 million,
randomly selected English-language tweets. The model was evaluated using two
methods: tweet semantic similarity and tweet sentiment categorization,
outperforming the previous state-of-the-art in both tasks. The evaluations
demonstrate the power of the tweet embeddings generated by our model for
various tweet categorization tasks. The vector representations generated by our
model are generic, and hence can be applied to a variety of tasks. Though the
model presented in this paper is trained on English-language tweets, the method
presented can be used to learn tweet embeddings for different languages.","Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM
  Encoder-Decoder",2016
465,0.001136702,0.001136537,0.001136498,0.035141186,0.00113649,0.20439171,0.001136657,0.001136512,0.105946673,0.647701037,Topic10,"We introduce an online neural sequence to sequence model that learns to
alternate between encoding and decoding segments of the input as it is read. By
independently tracking the encoding and decoding representations our algorithm
permits exact polynomial marginalization of the latent segmentation during
training, and during decoding beam search is employed to find the best
alignment path together with the predicted output sequence. Our model tackles
the bottleneck of vanilla encoder-decoders that have to read and memorize the
entire input sequence in their fixed-length hidden states before producing any
output. It is different from previous attentive models in that, instead of
treating the attention weights as output of a deterministic function, our model
assigns attention weights to a sequential latent variable which can be
marginalized out and permits online generation. Experiments on abstractive
sentence summarization and morphological inflection show significant
performance gains over the baseline encoder-decoders.",Online Segment to Segment Neural Transduction,2016
466,0.370326477,0.08671351,0.002439592,0.002439336,0.002439546,0.002439604,0.002439496,0.002439338,0.002439582,0.525883519,Topic10,"We present a novel semi-supervised approach for sequence transduction and
apply it to semantic parsing. The unsupervised component is based on a
generative model in which latent sentences generate the unpaired logical forms.
We apply this method to a number of semantic parsing tasks focusing on domains
with limited access to labelled training data and extend those datasets with
synthetically generated logical forms.",Semantic Parsing with Semi-Supervised Sequential Autoencoders,2016
467,0.22166057,0.000893115,0.000892996,0.000892999,0.000893006,0.000893019,0.000893032,0.000892991,0.00089317,0.771195102,Topic10,"This paper presents a deep learning architecture for the semantic decoder
component of a Statistical Spoken Dialogue System. In a slot-filling dialogue,
the semantic decoder predicts the dialogue act and a set of slot-value pairs
from a set of n-best hypotheses returned by the Automatic Speech Recognition.
Most current models for spoken language understanding assume (i) word-aligned
semantic annotations as in sequence taggers and (ii) delexicalisation, or a
mapping of input words to domain-specific concepts using heuristics that try to
capture morphological variation but that do not scale to other domains nor to
language variation (e.g., morphology, synonyms, paraphrasing ). In this work
the semantic decoder is trained using unaligned semantic annotations and it
uses distributed semantic representation learning to overcome the limitations
of explicit delexicalisation. The proposed architecture uses a convolutional
neural network for the sentence representation and a long-short term memory
network for the context representation. Results are presented for the publicly
available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a
significantly higher word error rate (WER).","Exploiting Sentence and Context Representations in Deep Neural Models
  for Spoken Language Understanding",2016
468,0.181388057,0.001123898,0.001123799,0.001123816,0.001123841,0.202935323,0.00112384,0.001123864,0.164688474,0.444245087,Topic10,"We formulate sequence to sequence transduction as a noisy channel decoding
problem and use recurrent neural networks to parameterise the source and
channel models. Unlike direct models which can suffer from explaining-away
effects during training, noisy channel models must produce outputs that explain
their inputs, and their component models can be trained with not only paired
training samples but also unpaired samples from the marginal output
distribution. Using a latent variable to control how much of the conditioning
sequence the channel model needs to read in order to generate a subsequent
symbol, we obtain a tractable and effective beam search decoder. Experimental
results on abstractive sentence summarisation, morphological inflection, and
machine translation show that noisy channel models outperform direct models,
and that they significantly benefit from increased amounts of unpaired output
data that direct models cannot easily use.",The Neural Noisy Channel,2016
469,0.102773808,0.022420113,0.001111298,0.001111257,0.001111426,0.001111442,0.001111373,0.001111303,0.116143792,0.751994187,Topic10,"Researchers have recently started investigating deep neural networks for
dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)
models have shown promising results for unstructured tasks, such as word-level
dialogue response generation. The hope is that such models will be able to
leverage massive amounts of data to learn meaningful natural language
representations and response generation strategies, while requiring a minimum
amount of domain knowledge and hand-crafting. An important challenge is to
develop models that can effectively incorporate dialogue context and generate
meaningful and diverse responses. In support of this goal, we review recently
proposed models based on generative encoder-decoder neural network
architectures, and show that these models have better ability to incorporate
long-term dialogue history, to model uncertainty and ambiguity in dialogue, and
to generate responses with high-level compositional structure.",Generative Deep Neural Networks for Dialogue: A Short Review,2016
470,0.00067129,0.000671437,0.000671395,0.104995741,0.000671304,0.000671297,0.000671341,0.11190271,0.079924441,0.699149045,Topic10,"To enhance developer productivity, all modern integrated development
environments (IDEs) include code suggestion functionality that proposes likely
next tokens at the cursor. While current IDEs work well for statically-typed
languages, their reliance on type annotations means that they do not provide
the same level of support for dynamic programming languages as for
statically-typed languages. Moreover, suggestion engines in modern IDEs do not
propose expressions or multi-statement idiomatic code. Recent work has shown
that language models can improve code suggestion systems by learning from
software repositories. This paper introduces a neural language model with a
sparse pointer network aimed at capturing very long-range dependencies. We
release a large-scale code suggestion corpus of 41M lines of Python code
crawled from GitHub. On this corpus, we found standard neural language models
to perform well at suggesting local phenomena, but struggle to refer to
identifiers that are introduced many tokens in the past. By augmenting a neural
language model with a pointer network specialized in referring to predefined
classes of identifiers, we obtain a much lower perplexity and a 5 percentage
points increase in accuracy for code suggestion compared to an LSTM baseline.
In fact, this increase in code suggestion accuracy is due to a 13 times more
accurate prediction of identifiers. Furthermore, a qualitative analysis shows
this model indeed captures interesting long-range dependencies, like referring
to a class member defined over 60 tokens in the past.",Learning Python Code Suggestion with a Sparse Pointer Network,2016
471,0.002564958,0.212815245,0.002564483,0.002564424,0.002564767,0.002564456,0.002564939,0.002565037,0.181717601,0.587514091,Topic10,"We describe an open-source toolkit for neural machine translation (NMT). The
toolkit prioritizes efficiency, modularity, and extensibility with the goal of
supporting NMT research into model architectures, feature representations, and
source modalities, while maintaining competitive performance and reasonable
training requirements. The toolkit consists of modeling and translation
support, as well as detailed pedagogical documentation about the underlying
techniques.",OpenNMT: Open-Source Toolkit for Neural Machine Translation,2017
472,0.001176759,0.213085845,0.001176637,0.001176664,0.001176813,0.001176758,0.001176896,0.001176775,0.283029394,0.49564746,Topic10,"Recent development of large-scale question answering (QA) datasets triggered
a substantial amount of research into end-to-end neural architectures for QA.
Increasingly complex systems have been conceived without comparison to simpler
neural baseline systems that would justify their complexity. In this work, we
propose a simple heuristic that guides the development of neural baseline
systems for the extractive QA task. We find that there are two ingredients
necessary for building a high-performing neural QA system: first, the awareness
of question words while processing the context and second, a composition
function that goes beyond simple bag-of-words modeling, such as recurrent
neural networks. Our results show that FastQA, a system that meets these two
requirements, can achieve very competitive performance compared with existing
models. We argue that this surprising finding puts results of previous systems
and the complexity of recent QA datasets into perspective.",Making Neural QA as Simple as Possible but not Simpler,2017
473,0.001205114,0.001205183,0.001204985,0.001205075,0.442259917,0.001205026,0.001205037,0.001205179,0.001205155,0.548099328,Topic10,"This paper surveys the current state of the art in Natural Language
Generation (NLG), defined as the task of generating text or speech from
non-linguistic input. A survey of NLG is timely in view of the changes that the
field has undergone over the past decade or so, especially in relation to new
(usually data-driven) methods, as well as new applications of NLG technology.
This survey therefore aims to (a) give an up-to-date synthesis of research on
the core tasks in NLG and the architectures adopted in which such tasks are
organised; (b) highlight a number of relatively recent research topics that
have arisen partly as a result of growing synergies between NLG and other areas
of artificial intelligence; (c) draw attention to the challenges in NLG
evaluation, relating them to similar challenges faced in other areas of Natural
Language Processing, with an emphasis on different evaluation methods and the
relationships between them.","Survey of the State of the Art in Natural Language Generation: Core
  tasks, applications and evaluation",2017
474,0.001163073,0.001163169,0.001163087,0.001163057,0.001163061,0.001163074,0.001163194,0.001163133,0.001163106,0.989532046,Topic10,"Sentence simplification reduces semantic complexity to benefit people with
language impairments. Previous simplification studies on the sentence level and
word level have achieved promising results but also meet great challenges. For
sentence-level studies, sentences after simplification are fluent but sometimes
are not really simplified. For word-level studies, words are simplified but
also have potential grammar errors due to different usages of words before and
after simplification. In this paper, we propose a two-step simplification
framework by combining both the word-level and the sentence-level
simplifications, making use of their corresponding advantages. Based on the
two-step framework, we implement a novel constrained neural generation model to
simplify sentences given simplified words. The final results on Wikipedia and
Simple Wikipedia aligned datasets indicate that our method yields better
performance than various baselines.","A Constrained Sequence-to-Sequence Neural Model for Sentence
  Simplification",2017
475,0.001298988,0.001299054,0.001298911,0.001299062,0.001298901,0.001298844,0.001298888,0.023693695,0.371649051,0.595564608,Topic10,"Relation detection is a core component for many NLP applications including
Knowledge Base Question Answering (KBQA). In this paper, we propose a
hierarchical recurrent neural network enhanced by residual learning that
detects KB relations given an input question. Our method uses deep residual
bidirectional LSTMs to compare questions and relation names via different
hierarchies of abstraction. Additionally, we propose a simple KBQA system that
integrates entity linking and our proposed relation detector to enable one
enhance another. Experimental results evidence that our approach achieves not
only outstanding relation detection performance, but more importantly, it helps
our KBQA system to achieve state-of-the-art accuracy for both single-relation
(SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.",Improved Neural Relation Detection for Knowledge Base Question Answering,2017
476,0.000943663,0.081832917,0.107242863,0.000943667,0.000943714,0.053118701,0.00094369,0.000943609,0.14795709,0.605130087,Topic10,"This paper addresses the problem of automatic speech recognition (ASR) error
detection and their use for improving spoken language understanding (SLU)
systems. In this study, the SLU task consists in automatically extracting, from
ASR transcriptions , semantic concepts and concept/values pairs in a e.g
touristic information system. An approach is proposed for enriching the set of
semantic labels with error specific labels and by using a recently proposed
neural approach based on word embeddings to compute well calibrated ASR
confidence measures. Experimental results are reported showing that it is
possible to decrease significantly the Concept/Value Error Rate with a state of
the art system, outperforming previously published results performance on the
same experimental data. It also shown that combining an SLU approach based on
conditional random fields with a neural encoder/decoder attention based
architecture , it is possible to effectively identifying confidence islands and
uncertain semantic output segments useful for deciding appropriate error
handling actions by the dialogue manager strategy .",ASR error management for improving spoken language understanding,2017
477,0.001234953,0.072083305,0.001234747,0.07461659,0.001234859,0.001234856,0.001234748,0.001234938,0.001235048,0.844655955,Topic10,"Common-sense or background knowledge is required to understand natural
language, but in most neural natural language understanding (NLU) systems, the
requisite background knowledge is indirectly acquired from static corpora. We
develop a new reading architecture for the dynamic integration of explicit
background knowledge in NLU models. A new task-agnostic reading module provides
refined word representations to a task-specific NLU architecture by processing
background knowledge in the form of free-text statements, together with the
task-specific inputs. Strong performance on the tasks of document question
answering (DQA) and recognizing textual entailment (RTE) demonstrate the
effectiveness and flexibility of our approach. Analysis shows that our models
learn to exploit knowledge selectively and in a semantically appropriate way.",Dynamic Integration of Background Knowledge in Neural NLU Systems,2017
478,0.001429024,0.001428994,0.00142884,0.001428941,0.001428903,0.001428875,0.167670612,0.001428793,0.177066793,0.645260225,Topic10,"We study the skip-thought model with neighborhood information as weak
supervision. More specifically, we propose a skip-thought neighbor model to
consider the adjacent sentences as a neighborhood. We train our skip-thought
neighbor model on a large corpus with continuous sentences, and then evaluate
the trained model on 7 tasks, which include semantic relatedness, paraphrase
detection, and classification benchmarks. Both quantitative comparison and
qualitative investigation are conducted. We empirically show that, our
skip-thought neighbor model performs as well as the skip-thought model on
evaluation tasks. In addition, we found that, incorporating an autoencoder path
in our model didn't aid our model to perform better, while it hurts the
performance of the skip-thought model.",Rethinking Skip-thought: A Neighborhood based Approach,2017
479,0.000840534,0.047104402,0.025458744,0.000840441,0.000840506,0.000840432,0.000840478,0.000840441,0.212689247,0.709704775,Topic10,"Factoid question answering (QA) has recently benefited from the development
of deep learning (DL) systems. Neural network models outperform traditional
approaches in domains where large datasets exist, such as SQuAD (ca. 100,000
questions) for Wikipedia articles. However, these systems have not yet been
applied to QA in more specific domains, such as biomedicine, because datasets
are generally too small to train a DL system from scratch. For example, the
BioASQ dataset for biomedical QA comprises less then 900 factoid (single
answer) and list (multiple answers) QA instances. In this work, we adapt a
neural QA system trained on a large open-domain dataset (SQuAD, source) to a
biomedical dataset (BioASQ, target) by employing various transfer learning
techniques. Our network architecture is based on a state-of-the-art QA system,
extended with biomedical word embeddings and a novel mechanism to answer list
questions. In contrast to existing biomedical QA systems, our system does not
rely on domain-specific ontologies, parsers or entity taggers, which are
expensive to create. Despite this fact, our systems achieve state-of-the-art
results on factoid questions and competitive results on list questions.",Neural Domain Adaptation for Biomedical Question Answering,2017
480,0.001075666,0.001075545,0.001075437,0.001075518,0.043040121,0.047480729,0.001075517,0.001075439,0.001075691,0.901950335,Topic10,"We propose a two-stage neural model to tackle question generation from
documents. Our model first estimates the probability that word sequences in a
document compose ""interesting"" answers using a neural model trained on a
question-answering corpus. We thus take a data-driven approach to
interestingness. Predicted key phrases then act as target answers that
condition a sequence-to-sequence question generation model with a copy
mechanism. Empirically, our neural key phrase detection model significantly
outperforms an entity-tagging baseline system and existing rule-based
approaches. We demonstrate that the question generator formulates good quality
natural language questions from extracted key phrases, and a human study
indicates that our system's generated question-answer pairs are competitive
with those of an earlier approach. We foresee our system being used in an
educational setting to assess reading comprehension and also as a data
augmentation technique for semi-supervised learning.",Neural Models for Key Phrase Detection and Question Generation,2017
481,0.00129897,0.001298987,0.057274862,0.001298895,0.001298913,0.00129889,0.001299231,0.001298887,0.096100956,0.837531409,Topic10,"This paper describes our submission to the 2017 BioASQ challenge. We
participated in Task B, Phase B which is concerned with biomedical question
answering (QA). We focus on factoid and list question, using an extractive QA
model, that is, we restrict our system to output substrings of the provided
text snippets. At the core of our system, we use FastQA, a state-of-the-art
neural QA system. We extended it with biomedical word embeddings and changed
its answer layer to be able to answer list questions in addition to factoid
questions. We pre-trained the model on a large-scale open-domain QA dataset,
SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our
approach, we achieve state-of-the-art results on factoid questions and
competitive results on list questions.",Neural Question Answering at BioASQ 5B,2017
482,0.001923383,0.001924199,0.00192326,0.001923239,0.001923427,0.001923424,0.001923458,0.001923314,0.512944848,0.471667449,Topic9,"While natural languages are compositional, how state-of-the-art neural models
achieve compositionality is still unclear. We propose a deep network, which not
only achieves competitive accuracy for text classification, but also exhibits
compositional behavior. That is, while creating hierarchical representations of
a piece of text, such as a sentence, the lower layers of the network distribute
their layer-specific attention weights to individual words. In contrast, the
higher layers compose meaningful phrases and clauses, whose lengths increase as
the networks get deeper until fully composing the sentence.",A Deep Network with Visual Text Composition Behavior,2017
483,0.001492836,0.001492874,0.001492756,0.001493301,0.001492891,0.001492815,0.42671543,0.001492973,0.077863921,0.484970202,Topic10,"There exist two main approaches to automatically extract affective
orientation: lexicon-based and corpus-based. In this work, we argue that these
two methods are compatible and show that combining them can improve the
accuracy of emotion classifiers. In particular, we introduce a novel variant of
the Label Propagation algorithm that is tailored to distributed word
representations, we apply batch gradient descent to accelerate the optimization
of label propagation and to make the optimization feasible for large graphs,
and we propose a reproducible method for emotion lexicon expansion. We conclude
that label propagation can expand an emotion lexicon in a meaningful way and
that the expanded emotion lexicon can be leveraged to improve the accuracy of
an emotion classifier.","Semi-supervised emotion lexicon expansion with label propagation and
  specialized word embeddings",2017
484,0.001470821,0.070039396,0.001470758,0.025972952,0.477491868,0.001470828,0.00147084,0.001470807,0.001470824,0.417670905,Topic5,"Many genres of natural language text are narratively structured, a testament
to our predilection for organizing our experiences as narratives. There is
broad consensus that understanding a narrative requires identifying and
tracking the goals and desires of the characters and their narrative outcomes.
However, to date, there has been limited work on computational models for this
problem. We introduce a new dataset, DesireDB, which includes gold-standard
labels for identifying statements of desire, textual evidence for desire
fulfillment, and annotations for whether the stated desire is fulfilled given
the evidence in the narrative context. We report experiments on tracking desire
fulfillment using different methods, and show that LSTM Skip-Thought model
achieves F-measure of 0.7 on our corpus.",Modelling Protagonist Goals and Desires in First-Person Narrative,2017
485,0.000806683,0.145288084,0.000806564,0.000806603,0.490439357,0.101962936,0.000806615,0.000806618,0.059034723,0.199241816,Topic5,"Neural network-based systems can now learn to locate the referents of words
and phrases in images, answer questions about visual scenes, and even execute
symbolic instructions as first-person actors in partially-observable worlds. To
achieve this so-called grounded language learning, models must overcome certain
well-studied learning challenges that are also fundamental to infants learning
their first words. While it is notable that models with no meaningful prior
knowledge overcome these learning obstacles, AI researchers and practitioners
currently lack a clear understanding of exactly how they do so. Here we address
this question as a way of achieving a clearer general understanding of grounded
language learning, both to inform future research and to improve confidence in
model predictions. For maximum control and generality, we focus on a simple
neural network-based language learning agent trained via policy-gradient
methods to interpret synthetic linguistic instructions in a simulated 3D world.
We apply experimental paradigms from developmental psychology to this agent,
exploring the conditions under which established human biases and learning
effects emerge. We further propose a novel way to visualise and analyse
semantic representation in grounded language learning agents that yields a
plausible computational account of the observed effects.",Understanding Grounded Language Learning Agents,2017
486,0.001351667,0.001351939,0.260731356,0.001351524,0.335068421,0.001351767,0.001351762,0.156668392,0.001351729,0.239421443,Topic5,"This paper presents the design of the machine learning architecture that
underlies the Alexa Skills Kit (ASK) a large scale Spoken Language
Understanding (SLU) Software Development Kit (SDK) that enables developers to
extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the
infrastructure powers over 25,000 skills deployed through the ASK, as well as
AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability
and a rapid iteration cycle for third party developers. It imposes inductive
biases that allow it to learn robust SLU models from extremely small and sparse
datasets and, in doing so, removes significant barriers to entry for software
developers and dialogue systems researchers.","Just ASK: Building an Architecture for Extensible Self-Service Spoken
  Language Understanding",2017
487,0.001010359,0.177554751,0.001010315,0.001010409,0.001010445,0.001010294,0.001010436,0.001010338,0.001010446,0.814362207,Topic10,"Reading comprehension (RC)---in contrast to information retrieval---requires
integrating information and reasoning about events, entities, and their
relations across a full document. Question answering is conventionally used to
assess RC ability, in both artificial agents and children learning to read.
However, existing RC datasets and tasks are dominated by questions that can be
solved by selecting answers using superficial information (e.g., local context
similarity or global term frequency); they thus fail to test for the essential
integrative aspect of RC. To encourage progress on deeper comprehension of
language, we present a new dataset and set of tasks in which the reader must
answer questions about stories by reading entire books or movie scripts. These
tasks are designed so that successfully answering their questions requires
understanding the underlying narrative rather than relying on shallow pattern
matching or salience. We show that although humans solve the tasks easily,
standard RC models struggle on the tasks presented here. We provide an analysis
of the dataset and the challenges it presents.",The NarrativeQA Reading Comprehension Challenge,2017
488,0.000709414,0.210591858,0.000709706,0.000709338,0.000709442,0.000709359,0.000709388,0.391196669,0.000709433,0.393245393,Topic10,"We propose Cognitive Databases, an approach for transparently enabling
Artificial Intelligence (AI) capabilities in relational databases. A novel
aspect of our design is to first view the structured data source as meaningful
unstructured text, and then use the text to build an unsupervised neural
network model using a Natural Language Processing (NLP) technique called word
embedding. This model captures the hidden inter-/intra-column relationships
between database tokens of different types. For each database token, the model
includes a vector that encodes contextual semantic relationships. We seamlessly
integrate the word embedding model into existing SQL query infrastructure and
use it to enable a new class of SQL-based analytics queries called cognitive
intelligence (CI) queries. CI queries use the model vectors to enable complex
queries such as semantic matching, inductive reasoning queries such as
analogies, predictive queries using entities not present in a database, and,
more generally, using knowledge from external sources. We demonstrate unique
capabilities of Cognitive Databases using an Apache Spark based prototype to
execute inductive reasoning CI queries over a multi-modal database containing
text and images. We believe our first-of-a-kind system exemplifies using AI
functionality to endow relational databases with capabilities that were
previously very hard to realize in practice.","Cognitive Database: A Step towards Endowing Relational Databases with
  Artificial Intelligence Capabilities",2017
489,0.001136688,0.16573349,0.001136768,0.001136618,0.001136616,0.26802733,0.176816244,0.001136742,0.224283893,0.15945561,Topic6,"Reinforcement learning (RL) is a promising approach to solve dialogue policy
optimisation. Traditional RL algorithms, however, fail to scale to large
domains due to the curse of dimensionality. We propose a novel Dialogue
Management architecture, based on Feudal RL, which decomposes the decision into
two steps; a first step where a master policy selects a subset of primitive
actions, and a second step where a primitive action is chosen from the selected
subset. The structural information included in the domain ontology is used to
abstract the dialogue state space, taking the decisions at each step using
different parts of the abstracted state. This, combined with an information
sharing mechanism between slots, increases the scalability to large domains. We
show that an implementation of this approach, based on Deep-Q Networks,
significantly outperforms previous state of the art in several dialogue domains
and environments, without the need of any additional reward signal.",Feudal Reinforcement Learning for Dialogue Management in Large Domains,2018
490,0.001754944,0.001754589,0.001754635,0.00175466,0.001754664,0.001754611,0.001754669,0.001754685,0.170908709,0.815053834,Topic10,"Many of the leading approaches in language modeling introduce novel, complex
and specialized architectures. We take existing state-of-the-art word level
language models based on LSTMs and QRNNs and extend them to both larger
vocabularies as well as character-level granularity. When properly tuned, LSTMs
and QRNNs achieve state-of-the-art results on character-level (Penn Treebank,
enwik8) and word-level (WikiText-103) datasets, respectively. Results are
obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single
modern GPU.",An Analysis of Neural Language Modeling at Multiple Scales,2018
491,0.001515436,0.001515313,0.001515645,0.328875537,0.001515367,0.001515418,0.040205722,0.001515529,0.330253663,0.29157237,Topic9,"We propose a spatial diffuseness feature for deep neural network (DNN)-based
automatic speech recognition to improve recognition accuracy in reverberant and
noisy environments. The feature is computed in real-time from multiple
microphone signals without requiring knowledge or estimation of the direction
of arrival, and represents the relative amount of diffuse noise in each time
and frequency bin. It is shown that using the diffuseness feature as an
additional input to a DNN-based acoustic model leads to a reduced word error
rate for the REVERB challenge corpus, both compared to logmelspec features
extracted from noisy signals, and features enhanced by spectral subtraction.","Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy
  and Reverberant Environments",2014
492,0.001111325,0.001111224,0.001111202,0.001111212,0.001111233,0.001111284,0.001111217,0.001111199,0.265931808,0.725178297,Topic10,"We describe a simple neural language model that relies only on
character-level inputs. Predictions are still made at the word-level. Our model
employs a convolutional neural network (CNN) and a highway network over
characters, whose output is given to a long short-term memory (LSTM) recurrent
neural network language model (RNN-LM). On the English Penn Treebank the model
is on par with the existing state-of-the-art despite having 60% fewer
parameters. On languages with rich morphology (Arabic, Czech, French, German,
Spanish, Russian), the model outperforms word-level/morpheme-level LSTM
baselines, again with fewer parameters. The results suggest that on many
languages, character inputs are sufficient for language modeling. Analysis of
word representations obtained from the character composition part of the model
reveals that the model is able to encode, from characters only, both semantic
and orthographic information.",Character-Aware Neural Language Models,2015
493,0.000917662,0.00091766,0.000917613,0.000917622,0.103167699,0.000917717,0.000917745,0.000917705,0.058059896,0.832348681,Topic10,"The quality of machine translation is rapidly evolving. Today one can find
several machine translation systems on the web that provide reasonable
translations, although the systems are not perfect. In some specific domains,
the quality may decrease. A recently proposed approach to this domain is neural
machine translation. It aims at building a jointly-tuned single neural network
that maximizes translation performance, a very different approach from
traditional statistical machine translation. Recently proposed neural machine
translation models often belong to the encoder-decoder family in which a source
sentence is encoded into a fixed length vector that is, in turn, decoded to
generate a translation. The present research examines the effects of different
training methods on a Polish-English Machine Translation system used for
medical data. The European Medicines Agency parallel text corpus was used as
the basis for training of neural and statistical network-based translation
systems. The main machine translation evaluation metrics have also been used in
analysis of the systems. A comparison and implementation of a real-time medical
translator is the main focus of our experiments.","Neural-based machine translation for medical text domain. Based on
  European Medicines Agency leaflet texts",2015
494,0.001163093,0.112298882,0.001163057,0.001163017,0.001163112,0.107723636,0.001163163,0.00116311,0.503775987,0.269222943,Topic9,"Recently a variety of LSTM-based conditional language models (LM) have been
applied across a range of language generation tasks. In this work we study
various model architectures and different ways to represent and aggregate the
source information in an end-to-end neural dialogue system framework. A method
called snapshot learning is also proposed to facilitate learning from
supervised sequential signals by applying a companion cross-entropy objective
function to the conditioning vector. The experimental and analytical results
demonstrate firstly that competition occurs between the conditioning vector and
the LM, and the differing architectures provide different trade-offs between
the two. Secondly, the discriminative power and transparency of the
conditioning vector is key to providing both model interpretability and better
performance. Thirdly, snapshot learning leads to consistent performance
improvements independent of which architecture is used.",Conditional Generation and Snapshot Learning in Neural Dialogue Systems,2016
495,0.000909351,0.127973354,0.000909253,0.192877497,0.000909285,0.000909321,0.000909253,0.000909377,0.11048737,0.563205939,Topic10,"In an end-to-end dialog system, the aim of dialog state tracking is to
accurately estimate a compact representation of the current dialog status from
a sequence of noisy observations produced by the speech recognition and the
natural language understanding modules. This paper introduces a novel method of
dialog state tracking based on the general paradigm of machine reading and
proposes to solve it using an End-to-End Memory Network, MemN2N, a
memory-enhanced neural network architecture. We evaluate the proposed approach
on the second Dialog State Tracking Challenge (DSTC-2) dataset. The corpus has
been converted for the occasion in order to frame the hidden state variable
inference as a question-answering task based on a sequence of utterances
extracted from a dialog. We show that the proposed tracker gives encouraging
results. Then, we propose to extend the DSTC-2 dataset with specific reasoning
capabilities requirement like counting, list maintenance, yes-no question
answering and indefinite knowledge management. Finally, we present encouraging
results using our proposed MemN2N based tracking model.","Dialog state tracking, a machine reading approach using Memory Network",2016
496,0.000885188,0.091928048,0.000885199,0.207912379,0.301009396,0.000885276,0.000885298,0.11959206,0.000885159,0.275131997,Topic5,"In accessibility tests for digital preservation, over time we experience
drifts of localized and labelled content in statistical models of evolving
semantics represented as a vector field. This articulates the need to detect,
measure, interpret and model outcomes of knowledge dynamics. To this end we
employ a high-performance machine learning algorithm for the training of
extremely large emergent self-organizing maps for exploratory data analysis.
The working hypothesis we present here is that the dynamics of semantic drifts
can be modeled on a relaxed version of Newtonian mechanics called social
mechanics. By using term distances as a measure of semantic relatedness vs.
their PageRank values indicating social importance and applied as variable
`term mass', gravitation as a metaphor to express changes in the semantic
content of a vector field lends a new perspective for experimentation. From
`term gravitation' over time, one can compute its generating potential whose
fluctuations manifest modifications in pairwise term similarity vs. social
importance, thereby updating Osgood's semantic differential. The dataset
examined is the public catalog metadata of Tate Galleries, London.",A Physical Metaphor to Study Semantic Drift,2016
497,0.001010283,0.001010328,0.001010262,0.001010295,0.001010407,0.224193889,0.170042763,0.001010354,0.416559608,0.18314181,Topic9,"Systems based on artificial neural networks (ANNs) have achieved
state-of-the-art results in many natural language processing tasks. Although
ANNs do not require manually engineered features, ANNs have many
hyperparameters to be optimized. The choice of hyperparameters significantly
impacts models' performances. However, the ANN hyperparameters are typically
chosen by manual, grid, or random search, which either requires expert
experiences or is computationally expensive. Recent approaches based on
Bayesian optimization using Gaussian processes (GPs) is a more systematic way
to automatically pinpoint optimal or near-optimal machine learning
hyperparameters. Using a previously published ANN model yielding
state-of-the-art results for dialog act classification, we demonstrate that
optimizing hyperparameters using GP further improves the results, and reduces
the computational time by a factor of 4 compared to a random search. Therefore
it is a useful technique for tuning ANN models to yield the best performances
for natural language processing tasks.","Optimizing Neural Network Hyperparameters with Gaussian Processes for
  Dialog Act Classification",2016
498,0.000833524,0.000833495,0.000833405,0.000833483,0.000833503,0.000833478,0.000833437,0.160491583,0.000833553,0.832840539,Topic10,"Speech Translation has always been about giving source text or audio input
and waiting for system to give translated output in desired form. In this
paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice
ear-piece translation device. We introduce and survey the recent advances made
in the field of Speech Engineering, to employ in the ADD, particularly focusing
on the three major processing steps of Recognition, Translation and Synthesis.
We tackle the problem of machine understanding of natural language by designing
a recognition unit for source audio to text, a translation unit for source
language text to target language text, and a synthesis unit for target language
text to target language speech. Speech from the surroundings will be recorded
by the recognition unit present on the ear-piece and translation will start as
soon as one sentence is successfully read. This way, we hope to give translated
output as and when input is being read. The recognition unit will use Hidden
Markov Models (HMMs) Based Tool-Kit (HTK), hybrid RNN systems with gated memory
cells, and the synthesis unit, HMM based speech synthesis system HTS. This
system will initially be built as an English to Tamil translation device.",A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder,2016
499,0.001818607,0.062294694,0.001818524,0.001818595,0.001818702,0.405466492,0.241514001,0.001818623,0.001818733,0.279813029,Topic6,"Multi-hop inference is necessary for machine learning systems to successfully
solve tasks such as Recognising Textual Entailment and Machine Reading. In this
work, we demonstrate the effectiveness of adaptive computation for learning the
number of inference steps required for examples of different complexity and
that learning the correct number of inference steps is difficult. We introduce
the first model involving Adaptive Computation Time which provides a small
performance benefit on top of a similar model without an adaptive component as
well as enabling considerable insight into the reasoning process of the model.",Learning to Reason With Adaptive Computation,2016
