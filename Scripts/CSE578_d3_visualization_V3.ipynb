import pickle
import time
import numpy as np
import pandas as pd
import os
import sys
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from nltk.corpus import stopwords
from pylab import *
from bs4 import BeautifulSoup
import random
from nltk.stem.porter import PorterStemmer
import re
import nltk
from nltk.corpus import stopwords 
import string
from nltk.tag import pos_tag
from sklearn import metrics
from textblob import TextBlob as tb
from collections import OrderedDict

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix, classification_report, f1_score
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

d=OrderedDict()
#d={}
text_blob=[]



###############################################################################
def tf(word, blob):
    return blob.words.count(word) / len(blob.noun_phrases)#.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob.noun_phrases)#.words)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))

def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)

def tokenize(text):
    text= str(text).lower()    
    text = text.replace("'", "")          
    text = re.sub('(\d{1,4})[/.-:](\d{1,2})[/.-:](\d{0,4})', ' ', text).strip()      
    text = re.sub(r"\b[-+]?[\d.]+\b", ' ', text)    
    text = re.sub('[^A-Za-z0-9]+', ' ', text)        
#     words = [word for word in nltk.word_tokenize(text)] #for sent in nltk.sent_tokenize(text)      
#     words = [w for w in words if w not in stop_words and len(w)>=2 ]          
#     words = [stemmer.stem(i) for i in words]   
    return text#words
###############################################################################

def build_LDA_Model(tfs):
    lda_model = LatentDirichletAllocation(n_topics=8,               # Number of topics
                                          max_iter=10,               
    # Max learning iterations
                                          learning_method='online',   
                                          random_state=100,          
    # Random state
                                          batch_size=128,            
    # n docs in each learning iter
                                          evaluate_every = -1,       
    # compute perplexity every n iters, default: Don't
                                          n_jobs = -1,               
    # Use all available CPUs
                                         )
    lda_output = lda_model.fit_transform(tfs)
    print(lda_model)  # Model attributes
    
    #Analyzed model performance with per
    

def classify_doc_topics(docnames,tfs, feature_names,best_lda_model):
    # Create Document — Topic Matrix
    lda_output = best_lda_model.transform(tfs)
    # column names
    topicnames = ["Topic" + str(i) for i in range(best_lda_model.n_topics)]
    # index names
#     docnames = ["Doc" + str(i) for i in range(len(data))]
    # Make the pandas dataframe
    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)
    # Get dominant topic for each document
    dominant_topic = np.argmax(df_document_topic.values, axis=1)
    df_document_topic['dominant_topic'] = dominant_topic
    
    print('df_document_topics: ',df_document_topic)

    # Topic-Keyword Matrix
    df_topic_keywords = pd.DataFrame(best_lda_model.components_)
    # Assign Column and Index
    df_topic_keywords.columns = feature_names
    df_topic_keywords.index = topicnames
    
    df_document_topic.to_csv('doc_topic.csv')
    df_topic_keywords.to_csv('topic_keywords.csv')
    # View
    print(df_topic_keywords.head())
    
    return df_document_topic,topicnames,lda_output    

def Gridserachcv_LDA(tfs):
    # Define Search Param
    search_params = {'n_topics': [8], 'learning_decay': [0.2,0.3,.4,.5,.6, .7, .9]}#3,5,6,7,8
    # Init the Model
    lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)
    # Init Grid Search Class
    model = GridSearchCV(lda, param_grid=search_params)
    # Do the Grid Search
    model.fit(tfs)
#     GridSearchCV(cv=None, error_score='raise',
#            estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,
#                  evaluate_every=-1, learning_decay=0.7, learning_method=None,
#                  learning_offset=10.0, max_doc_update_iter=100, max_iter=10,
#                  mean_change_tol=0.001, n_topics=10, n_jobs=1,
# #                  n_topics=None, perp_tol=0.1, random_state=None,
#                  topic_word_prior=None, total_samples=1000000.0, verbose=0),
#            fit_params=None, iid=True, n_jobs=1,
#            param_grid={'n_topics': [2,3,45,6,10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},
#            pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
#            scoring=None, verbose=0)
    
    # Best Model
    best_lda_model = model.best_estimator_
    # Model Parameters
    print("Best Model's Params: ", model.best_params_)
    # Log Likelihood Score
    print("Best Log Likelihood Score: ", model.best_score_)
    # Perplexity
    print("Model Perplexity: ", best_lda_model.perplexity(tfs))
    return best_lda_model




def text_blob_process(corpus_lst,df_result):
    # print('corpus_lst: ', corpus_lst)
    for i, blob in enumerate(corpus_lst):
        print("Top words in document {}".format(i + 1))
    #     for word in blob.noun_phrases:#words:
    #         print(word)
        print(type(blob))
        scores = {word: tfidf(word, (blob), corpus_lst) for word in (blob).noun_phrases}#.words}
        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        for word, score in sorted_words[:15]:
#             d[(i,word)]=score  #Kalpana
            d[(df_result.iloc[i]['label'],word)]=score
            text_blob.append(word)
            #print("\tWord: {}, TF-IDF: {}".format(word, round(score, 15)))
    return text_blob,d

# function to remove stopwords
def remove_stopwords(text):
    no_stopword_text = [w for w in text.split() if not w in stop_words]
    return ' '.join(no_stopword_text)

def tb_conversion_df(d):
    #Kalpana
    print('Dictionary start: ')
    lst_colname=[]
    lst_idx=[]
    lst_colname.append('doc')
    for key, value in d.items():
        lst_colname.append(key[1])
        lst_idx.append(key[0])
        print (key[0], " : " , key[1], " : ", value)     
    
    df = pd.DataFrame(columns=list(set(lst_colname)), index= lst_idx)
    i=0
    for key, value in d.items():    
        df.iloc[i][key[1]] = value    
        i = i + 1
    df=df.fillna(0)  
    return df
    
def svm_model(df_result, text_blob):
    
    tfidf_vectorizer = TfidfVectorizer(vocabulary = list(set(text_blob)),ngram_range=(1, 5))#,tokenizer=tokenize, stop_words=stop_words
    X = tfidf_vectorizer.fit_transform(df_result['text_clean'])
    y = df_result['label']
    
    ovr = OneVsRestClassifier(LinearSVC(random_state=0,C=0.6)).fit(X, y)
    distance_to_decision_boundary = ovr.decision_function(X)
    print('X: ',list(distance_to_decision_boundary))
    df_lst = pd.DataFrame(distance_to_decision_boundary) #, columns=['Label0','Label1','Label2','Label3']
    df_lst.to_csv('svm_distance.csv')
    print(df_lst)
#     print('y: ',y)

    prediction=ovr.predict(X)
    df_result['prediction'] = prediction
    df_result.to_csv('df_result_svm_v2.csv')
    # model accuracy for X_test   
    accuracy = ovr.score(X, y) 
    print('accuracy: ', accuracy)  
    # creating a confusion matrix 
    cm = confusion_matrix(y, prediction) 
    print(cm)
    
def gridsearch_svm_model(df_result, text_blob):
    
    print('text_blob: ', text_blob)
#     df_result['text_clean'] = df_result['sentiment'].apply(lambda x: remove_stopwords(x))
    tfidf_vectorizer = TfidfVectorizer(vocabulary = list(set(text_blob)),ngram_range=(1, 5))#,tokenizer=tokenize, stop_words=stop_words
    xtrain_tfidf = tfidf_vectorizer.fit_transform(df_result['text_clean'])
    ytrain = df_result['label']
    
    svm_model(xtrain_tfidf, ytrain, df_result)
    print('ytrain: ', ytrain)
    parameters = {'estimator__kernel':('linear', 'rbf', 'poly'), 'estimator__C':[0.1,0.01,0.001,0.2,0.02,0.3,0.03,0.4,0.04,0.5,0.05,0.6,0.7,0.8, 1]}
    svr = OneVsRestClassifier(SVC(probability=True))
    clf = GridSearchCV(svr, parameters)
    clf.fit(xtrain_tfidf,ytrain)
    
    prediction=clf.predict(xtrain_tfidf)
    print('prediction: ', prediction)
    df_result['prediction'] = prediction
    df_result.to_csv('D:/df_result.csv')
    # model accuracy for X_test   
    accuracy = clf.score(xtrain_tfidf, ytrain) 
    print('accuracy: ', accuracy)  
    # creating a confusion matrix 
    cm = confusion_matrix(ytrain, prediction) 
    print(cm)
    
    distance_to_decision_boundary = clf.decision_function(xtrain_tfidf)
    print('X: ',list(distance_to_decision_boundary))
    df_lst = pd.DataFrame(distance_to_decision_boundary)#, columns=['Label0','Label1','Label2','Label3']
    print('distance: ',df_lst)
#     print('y: ',y)

#     prediction=clf.predict(corpus_lst)
#     print('prediction: ',prediction)
    print('best_param: ',clf.best_params_) 

#     # print how our model looks after hyper-parameter tuning 
#     print(clf.best_score_) 

#     # print classification report 
#     print(classification_report(y, prediction)) 

file_name=[]
corpus_lst=[]
def read_dir_into_df_txt(fpath):
    
    dirListing = os.listdir(fpath)
    bloblist = []
    stop_words = set(stopwords.words('english')) 
    simi_result=[]
    df=pd.DataFrame()
    corpus = []
    for item in dirListing:
        if ".txt" in item:
            text_file = (fpath+'\\'+item)
            file_name.append(item)
            corp_txt=''
            text1=''
            with open (text_file) as f:
                corp_txt = f.read()
                
                #my1 = [word for word in tokenize(f.read()) if word not in stopwords.words('english')]
                text1 = ' '.join(corp_txt)
                corpus.append(f.read() ) #kalpana                
#                 corpus_lst.append(f.read())
                f.close()
            print('corp_txt: ',corp_txt)
    df_text = pd.DataFrame(corpus,columns=['text']) 
    df_text['doc_name'] = list(file_name)
    df_text.set_index('doc_name', inplace=True)
    print(file_name)
    return df_text,file_name, corpus_lst


def sentiment_calc(text):
    try:
#         text = tokenize(text)
        return tb(text)#.sentiment
    except:
        return None




def read_dir_into_df_csv(filepath):
    df_csv = pd.read_csv(filepath+"food_reviews.csv")
    
    df_csv['text_clean'] = df_csv['text'].apply(lambda x: remove_stopwords(x))
    df_csv['sentiment'] = df_csv['text_clean'].apply(sentiment_calc)
    corpus_lst=df_csv['sentiment']
    return df_csv,corpus_lst

def main():
    file_path='D:/Kalpana_II/food_reviews/'
#     df_result,file_name,corpus_lst = read_dir_into_df_txt(file_path)
    df_result,corpus_lst = read_dir_into_df_csv(file_path)
    
#     print(df_result)
#     corpus_lst=[document1,document2,document3,document4]
    text_blob,d = text_blob_process(corpus_lst,df_result)
    df = tb_conversion_df(d)  
    df.to_csv(file_path+'df_result_hmm.csv')
#     build_LDA_Model(df)
    best_lda_model = Gridserachcv_LDA(df)
    doc_topic,topicnames,lda_output = classify_doc_topics(df.index,df, df.columns, best_lda_model)
#     print('doc_topic: ', list(doc_topic))
#     print('lda_output: ', lda_output)
#     x = df#lda_output
#     print(x.shape)
#     print('lda_output: ', lda_output.shape)
#     y = df.index #df_result['label']#
#     print(y.shape)
    
    
#     gridsearch_svm_model(df_result, text_blob)
    svm_model(df_result, text_blob)
#     svm_model(x,y)
    
main()  

