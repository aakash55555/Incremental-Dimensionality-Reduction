{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "['all-exchanges-strings.lc.txt', 'all-orgs-strings.lc.txt', 'all-people-strings.lc.txt', 'all-places-strings.lc.txt', 'all-topics-strings.lc.txt', 'cat-descriptions_120396.txt', 'feldman-cia-worldfactbook-data.txt']\n",
      "                                                                                 text\n",
      "doc_name                                                                             \n",
      "all-exchanges-strings.lc.txt        amex\\nase\\nasx\\nbiffex\\nbse\\ncboe\\ncbt\\ncme\\nc...\n",
      "all-orgs-strings.lc.txt             adb-africa\\nadb-asia\\naibd\\naid\\nanrpc\\nasean\\...\n",
      "all-people-strings.lc.txt           abdel-hadi-kandeel\\nalfonsin\\nalhaji-abdul-ahm...\n",
      "all-places-strings.lc.txt           afghanistan\\nalbania \\nalgeria \\namerican-samo...\n",
      "all-topics-strings.lc.txt           acq\\nalum\\naustdlr\\naustral\\nbarley\\nbfr\\nbop\\...\n",
      "cat-descriptions_120396.txt         \\nSome notes on the Reuters Categories\\nDavid ...\n",
      "feldman-cia-worldfactbook-data.txt  This is a set of Prolog assertions for facts a...\n",
      "                                       ababa      abal     abdel     abdul  \\\n",
      "doc_name                                                                     \n",
      "all-exchanges-strings.lc.txt        0.000000  0.000000  0.000000  0.000000   \n",
      "all-orgs-strings.lc.txt             0.000000  0.000000  0.000000  0.000000   \n",
      "all-people-strings.lc.txt           0.000000  0.051188  0.051188  0.051188   \n",
      "all-places-strings.lc.txt           0.000000  0.000000  0.000000  0.000000   \n",
      "all-topics-strings.lc.txt           0.000000  0.000000  0.000000  0.000000   \n",
      "cat-descriptions_120396.txt         0.000000  0.007276  0.007276  0.007276   \n",
      "feldman-cia-worldfactbook-data.txt  0.000336  0.000000  0.000000  0.000000   \n",
      "\n",
      "                                    abdulaziz     abeda      abel       abl  \\\n",
      "doc_name                                                                      \n",
      "all-exchanges-strings.lc.txt         0.000000  0.000000  0.000000  0.000000   \n",
      "all-orgs-strings.lc.txt              0.000000  0.000000  0.000000  0.000000   \n",
      "all-people-strings.lc.txt            0.000000  0.000000  0.000000  0.000000   \n",
      "all-places-strings.lc.txt            0.000000  0.000000  0.000000  0.000000   \n",
      "all-topics-strings.lc.txt            0.000000  0.000000  0.000000  0.000000   \n",
      "cat-descriptions_120396.txt          0.004383  0.000000  0.004383  0.000000   \n",
      "feldman-cia-worldfactbook-data.txt   0.000000  0.005708  0.000000  0.000336   \n",
      "\n",
      "                                         abu     abuja  ...      zhao  \\\n",
      "doc_name                                                ...             \n",
      "all-exchanges-strings.lc.txt        0.000000  0.000000  ...  0.000000   \n",
      "all-orgs-strings.lc.txt             0.000000  0.000000  ...  0.000000   \n",
      "all-people-strings.lc.txt           0.000000  0.000000  ...  0.051188   \n",
      "all-places-strings.lc.txt           0.000000  0.000000  ...  0.000000   \n",
      "all-topics-strings.lc.txt           0.000000  0.000000  ...  0.000000   \n",
      "cat-descriptions_120396.txt         0.000000  0.000000  ...  0.007276   \n",
      "feldman-cia-worldfactbook-data.txt  0.000336  0.000336  ...  0.000000   \n",
      "\n",
      "                                       zheng       zia  zilberstein   zimbabw  \\\n",
      "doc_name                                                                        \n",
      "all-exchanges-strings.lc.txt        0.000000  0.000000     0.000000  0.000000   \n",
      "all-orgs-strings.lc.txt             0.000000  0.000000     0.000000  0.000000   \n",
      "all-people-strings.lc.txt           0.051188  0.051188     0.000000  0.000000   \n",
      "all-places-strings.lc.txt           0.000000  0.000000     0.000000  0.061550   \n",
      "all-topics-strings.lc.txt           0.000000  0.000000     0.000000  0.000000   \n",
      "cat-descriptions_120396.txt         0.007276  0.007276     0.000000  0.003110   \n",
      "feldman-cia-worldfactbook-data.txt  0.000000  0.000000     0.000336  0.003812   \n",
      "\n",
      "                                        zinc    ziyang      zone       zse  \\\n",
      "doc_name                                                                     \n",
      "all-exchanges-strings.lc.txt        0.000000  0.000000  0.000000  0.162799   \n",
      "all-orgs-strings.lc.txt             0.000000  0.000000  0.000000  0.000000   \n",
      "all-people-strings.lc.txt           0.000000  0.051188  0.000000  0.000000   \n",
      "all-places-strings.lc.txt           0.000000  0.000000  0.000000  0.000000   \n",
      "all-topics-strings.lc.txt           0.046211  0.000000  0.000000  0.000000   \n",
      "cat-descriptions_120396.txt         0.006219  0.007276  0.000000  0.003638   \n",
      "feldman-cia-worldfactbook-data.txt  0.012388  0.000000  0.001007  0.000000   \n",
      "\n",
      "                                      zurich  \n",
      "doc_name                                      \n",
      "all-exchanges-strings.lc.txt        0.000000  \n",
      "all-orgs-strings.lc.txt             0.000000  \n",
      "all-people-strings.lc.txt           0.000000  \n",
      "all-places-strings.lc.txt           0.000000  \n",
      "all-topics-strings.lc.txt           0.000000  \n",
      "cat-descriptions_120396.txt         0.004383  \n",
      "feldman-cia-worldfactbook-data.txt  0.000000  \n",
      "\n",
      "[7 rows x 2800 columns]\n",
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_jobs=-1, n_topics=5, perp_tol=0.1, random_state=100,\n",
      "             topic_word_prior=None, total_samples=1000000.0, verbose=0)\n",
      "Log Likelihood:  -2539.8484776458417\n",
      "Perplexity:  2246373586795096.0\n",
      "{'n_jobs': -1, 'max_doc_update_iter': 100, 'max_iter': 10, 'total_samples': 1000000.0, 'learning_method': 'online', 'random_state': 100, 'perp_tol': 0.1, 'mean_change_tol': 0.001, 'learning_decay': 0.7, 'topic_word_prior': None, 'n_topics': 5, 'batch_size': 128, 'doc_topic_prior': None, 'learning_offset': 10.0, 'verbose': 0, 'evaluate_every': -1}\n",
      "Best Model's Params:  {'learning_decay': 0.5, 'n_topics': 3}\n",
      "Best Log Likelihood Score:  -1382.4502084665708\n",
      "Model Perplexity:  186892598631.5786\n",
      "           ababa      abal     abdel     abdul  abdulaziz     abeda      abel  \\\n",
      "Topic0  0.739451  0.676809  0.753786  0.611065   0.645405  0.672699  0.687554   \n",
      "Topic1  0.647784  0.665147  0.677093  0.593666   0.647031  0.636980  0.612811   \n",
      "Topic2  0.725677  0.686702  0.645446  0.720652   0.610565  0.689475  0.628211   \n",
      "\n",
      "             abl       abu     abuja  ...      zhao     zheng       zia  \\\n",
      "Topic0  0.656114  0.671767  0.666374  ...  0.715547  0.628891  0.605939   \n",
      "Topic1  0.685528  0.688785  0.749895  ...  0.707701  0.693149  0.637659   \n",
      "Topic2  0.693698  0.653778  0.710864  ...  0.657403  0.670257  0.630251   \n",
      "\n",
      "        zilberstein   zimbabw      zinc    ziyang      zone       zse  \\\n",
      "Topic0     0.623655  0.682447  0.665229  0.566119  0.543577  0.660138   \n",
      "Topic1     0.647923  0.654407  0.696610  0.646847  0.689752  0.685156   \n",
      "Topic2     0.708015  0.730789  0.703741  0.663667  0.689324  0.777753   \n",
      "\n",
      "          zurich  \n",
      "Topic0  0.682396  \n",
      "Topic1  0.690333  \n",
      "Topic2  0.691404  \n",
      "\n",
      "[3 rows x 2800 columns]\n",
      "----------------------\n",
      "<class 'numpy.ndarray'>\n",
      "7\n",
      "3\n",
      "----------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-12c5ae7bb2ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[0mpredictclass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IntelPython35\\Lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \"\"\"\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IntelPython35\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\IntelPython35\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 181\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 3]"
     ]
    }
   ],
   "source": [
    "#https://github.com/chrisjmccormick/LSA_Classification\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import stopwords\n",
    "from pylab import *\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "###############################################################################\n",
    "#  Load the raw text dataset.\n",
    "###############################################################################\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# The raw text dataset is stored as tuple in the form:\n",
    "# (X_train_raw, y_train_raw, X_test_raw, y_test)\n",
    "# The 'filtered' dataset excludes any articles that we failed to retrieve\n",
    "# fingerprints for.\n",
    "file_path='D:/Kalpana_II/reuters21578/'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['around','need','addess','due','goes','go','previous','next','kind','like','week','year','month','led','previously','daily','day',\n",
    "                   '<br>','</br>','<br/>','peter','via','based','base','upon','currently','pm','am','several','also','monday','tuesday',\n",
    "                   'wednesday','thursday','friday','sunday', 'recent', 'since', 'per', 'look', 'see', 'set', 'still', \n",
    "                   'following', 'max'  ,'&nbsp;','&nbsp','nbsp','soon','please','need', ',', '\"', \"'\",  '!', ':', ';',\n",
    "                   '(', ')', '[', ']', '{', '}', '?',',','/','\"','$',\"'d\",\"'s\",\"'ve\",\"+at\",\"+-ep\", \"<\",\">\", \"br\",\".\",\n",
    "                    '*', '***','***', '&', '+','{', '}', '~', '#',\"''\",\"=+\",\"***doe\"]) # remove it if you need punctuation \n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize_org(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text_1=soup.get_text()\n",
    "    text_1=text_1.replace('\\d+', '')\n",
    "    tokens = nltk.word_tokenize(text_1)\n",
    "    html_tag = [i for i in tokens if i  in remove_tags(i) and (len(i)>=3)] \n",
    "    digit_rem=[i for i in html_tag if not i.isdigit()]    \n",
    "    stems = stem_tokens(digit_rem, stemmer)\n",
    "    return stems\n",
    "\n",
    "def tokenize(text):\n",
    "    text= str(text).lower()    \n",
    "    text = text.replace(\"'\", \"\")     \n",
    "#     text = replace_all(text,replace_dict)        \n",
    "    text = re.sub('(\\d{1,4})[/.-:](\\d{1,2})[/.-:](\\d{0,4})', ' ', text).strip()      \n",
    "    text = re.sub(r\"\\b[-+]?[\\d.]+\\b\", ' ', text)    \n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)        \n",
    "#     text = replace_all(text,replace_dict) \n",
    "    words = [word for word in nltk.word_tokenize(text)] #for sent in nltk.sent_tokenize(text)      \n",
    "    words = [w for w in words if w not in stop_words and len(w)>=2 ]          \n",
    "    words = [stemmer.stem(i) for i in words]   \n",
    "    return words\n",
    "\n",
    "def build_LDA_Model(tfs):\n",
    "    lda_model = LatentDirichletAllocation(n_topics=5,               # Number of topics\n",
    "                                          max_iter=10,               \n",
    "    # Max learning iterations\n",
    "                                          learning_method='online',   \n",
    "                                          random_state=100,          \n",
    "    # Random state\n",
    "                                          batch_size=128,            \n",
    "    # n docs in each learning iter\n",
    "                                          evaluate_every = -1,       \n",
    "    # compute perplexity every n iters, default: Don't\n",
    "                                          n_jobs = -1,               \n",
    "    # Use all available CPUs\n",
    "                                         )\n",
    "    lda_output = lda_model.fit_transform(tfs)\n",
    "    print(lda_model)  # Model attributes\n",
    "    \n",
    "    #Analyzed model performance with perplexity & log-likelihood\n",
    "    # Log Likelyhood: Higher the better\n",
    "    print(\"Log Likelihood: \", lda_model.score(tfs))\n",
    "    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "    print(\"Perplexity: \", lda_model.perplexity(tfs))\n",
    "    # See model parameters\n",
    "    print(lda_model.get_params())\n",
    "\n",
    "def Gridserachcv_LDA(tfs):\n",
    "    # Define Search Param\n",
    "    search_params = {'n_topics': [3,5,6], 'learning_decay': [.5, .7, .9]}\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "    # Do the Grid Search\n",
    "    model.fit(tfs)\n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "           estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "                 evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "                 learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "                 mean_change_tol=0.001, n_topics=10, n_jobs=1,\n",
    "#                  n_topics=None, perp_tol=0.1, random_state=None,\n",
    "                 topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "           fit_params=None, iid=True, n_jobs=1,\n",
    "           param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "           pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "           scoring=None, verbose=0)\n",
    "    \n",
    "    # Best Model\n",
    "    best_lda_model = model.best_estimator_\n",
    "    # Model Parameters\n",
    "    print(\"Best Model's Params: \", model.best_params_)\n",
    "    # Log Likelihood Score\n",
    "    print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "    # Perplexity\n",
    "    print(\"Model Perplexity: \", best_lda_model.perplexity(tfs))\n",
    "    return best_lda_model\n",
    "\n",
    "def classify_doc_topics(docnames,tfs, feature_names,best_lda_model):\n",
    "    # Create Document — Topic Matrix\n",
    "    lda_output = best_lda_model.transform(tfs)\n",
    "    # column names\n",
    "    topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_topics)]\n",
    "    # index names\n",
    "#     docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "    # Make the pandas dataframe\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    # Get dominant topic for each document\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic\n",
    "    # Styling\n",
    "    def color_green(val):\n",
    "        color = 'green' if val > .1 else 'black'\n",
    "        return 'color: {col}'.format(col=color)\n",
    "    def make_bold(val):\n",
    "        weight = 700 if val > .1 else 400\n",
    "        return 'font-weight: {weight}'.format(weight=weight)\n",
    "    # Apply Style\n",
    "    df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "    df_document_topics\n",
    "    \n",
    "    # Topic-Keyword Matrix\n",
    "    df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "    # Assign Column and Index\n",
    "    df_topic_keywords.columns = feature_names\n",
    "    df_topic_keywords.index = topicnames\n",
    "    # View\n",
    "    print(df_topic_keywords.head())\n",
    "    \n",
    "    return df_document_topics,topicnames,lda_output\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "\n",
    "\n",
    "file_name=[]\n",
    "def read_dir_into_df(fpath):\n",
    "    \n",
    "    dirListing = os.listdir(fpath)\n",
    "    bloblist = []\n",
    "    \n",
    "    simi_result=[]\n",
    "    df=pd.DataFrame()\n",
    "    corpus = []\n",
    "    for item in dirListing:\n",
    "        if \".txt\" in item:\n",
    "            text_file = (fpath+'\\\\'+item)\n",
    "            file_name.append(item)\n",
    "            with open (text_file) as f:\n",
    "                corpus.append(f.read()) \n",
    "                f.close()\n",
    "    df_text = pd.DataFrame(corpus,columns=['text']) \n",
    "    df_text['doc_name'] = list(file_name)\n",
    "    df_text.set_index('doc_name', inplace=True)\n",
    "    print(file_name)\n",
    "    return df_text,file_name\n",
    "\n",
    "df_result,file_name = read_dir_into_df(file_path)\n",
    "print(df_result)\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words,ngram_range=(1, 1))\n",
    "#feature_names = tfidf.get_feature_names()\n",
    "tfs = tfidf.fit_transform((df_result['text']))#, index=df_result['text'])#, columns=feature_names)\n",
    "# print(tfs)\n",
    "scores = np.asarray(tfs.sum(axis=0)).ravel()\n",
    "scores_lst = [i for i,v in enumerate(scores) if v>0]\n",
    "feature_names = tfidf.get_feature_names()\n",
    "dfFinal = pd.DataFrame(tfs.toarray())\n",
    "\n",
    "a1=(pd.DataFrame(tfs.A, columns=(tfidf.get_feature_names()))) #print matrix\n",
    "a1.index = df_result.index\n",
    "print(a1)\n",
    "\n",
    "\n",
    "#calling LDA model\n",
    "build_LDA_Model(tfs)\n",
    "best_lda_model = Gridserachcv_LDA(tfs)\n",
    "doc_topic,topicnames,lda_output = classify_doc_topics(file_name,tfs, feature_names, best_lda_model)\n",
    "topic_keywords = show_topics(tfidf, best_lda_model, 20)\n",
    "\n",
    "# topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords\n",
    "\n",
    "print('----------------------')\n",
    "print(type(lda_output))#nd_array\n",
    "print(lda_output.shape[0])#7\n",
    "print(lda_output.shape[1])#\n",
    "print('----------------------')\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "# labels=[]\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# sv=MultinomialNB()\n",
    "# # labels=[0]\n",
    "# for i in range(0, 3):\n",
    "#     labels.append(0)\n",
    "# sv.fit(lda_output,labels)\n",
    "# predictclass = sv.predict(lda_output)\n",
    "\n",
    "# testLables=[0]\n",
    "# from sklearn import metrics\n",
    "# yacc=metrics.accuracy_score(testLables,predictclass)\n",
    "# print (yacc)\n",
    "# n=dataset.target[:20]\n",
    "# print (n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Import the dataset\n",
    "dataset = pd.read_csv('LDA_Data.csv')\n",
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Feature Scaling to Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Implement LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 2)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression with LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Results of Regression with LDA\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
