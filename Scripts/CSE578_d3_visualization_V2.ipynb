import pickle
import time
import numpy as np
import pandas as pd
import os
import sys
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from nltk.corpus import stopwords
from pylab import *
from bs4 import BeautifulSoup
import random
from nltk.stem.porter import PorterStemmer
import re
import nltk
from nltk.corpus import stopwords 
import string
from nltk.tag import pos_tag
from sklearn import metrics
from textblob import TextBlob as tb
from collections import OrderedDict

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix, classification_report, f1_score

TAG_RE = re.compile(r'<[^>]+>')
def remove_tags(text):
    return TAG_RE.sub('', text)

token_dict = {}
stemmer = PorterStemmer()



###############################################################################
def tf(word, blob):
    return blob.words.count(word) / len(blob.noun_phrases)#.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob.noun_phrases)#.words)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))

def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)
###############################################################################

###############################################################################
#  Load the raw text dataset.
###############################################################################

print("Loading dataset...")

# The raw text dataset is stored as tuple in the form:
# (X_train_raw, y_train_raw, X_test_raw, y_test)
# The 'filtered' dataset excludes any articles that we failed to retrieve
# fingerprints for.
# file_path='D:/Kalpana_II/reuters21578/'
file_path='D:/Kalpana_II/food_reviews/food_reviews_txt/'

stop_words = set(stopwords.words('english'))
stop_words.update(['around','need','addess','due','goes','go','previous','next','kind','like','week','year','month','led','previously','daily','day',
                   '<br>','</br>','<br/>','peter','via','based','base','upon','currently','pm','am','several','also','monday','tuesday',
                   'wednesday','thursday','friday','sunday', 'recent', 'since', 'per', 'look', 'see', 'set', 'still', 
                   'following', 'max'  ,'&nbsp;','&nbsp','nbsp','soon','please','need', ',', '"', "'",  '!', ':', ';',
                   '(', ')', '[', ']', '{', '}', '?',',','/','"','$',"'d","'s","'ve","+at","+-ep", "<",">", "br",".",
                    '*', '***','***', '&', '+','{', '}', '~', '#',"''","=+","***doe"]) # remove it if you need punctuation 
def stem_tokens(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

def tokenize_org(text):
    soup = BeautifulSoup(text)
    text_1=soup.get_text()
    text_1=text_1.replace('\d+', '')
    tokens = nltk.word_tokenize(text_1)
    html_tag = [i for i in tokens if i  in remove_tags(i) and (len(i)>=3)] 
    digit_rem=[i for i in html_tag if not i.isdigit()]    
    stems = stem_tokens(digit_rem, stemmer)
    return stems

def tokenize(text):
    text= str(text).lower()    
    text = text.replace("'", "")     
#     text = replace_all(text,replace_dict)        
    text = re.sub('(\d{1,4})[/.-:](\d{1,2})[/.-:](\d{0,4})', ' ', text).strip()      
    text = re.sub(r"\b[-+]?[\d.]+\b", ' ', text)    
    text = re.sub('[^A-Za-z0-9]+', ' ', text)        
#     text = replace_all(text,replace_dict) 
    words = [word for word in nltk.word_tokenize(text)] #for sent in nltk.sent_tokenize(text)      
    words = [w for w in words if w not in stop_words and len(w)>=2 ]          
    words = [stemmer.stem(i) for i in words]   
    return words

def build_LDA_Model(tfs):
    lda_model = LatentDirichletAllocation(n_topics=5,               # Number of topics
                                          max_iter=10,               
    # Max learning iterations
                                          learning_method='online',   
                                          random_state=100,          
    # Random state
                                          batch_size=128,            
    # n docs in each learning iter
                                          evaluate_every = -1,       
    # compute perplexity every n iters, default: Don't
                                          n_jobs = -1,               
    # Use all available CPUs
                                         )
    lda_output = lda_model.fit_transform(tfs)
    print(lda_model)  # Model attributes
    
    #Analyzed model performance with perplexity & log-likelihood
    # Log Likelyhood: Higher the better
    print("Log Likelihood: ", lda_model.score(tfs))
    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)
    print("Perplexity: ", lda_model.perplexity(tfs))
    # See model parameters
    print(lda_model.get_params())

def Gridserachcv_LDA(tfs):
    # Define Search Param
    search_params = {'n_topics': [3,5,6], 'learning_decay': [.5, .7, .9]}
    # Init the Model
    lda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50.,random_state=0)
    # Init Grid Search Class
    model = GridSearchCV(lda, param_grid=search_params)
    # Do the Grid Search
    model.fit(tfs)
    GridSearchCV(cv=None, error_score='raise',
           estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,
                 evaluate_every=-1, learning_decay=0.7, learning_method=None,
                 learning_offset=10.0, max_doc_update_iter=100, max_iter=10,
                 mean_change_tol=0.001, n_topics=10, n_jobs=1,
#                  n_topics=None, perp_tol=0.1, random_state=None,
                 topic_word_prior=None, total_samples=1000000.0, verbose=0),
           fit_params=None, iid=True, n_jobs=1,
           param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},
           pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
           scoring=None, verbose=0)
    
    # Best Model
    best_lda_model = model.best_estimator_
    # Model Parameters
    print("Best Model's Params: ", model.best_params_)
    # Log Likelihood Score
    print("Best Log Likelihood Score: ", model.best_score_)
    # Perplexity
    print("Model Perplexity: ", best_lda_model.perplexity(tfs))
    return best_lda_model

def classify_doc_topics(docnames,tfs, feature_names,best_lda_model):
    # Create Document â€” Topic Matrix
    lda_output = best_lda_model.transform(tfs)
    # column names
    topicnames = ["Topic" + str(i) for i in range(best_lda_model.n_topics)]
    # index names
#     docnames = ["Doc" + str(i) for i in range(len(data))]
    # Make the pandas dataframe
    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)
    # Get dominant topic for each document
    dominant_topic = np.argmax(df_document_topic.values, axis=1)
    df_document_topic['dominant_topic'] = dominant_topic
    
    print('df_document_topics: ',df_document_topic)
    # Styling
    def color_green(val):
        color = 'green' if val > .1 else 'black'
        return 'color: {col}'.format(col=color)
    def make_bold(val):
        weight = 700 if val > .1 else 400
        return 'font-weight: {weight}'.format(weight=weight)
    # Apply Style
    df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)
    df_document_topics
    
    # Topic-Keyword Matrix
    df_topic_keywords = pd.DataFrame(best_lda_model.components_)
    # Assign Column and Index
    df_topic_keywords.columns = feature_names
    df_topic_keywords.index = topicnames
    # View
    print(df_topic_keywords.head())
    
    return df_document_topic,topicnames,lda_output

# Show top n keywords for each topic
def show_topics(vectorizer, lda_model, n_words):
    keywords = np.array(vectorizer.get_feature_names())
    topic_keywords = []
    for topic_weights in lda_model.components_:
        top_keyword_locs = (-topic_weights).argsort()[:n_words]
        topic_keywords.append(keywords.take(top_keyword_locs))
    return topic_keywords



file_name=[]
corpus_lst=[]
file_idx=[]
def read_dir_into_df(fpath):
    
    dirListing = os.listdir(fpath)
    bloblist = []
    stop_words = set(stopwords.words('english')) 
    simi_result=[]
    df=pd.DataFrame()
    corpus = []
    i = 0
    for item in dirListing:
        if ".txt" in item:
            text_file = (fpath+'\\'+item)
            file_name.append(item)
            file_idx.append(i)
            i = i + 1
            with open (text_file) as f:
                #my1 = [word for word in tokenize(f.read()) if word not in stopwords.words('english')]
                #text1 = ' '.join(my1)
                corpus.append(f.read() ) #kalpana
#                 corpus_lst.append(f.read())
                f.close()
        
    df_text = pd.DataFrame(corpus,columns=['text']) 
    df_text['doc_name'] = list(file_name)
    df_text.set_index('doc_name', inplace=True)
    print(file_name)
    return df_text,file_name,file_idx

df_result,file_name,file_idx = read_dir_into_df(file_path)
print(df_result)

text_blob = []

lst_word=[]
lst_doc=[]
lst_score=[]
# from collections import defaultdict
d={}
#kalpana
for idx, row in df_result.iterrows():
  
    corpus_lst.append(tb(row['text']))
# print('corpus_lst: ', corpus_lst)
for i, blob in enumerate(corpus_lst):
    print("Top words in document {}".format(i + 1))
#     for word in blob.noun_phrases:#words:
#         print(word)
    scores = {word: tfidf(word, (blob), corpus_lst) for word in (blob).noun_phrases}#.words}
    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    for word, score in sorted_words[:10]:
        d[(i,word)]=score
        text_blob.append(word)
        print("\tWord: {}, TF-IDF: {}".format(word, round(score, 5)))

#Kalpana
print('Dictionary start: ')
for key, value in d.items():
    print (key[0], " : " , key[1], " : ", value)
print('Dictionary end: ')
# print('kalpana_df: ',kalpana_df)
# print('text_blob: ',text_blob)

tfidf = TfidfVectorizer(vocabulary = list(set(text_blob)),tokenizer=tokenize, stop_words=stop_words,ngram_range=(1, 4))
#feature_names = tfidf.get_feature_names()
tfs = tfidf.fit_transform((df_result['text']))#, index=df_result['text'])#, columns=feature_names)
print('tfs: ',tfs)
scores = np.asarray(tfs.sum(axis=0)).ravel()
scores_lst = [i for i,v in enumerate(scores) if v>0]
feature_names = tfidf.get_feature_names()
dfFinal = pd.DataFrame(tfs.toarray())
print('dfFinal: ',dfFinal)
a1=(pd.DataFrame(tfs.A, columns=(tfidf.get_feature_names()))) #print matrix
a1.index = df_result.index
print("a1: ",a1)


#calling LDA model
build_LDA_Model(tfs)
best_lda_model = Gridserachcv_LDA(tfs)
doc_topic,topicnames,lda_output = classify_doc_topics(file_name,tfs, feature_names, best_lda_model)
print('doc_topic: ', list(doc_topic))
doc_topic.to_csv('doc_topic.csv')
topic_keywords = show_topics(tfidf, best_lda_model, 50)


# topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)
# Topic - Keywords Dataframe
df_topic_keywords = pd.DataFrame(topic_keywords)
df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]
df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]
df_topic_keywords
df_topic_keywords.to_csv('df_topic_keywords.csv')

print('----------------------')
print(type(lda_output))#nd_array
print('0-output: ',doc_topic)#7
print('1-output: ',lda_output.shape[1])#
print('----------------------')

def gridsearch_svm_model(X,y):
    parameters = {'estimator__kernel':('linear', 'rbf', 'poly'), 'estimator__C':[0.1,0.01,0.001,0.2,0.02,0.3,0.03,0.4,0.04,0.5,0.05,0.6,0.7,0.8, 1]}
    svr = OneVsRestClassifier(SVC(probability=True))
    clf = GridSearchCV(svr, parameters)
    clf.fit(X,y)
    
    prediction=clf.predict(X)
    print('prediction: ', prediction)
    # model accuracy for X_test   
    accuracy = clf.score(X, y) 
    print('accuracy: ', accuracy)  
    # creating a confusion matrix 
    cm = confusion_matrix(y, prediction) 
    print(cm)
    
    print(clf.best_params_) 

    # print how our model looks after hyper-parameter tuning 
    print(clf.best_score_) 

    # print classification report 
    print(classification_report(y, prediction)) 
    
    distance_to_decision_boundary = clf.decision_function(X)
#     print('X: ',list(distance_to_decision_boundary))
    df_lst = pd.DataFrame(distance_to_decision_boundary,columns=filename)#, columns=['Label0','Label1','Label2','Label3']
    df_lst.to_csv('svm_distance_result.csv')
    print(df_lst)
   

def svm_model(X,y):
    
    ovr = OneVsRestClassifier(LinearSVC(random_state=0,C=0.001)).fit(X, y)
    distance_to_decision_boundary = ovr.decision_function(X)
    print('X: ',list(distance_to_decision_boundary))
    df_lst = pd.DataFrame(distance_to_decision_boundary)
    df_lst.to_csv('distance_result.csv')
    print(df_lst)
#     print('y: ',y)

    prediction=ovr.predict(X)
    # model accuracy for X_test   
    accuracy = ovr.score(X, y) 
    print('accuracy: ', accuracy)  
    # creating a confusion matrix 
    cm = confusion_matrix(y, prediction) 
    print(cm)
    

from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

# function to remove stopwords
def remove_stopwords(text):
    no_stopword_text = [w for w in text.split() if not w in stop_words]
    return ' '.join(no_stopword_text)


df_result['text_clean'] = df_result['text'].apply(lambda x: remove_stopwords(x))

tfidf_vectorizer = TfidfVectorizer(vocabulary = list(set(text_blob)),tokenizer=tokenize, stop_words=stop_words,ngram_range=(1, 4))
xtrain_tfidf = tfidf_vectorizer.fit_transform(df_result['text_clean'])



# X=df_result['text_clean']
print(y)
y=file_idx
# gridsearch_svm_model(xtrain_tfidf,y)
svm_model(xtrain_tfidf,y)